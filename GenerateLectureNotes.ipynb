{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e354ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Final project Neural Networks\n",
    "\n",
    "Thijs van der Laan, s3986721\n",
    "Aydan Yagublu, s3980804\n",
    "Eric Brouwer, s3934640\n",
    "Filipe Laitenberger, s3894479\n",
    "\n",
    "Sources:\n",
    "\n",
    "https://www.tensorflow.org/text/tutorials/text_generation \n",
    "(this tensor flow tutorial served as inspiration for our model \n",
    "and taught us how to build a language processing model like this)\n",
    "\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from pylatexenc.latex2text import LatexNodes2Text\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c37e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'NN_LN_text.txt'\n",
    "# open the latex file\n",
    "latex = open(filename, 'rb').read().decode(encoding='utf-8')\n",
    "# convert the latex to real text\n",
    "text = LatexNodes2Text().latex_to_text(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e12c8d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length: 20.486900206064174\n"
     ]
    }
   ],
   "source": [
    "sentences = text.split(\".\")\n",
    "words = text.split(\" \")\n",
    "print(\"Average sentence length: \" + str(len(words)/len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb6596ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique characters in the text\n",
    "vocab = sorted(set(text))\n",
    "\n",
    "# convert chars into ids\n",
    "convertCharactersIntoIds = preprocessing.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)\n",
    "\n",
    "# convert ids into chars\n",
    "convertIdsIntoCharacters = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=convertCharactersIntoIds.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6530ad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the entire text to ids\n",
    "ids = convertCharactersIntoIds(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "\n",
    "# convert the ids into a dataset\n",
    "idsDataset = tf.data.Dataset.from_tensor_slices(ids)\n",
    "\n",
    "# create text sequences from the ids\n",
    "sequenceLength = 100\n",
    "sequences = idsDataset.batch(sequenceLength+1, drop_remainder=True)\n",
    "\n",
    "# split a sequence into an input and a target output. Example: \"Hello\" -> input: \"Hell\", output: \"ello\"\n",
    "def splitSequenceTrainingExample(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "# split all sequences into input output pairs to create the training dataset\n",
    "dataset = sequences.map(splitSequenceTrainingExample)\n",
    "\n",
    "# Batch size\n",
    "batchSize = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "bufferSize = 10000\n",
    "\n",
    "# shuffle dataset\n",
    "# Prefetch: while the model is executing training step n, the input pipeline is reading the data for step n+1,\n",
    "# so that the training time is reduces as much as possible\n",
    "dataset = dataset.shuffle(bufferSize).batch(batchSize, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fff6af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocabularySize = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embeddingDimensions = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnnUnits = 1024\n",
    "\n",
    "class TextGenerationModel(tf.keras.Model):\n",
    "  def __init__(self, vocabularySize, embeddingDimensions, rnnUnits):\n",
    "    super().__init__(self)\n",
    "    \n",
    "    # word embedding layer\n",
    "    self.embedding = tf.keras.layers.Embedding(vocabularySize, embeddingDimensions)\n",
    "    \n",
    "    # GRU units\n",
    "    self.gru = tf.keras.layers.GRU(rnnUnits, return_sequences=True, return_state=True)\n",
    "    \n",
    "    # dense layer\n",
    "    self.dense = tf.keras.layers.Dense(vocabularySize)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    \n",
    "    # run it through the embedding layer\n",
    "    x = self.embedding(x, training=training)\n",
    "    \n",
    "    # create states if there are none yet\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    \n",
    "    # run it through the GRU layer\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    \n",
    "    # run it through the dense layer\n",
    "    x = self.dense(x, training=training)\n",
    "    \n",
    "    # return the states and preliminary outputs\n",
    "    if return_state:\n",
    "      return x, states\n",
    "\n",
    "    # return the eventual outputs\n",
    "    else:\n",
    "      return x\n",
    "\n",
    "model = TextGenerationModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocabularySize=len(convertCharactersIntoIds.get_vocabulary()),\n",
    "    embeddingDimensions=embeddingDimensions,\n",
    "    rnnUnits=rnnUnits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f3ad33b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "65/65 [==============================] - 81s 1s/step - loss: 4.1500\n",
      "Epoch 2/20\n",
      "65/65 [==============================] - 75s 1s/step - loss: 2.6511\n",
      "Epoch 3/20\n",
      "65/65 [==============================] - 80s 1s/step - loss: 2.4041\n",
      "Epoch 4/20\n",
      "65/65 [==============================] - 78s 1s/step - loss: 2.1861\n",
      "Epoch 5/20\n",
      "65/65 [==============================] - 87s 1s/step - loss: 1.9549\n",
      "Epoch 6/20\n",
      "65/65 [==============================] - 79s 1s/step - loss: 1.7434\n",
      "Epoch 7/20\n",
      "65/65 [==============================] - 83s 1s/step - loss: 1.5766\n",
      "Epoch 8/20\n",
      "65/65 [==============================] - 80s 1s/step - loss: 1.4437\n",
      "Epoch 9/20\n",
      "65/65 [==============================] - 78s 1s/step - loss: 1.3452\n",
      "Epoch 10/20\n",
      "65/65 [==============================] - 83s 1s/step - loss: 1.2664\n",
      "Epoch 11/20\n",
      "65/65 [==============================] - 79s 1s/step - loss: 1.1954\n",
      "Epoch 12/20\n",
      "65/65 [==============================] - 85s 1s/step - loss: 1.1428\n",
      "Epoch 13/20\n",
      "65/65 [==============================] - 79s 1s/step - loss: 1.0892\n",
      "Epoch 14/20\n",
      "65/65 [==============================] - 83s 1s/step - loss: 1.0448\n",
      "Epoch 15/20\n",
      "65/65 [==============================] - 83s 1s/step - loss: 0.9897\n",
      "Epoch 16/20\n",
      "65/65 [==============================] - 83s 1s/step - loss: 0.9442\n",
      "Epoch 17/20\n",
      "65/65 [==============================] - 84s 1s/step - loss: 0.8895\n",
      "Epoch 18/20\n",
      "65/65 [==============================] - 84s 1s/step - loss: 0.8470\n",
      "Epoch 19/20\n",
      "65/65 [==============================] - 82s 1s/step - loss: 0.7895\n",
      "Epoch 20/20\n",
      "65/65 [==============================] - 76s 1s/step - loss: 0.7382\n"
     ]
    }
   ],
   "source": [
    "# loss function\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# compile model using adam optimizer\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpointPrefix = os.path.join('./training_checkpoints', \"ckpt_{epoch}\")\n",
    "\n",
    "# checkpoint callback for the model to save progress\n",
    "checkpointCallback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpointPrefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "# train model\n",
    "history = model.fit(dataset, epochs=20, callbacks=[checkpointCallback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc83547d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer architecture was done in the Le-araptive member from its exactly this zero. Us important do this transition). For instance, a convinced connect with the empirical risk is still in Figure 13 gives plup, the proposed models of neuron neural learning task within the BM can do that this incommand on plain gradient ∇ℛ(θ^((n)) is the back other blocks or the same time, plefif the same paga. For instance concepts and not before eregay of the Metropolis gradient is important and don’t as a reaction dynamics will cover the same landscape over the basics of the proposal distribution given by a pdf. ]\n",
      "\n",
      "[As become an important clear in and Equation [eEepricables of equations which certain within-thinking has too structural stability, that is one of the tradifitions to be a quadratic loss of each presentative effects of easy constant for the a learning problem, it consists of sigiofffencouse: simulated annealing — fesired e another to tom No experiment.\n",
      "\n",
      "The HoffRNN visible brain is a very long rule is revalled for a high-flexibility that we stwouen control circuit. The only this is still academ – revial examples:\n",
      "\n",
      "-   The generic in this longer theory are particularly solutions converge to a dead hurd vectors in index i, the recent dynamics S₁ × … W S² is denote microscopic distributions are inspired unsurcould uselebs are maniebles which heeplys a noterovly that is stable, for every different neural encodions of neuron x are computed from this biologically motion with neural networks (RNNs), as olvisally in combine Decland patterns should convering blocks or nuclear fields, a₃ = 010 and h ∈ ℝ, expect the infinite properties of the future just purpose of its characterizes the energy minimization engine in run in being subfortunately (not today, alε of metails, and is still sorts on his robous something verity vector bif conveniencles, and used these local information [HNWhirll]. The training error, while the next state x(n + 1) paper which φ is such a feward play a model of a molecome used annealing with and sample inside; but wat falled the origin. An electronic neurons is² image that the (B, the training example of a pur of digated in this secrients with a folded way that different mechanism ℋ, the faucet that survey mextly nalls known assumed that the activation sequence was kell, once any infinite microstates because circuments a crashic sample Search for the entire principle, dees live leaky introduced by analyzal weight matrix \n",
      "The transition function is a memory learning system is defined by LTas. Cails, I an p_(ij) is the selected signal gradient descent algorithms 𝒜_(k) that for beach of attempts to simulate in the 10, like control parameters remained ungire it a HN, Here is a function f : ℝ^( ≥ 0) is deeped a function after a few intervals best we ents in the gree) sparing time. However, all of energy-based most knowing that engois than the formula may excite complex Perceptrons can be found in the famous S_(i) that is the same kind. For instance, if adday, even we grow with the electric technical model θ^((∞)). Oll from other temperature of a point relations in machine learning — here.\n",
      "\n",
      "It as sometimes in built, equations one gets in much simple linear function f : [0, Y = ℝ^(L)(x^(n)).\n",
      "\n",
      "-   Thus, constant RC tutorial through the Boltzmann distribution P_(Uchop to the facts of computationally sign-invisuble has the formation turned until the propagaritioning combined in this lecture of applications: becase what is the basis of fattern “photographical RNN — is computed as yourself, bu  wild be computed. Lerg well (unformule of what is the probabilities of a probability distribution on this defined to [earringumation method which corresponsion F(x) are the serious of the three tasks. The main algeir speed r. One ty is both start would be an enough time and everywhen it is gothered known is that they are binary clever operation θ^((n)).\n",
      "\n",
      "-   Thus, a main deep became available, one cannot plot and field, a 2-dimensional HNs is the amany derelishing of RThe way if a pre of repeating in use. It to special formula which corresponding move in but a famous or observe a certain again σ − 1, represent RCA: states x(n), u(n)) = x^(n) dot’s version as a componention of the starters 1, 4, …, …, v_(k)].\n",
      "\n",
      "It also treated models of real-world coptimum one assembled in the arrow) gets it is often constant doing exactly closer to the should adapted models if a vector of microstates a very however, summing a graph of a reason was belong to the backprop algorithms 𝒜_(k).\n",
      "\n",
      "-   The dear network fine-given by asely to brain age assigning an industrial convoirs (the activation function (they are k, the verients you biological neural mechanism to draw a line-algorithm would structure for more commaniestically distributed when the training data how an entirely descent of chaos is obtained areable. Yot feedifocistically ynitting and now teaches into a pribuld and react far many different trajectories to have any of this zero simply, no (describe Gradient descent, ett. Howescientists of increases the origin online let us not unof y ∈ ℝ, and which is recall itself and houses, you fire to have meepled and what often one essentials is computed from two initial state space 𝒳 = ^(L), 0, 0, …)^(L))′ is defined in this special variants of CA run for 10tD Furthermore the deep learning set-up uniquely may labuled algorithm (carchine” executes to a heading input/undurgy, for instance, in particularly switches to the 1919 because this function is that if a proposal distribution P₀E[X, X, on − 1), bit particularly slow cooling is higher. The analytical solution to the testing wat:\n",
      "poonce. This is so usefte the Percess of about what the or input vector which closer to the intervals of various polynomials, u ∈ ℝ^(K)) ∈ ℝ^(M). (un. This corresponding becomes some input-driven systems on the neural later in this could be esplieding in mathematically streamlined in mevory of this contour minimum, meanotical any given in order to get — 1 are (i₀ “jee). Among overfitting. Figure 73 illustrates the material orders 2, 1, 2, 1, 4,\n",
      "9, (2, 1, 0, ); C) is the data time, and wishes to excire to fail for the age. An energy-based formula characterized computing in metropolis sampling (blue sum recurrently pilled, acceptance surface. For a drawing with these limited to an output signal implement a regiven inputs u(n) can be mothered, for each kick to you will found them. I are Boltzmann machine. Let S_(i)(θ_(i) = 4_(j) = b_(M). You smelt symmetical models of this finite-state DS within heeps\n",
      "\n",
      "On the part of the units are ℛ_(i) = 5^(D + M) dovay, Ohe can low cellular persors in the mathematician variant dattern/4. It is a plastically is goving the data distribution P′(y):\n",
      " -  which a well-defined on a phase of what is stable.\n",
      "\n",
      "This example. \n",
      "Loke its initial notation for the abmean the pattern algorithm 𝒜 which on the state space 𝒳 = ℝ² is orderfitting.\n",
      "\n",
      "Bifurcation less usef lectuses A one biological brains is such a very good endine sort of penalty sensed by F(θ) = . The temperature T, is about the Perceptron of an about what I drip-– this is not to train a model f̂ : ℝ^(K) → ℝ^(M) for a dead have on the formula\n",
      "    \n",
      "            +_(\n",
      "          \n",
      "\n",
      "    \n",
      "                                                                                                                             -                                                                                                                                                                                                                                                                                    a_                                        \n",
      "                                                                                                                                                                                                  \n",
      "         \n",
      "                                            \n",
      "                                11       \n",
      "                                                              \n",
      "       \n",
      "                                                                                       \n",
      "                                                                                                              \n",
      "                                                                                               \n",
      "            a majo ververge to its gives rise to the distribution of your brain with millions of neurons is noils the formula\n",
      "\n",
      "        \n",
      "          \n",
      "                                    P\n",
      "        \n",
      "                                          \n",
      "                             \n",
      "                       P\n",
      "                                                                                      \n",
      "                                        \n",
      "                                             \n",
      "                                                                  \n",
      "        \n",
      "    \n",
      "           P s_ij\n",
      "\n",
      "            \n",
      "                \n",
      "\n",
      "      \n",
      "                                                                                     com(X = \n",
      " gut\n",
      "\n",
      "-              \n",
      "                                                                       \n",
      "                   \n",
      "          \n",
      "                                   1                           in\n",
      "\n",
      "    W                                                                  ar if y ∈ ℝ, energy based models. This is only two main tooled to the clean courticular instantial RNN is too large, the δ_Y again if it evis allow our moves on a probability P(X = g_(i) | Y = a_(j) | X = g_(j) apparently the demonstrate to the covariance, O. Distributed-in Equation, Given a simulated annealing models d(X′ =  − 1.\n",
      "\n",
      "(ikild an input: [eDefRNNoutput] suisted by the free energy relates.\n",
      "\n",
      "In such an “lecture notes, like a quickly grown approximate about 5000, 0000, explaining the L-outputs should added again fundamental mechanisms that you might very elementary in the rask. Second, which house, you will be large the decreasing with risk, and asy important and cognitive neural ghostly is indicated i \n",
      "\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class GenerateNextCharacter(tf.keras.Model):\n",
    "  def __init__(self, model, convertIdsIntoCharacters, convertCharactersIntoIds, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.convertIdsIntoCharacters = convertIdsIntoCharacters\n",
    "    self.convertCharactersIntoIds = convertCharactersIntoIds\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.convertCharactersIntoIds(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(convertCharactersIntoIds.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.convertCharactersIntoIds(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.convertIdsIntoCharacters(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states\n",
    "\n",
    "one_step_model = GenerateNextCharacter(model, convertIdsIntoCharacters, convertCharactersIntoIds)\n",
    "\n",
    "states = None\n",
    "next_char = tf.constant(['The Transformer architecture '])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(10000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
