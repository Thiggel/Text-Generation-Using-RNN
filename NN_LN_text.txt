BSc program in Artificial Intelligence
Rijksuniversiteit Groningen, Bernoulli Institute

Fasten your seatbelts – we enter the world of neural networks

Let’s pick up some obvious facts from the surface and take a look at the bottomless voids that open up underneath them.

What makes you YOU is your brain. Your brain is a neural network. A neural network is a network made of neurons which connect to each other by synaptic links. Thus, one of the big question of science and life is how YOU are a NETWORK of interconnected NEURONS.

The answer seemed clear enough 77 years ago for the pioneers of what we now call computational neuroscience. In the kick-start work on neural networks , a neuron x was cast as binary switch that could have two states — call them 0 and 1, or false and true — and this neuron x becomes switched depending on the 0-1 states of the neurons y₁, …, y_(k) which have synaptic links to x. A brain thus was seen as a Boolean circuit. The final sentence in that paper is “Thus in psychology, introspective, behavioristic or physiological, the fundamental relations are those of two-valued logic.” In other words, brains (and you) are digital computers.

But.

What followed is 77 years of scientific and philosophical dispute, sometimes fierce, and with no winners to the present day. The more we have been learning about neurons and brains and humans and robots and computers, the more confusing the picture became. As of now (the year 2020), neither of what is a NEURON, a NETWORK, or a YOU is clear:

NEURONS.

    Biological neurons are extremely complicated physico-chemical objects, with hundreds of fast and slow chemical and genetic processes interacting with each other in a tree-like 3D volume with thousands of branches and roots. Even a supercomputing cluster cannot simulate in real-time all the detail of the nonlinear dynamical processes happening in a single neuron. It is a very much unanswered question whether this extreme complexity of a biological neuron is necessary for the functioning of brains, or whether it is just one of the whims of evolution and one could build equally good brains with much much simpler neurons.

NETWORKS.

    It is generally believed that the power of brains arises from the synaptic interconnectivity architecture. A brain is highly organized society of neurons, with many kinds of communication channels, local communication languages and dialects, kings and workers and slaves. However, the global, total blueprint of the human brain is not known. It is very difficult to experimentally trace neuron-to-neuron connections in biological brains. Furthermore, some neuron A can connect to another neuron B in many ways, with different numbers and kinds of synapses, attaching to different sites on the target neuron. Most of this connectivity detail is almost impossible to observe experimentally. The Human Connectome Project (https://en.wikipedia.org/wiki/Human_Connectome_Project), one of the largest national U.S. research programs in the last years, invested a gigantic concerted effort to find out more and found that progress is slow.

YOU.

    An eternal, and unresolved, philosophical and scientific debate is about whether YOU can be reduced to the electrochemical mechanics of your brain. Even when one assumes that a complete, detailed physico-chemical model of your brain were available (it isn’t), such that one could run a realistic, detailed simulation of your brain on a supercomputer (one can’t), would this simulation explain YOU - entirely and in your essence? the riddles here arise from phenomena like consciousness, the subjective experience of qualia (that you experience “redness” when you see a strawberry), free will and other such philosophical bummers. Opinions are split between researchers / philosophers who, on the one side, claim that everything about you can be explained by a reduction to physico-chemical-anatomical detail, and on the other side claim that this is absolutely impossible. Both have very good arguments. When you start reading this literature you spiral into vertigo.

And, on top of that, let me add that it is also becoming unclear again in these days what a COMPUTER is (I’ll say more about that in the last lecture of this course).

Given that the scientific study of neural networks is so closely tied up with fundamental questions about ourselves, it is no wonder that enormous intellectual energies have been spent in this field. Over the decades, neural network research has produced a dazzling zoo of mathematical and computational models. They range from detailed accounts of the functioning of small neural circuits, comprising a few handfuls of neurons (celebrated: the modeling of a 30-neuron circuit in crustaceans ), to global brain models of grammatical and semantic language processing in humans ; from low-detail models of single neurons (1963 Nobel prize for Alan Hodgkin and Andrew Huxley for a electrical engineering style formula describing the overall electrical dynamics of a neuron in good approximation ), to supercomplex geometrical-physiological compartment models on the high-detail side ); from statistical physics oriented models that can “only” explain how a piece of brain tissue maintains an average degree of activity to AI inspired models that attempt to explain every millisecond in speech understanding ; from models that aim to capture biological brains but thereby become so complex that they give satisfactory simulation results only if they are super delicately fine-tuned to the super general and flexible and robust neural learning architectures that have made deep learning the most powerful tool of modern machine learning applications ; or from models springing from a few most elegant mathematical principles like the Boltzmann machine to complex neural architectures that are intuitively put together by their inventors and which function well but leave the researchers without a glimmer of hope for mathematical analysis (for instance the “neural Turing machine” of ).

In this course I want to unfold for you this world of wonder. My goal is to make you aware of the richness of neural network research, and of the manifold perspectives on cognitive processes afforded by neural network models. A student of AI should, I am convinced, be able to look at “cognition” from many sides — from application-driven machine learning to neuroscience to philosophical debates — and at many levels of abstraction. All neural network models spring from the same core idea, namely that intelligent information processing emerges from the collective dynamics in networks of simple atomic processing units. This gives the field a certain coherence. At the same time, it is amazing into how many different directions one can step forward from this basic idea. My plan is to present a quite diverse choice of neural network models, all classics of the field and must-know’s for any serious AI/cognitive science/machine learning disciple. You will see that the world of neural networks has so much more to offer than just “deep learning” networks, which in these days outshine all other kinds in the public perception.

This said, I will nonetheless start the course with an introduction to that currently most visible kind of neural networks, feedforward neural networks, from the classical, simple Perceptrons via multilayer perceptrons to a number of deep learning models. I position this material at the beginning of the course because these models most directly lend themselves to practical programming projects, such that you can swiftly start working on the practical project that accompanies the theory lectures.

But, please, be aware that scientific tides come and go, and the day will come when deep learning methods will recede in favor and some other magic will move up front. Then the other themes and methods that you have learnt about in this course will help you to connect with whatever else comes next. In this vein, it may interest you to learn that the now-dominant paradigm of deep learning directly emerged from quite another brand of neural networks, the Boltzmann machine which you will get to know in a few weeks. The celebrated paper which today is widely seen as the starter for deep learning in fact was written from the perspective of the statistical physics which rules Boltzmann machines, and the option to use them as an initialization submechanism for what today are called deep networks was only mentioned in passing. A few years later, the Boltzmann machine theme receded into the background and deep deep deep became the motto of the day. Such shifts in focus will happen again! And I will dare to give a forecast in the last lecture of this course.

A note on mathematical background that is required. Neural networks process “training data” and these data typically originally come in the format of Excel files (yes! empirical scientists who actually generate those valuable “raw data” often use Excel!), which are just matrices if seen with the eye of a machine learner. Furthermore, neural networks are shaped by connecting neurons with weighted “synaptic links”, and these weights are again naturally sorted in matrices. And the main operation that a neural network actually does is formalized by a matrix-vector multiplication. So it’s matrices and vectors all over the place, no escape possible. You will need at least a basic, robust understanding of linear algebra to survive or even enjoy this course. We will arrange a linear algebra crash refresher early in the course. A good free online resource is the book “Mathematics for Machine Learning” .

Furthermore, to a lesser degree, also some familiarity with statistics and probability is needed. You find a summary of the must-knows in the appendix of these lecture notes, and again a tutorial exposition in .

Finally, a little (not much) calculus is needed to top it off. If you are familiar with the notion of partial derivatives, that should do it. In case of doubt - again it’s all (and more) in .

A very fast rehearsal of machine learning basics

Because I want to start telling the neural network story with examples that currently dominate machine learning (ML), let us make sure we are on the same page concerning the basics of ML. The presentation in this section can only be a condensed summary — a steep crash course on a rather abstract level — not easy I am afraid. If you want to read up on details, I recommend the online lecture notes from my Master course on ML. For basic mathematical notation of sets and formal product constructions, consult the Appendix.

From the three main kinds of machine learning tasks — supervised learning, unsupervised learning, and reinforcement learning — here I only consider the first one. Furthermore, I will limit the presentation to data that are given in real-valued vector format, because that is the format used for most neural networks.

A very good, comprehensive yet concise introduction to basics of machine learning can be found in Section 5, “Machine Learning Basics”, in the deep learning “bible” of . A free online version is available. I want to alert you to the fact that this famous textbook starts with a chapter on linear algebra essentials. Neural networks process data, and data come in data records sorted in tables — that is, matrices. Neural networks are made of neurons that are interconnected by weighted links, and these weights are collected in matrices. When a neural network is “trained” or when the trained network later does the data-processing job that it has been trained for, the central operation inside the net are matrix-vector multiplications. There is no way to escape from linear algebra when dealing with neural networks. If you have forgotten the definition of eigenvectors — do a recap of linear algebra, please! it will help you so much.

In this course and this section I can cover only a small part ML essentials — the essentials of the essentials. This should however suffice to give you enough starter knowledge to use neural networks in supervised learning tasks in your semester project.

Training data.

A supervised learning tasks starts from labelled training data, that is a sample S = (u_(i), y_(i))_(i = 1, …, N) of input-output pairs (u_(i), y_(i)). For simplicity we assume that the inputs and output are real-valued vectors, that is u_(i) ∈ ℝ^(K), y_(i) ∈ ℝ^(M), although other formats are possible.

Brief note on notation: In most ML textbooks the inputs are denoted as x, not u. However, we will later often deal with the internal processing states of neural networks. Following traditions in signal processing and dynamical systems maths, I will use the symbol x for these internal state vectors of a signal processing device (for us: a neural network), and use u for external inputs.

Two examples:

-   Image classification tasks: the input patterns u_(i) are vectors whose entries are the red, green, blue intensity values of the pixels of photographic images. For instance, if the images are sized 600 × 800 pixels, the input pattern vectors are of dimension n = 600 ⋅ 800 ⋅ 3 = 1, 440, 000. The output vectors might then be binary “one-hot encodings” of the picture classes to be recognized. For example, if the task is to recognize images of the ten sorts of handwritten digits 0, 1, 2, ..., 9, the output vector y_(i) for a “1” input image would be the 10-dimensional vector (0, 1, 0, …, 0)′ that has a 1 in the second component and is zero everywhere else.

    Quick note in passing: vectors come as row or column vectors. I always use column vectors. Thus, when I write u_(i) For the input vector representing an image, it is a 1, 440, 000 column vector. To turn a column vector to a row vector, or vice versa, one applies the transpose operation, which I denote by a prime. Thus u′_(i) is a row vector, and (0, 1, 0, …, 0)′ is a column vector.

-   Function approximation: Assume that there is a function f : ℝ^(K) → ℝ^(M), and the (u_(i), y_(i)) are randomly drawn argument-value examples with some added noise ν, that is y_(i) = f(u_(i)) + ν. For example (an example that I was tasked with in my past), the u_(i) could be vectors of measurement values taken from an array of sensors in a nuclear fission reactor, and the M = 1 dimensional output values y_(i) would indicate the time in milliseconds until the plasma in the reactor will become unstable and explosively damage its containment, a very expensive event which nuclear fission experts really want to avoid. When the neural network that they trained predicts that the plasma will explode in 5 milliseconds (time runs fast in these reactors), they shoot a pellet of frozen hydrogen into the plasma with a gun that can shoot frozen hydrogen pellets, no joking.

    Function approximation tasks are also often called regression tasks.

Training objectives.

Given a training sample S = (u_(i), y_(i))_(i = 1, …, N), the task of supervised learning is to train a model of the input-output function f that gave rise to the observed training data. Such an ML model is an algorithm f̂ which accepts inputs from ℝ^(K) and computes outputs in ℝ^(M). (A note on terminology: In statistical modeling, f̂ is an estimate of the true function f. The notation is often used to denote estimates of something. Also, in statistics, functions that map input values to output values — here we named them f or f̂ — are often called decision functions, a naming that I adopted in the Machine Learning lecture notes.)

A machine learning algorithm is a compuational procedure which gets a training sample S as input and “learns” (that is, computes — and a stastician would say, estimates) a model f̂ : ℝ^(K) → ℝ^(M). In machine learning, the model f̂ will be an executable algorithm (for instance, a neural network). Thus, a machine learning algorithm is an algorithm that transforms data into algorithms!

In order to decide whether an estimated model f̂ : ℝ^(K) → ℝ^(M) is “good” or “bad”, one needs a well-defined measure to quantify the goodness of a model. This is achieved by a loss function

L : ℝ^(M) × ℝ^(M) → ℝ^( ≥ 0).

The idea is that a loss function measures the “cost” of a mismatch between the correct value y and the model output f̂(u), given a correct input-output pair (u, y) from the training data or from testing data. Higher cost means lower quality of f̂. By convention, loss functions are non-negative and higher values means worse fit between true and estimated output. Many loss functions are in use. An experienced machine learning spends much care on tailoring the loss function to the learning problem that s/he is dealing with. Two basic examples:

-   A loss that counts misclassifications in pattern classification tasks: assume that one is dealing with a pattern classification set-up where the target outputs y in the training/testing data and the outputs returned by a model f̂ are m-dimensional binary “one-hot” class encoding vectors. Let h : ℝ^(K) → ^(M) be any candidate model. Then the loss
    counts misclassifiation errors. It is sometimes called the counting loss.

-   A very popular loss penalizes quadratic errors of vector-valued targets:
    L(h(u), y) = ∥h(u) − y∥².
    This loss is often just called “quadratic loss”, and it is one of the most frequently used ones in regression tasks.

In order to understand the nature of supervised learning tasks, one has to frame it in the context of probability theory. You find a summary of the necessary probability concepts and notation in the appendix. We assume that the training and testing input-output data u, y are obtained from random variables U, Y.

Learning algorithms should minimize the expected loss, that is, a good learning algorithm should yield a model f̂ whose risk
R(f̂) = E[L(f̂(U), Y)]
is small. The expectation here is taken with respect to the true joint distribution P_(U, Y) of the data-generating random variables U and Y. For example, in a case where U and Y are numerical RVs and their joint distribution is described by a pdf p, the risk of a candidate model h would be given by
R(h) = ∫_(ℝ^(K) × ℝ^(M))L(h(u), y) p(u, y) d(u, y).
However, the true distribution P_(U, Y) and its pdf p are unknown. The mission to find a model f̂ which minimizes [eDefRisk] is, in fact, hopeless. The only access to P_(U, Y) that the learning algorithm affords is the scattered reflection of P_(U, Y) in the training sample (u_(i), y_(i))_(i = 1, …, N).

A natural escape from this impasse is to tune a learning algorithm such that instead of attempting to minimize the risk [eDefRisk] it tries to minimize the empirical risk

The set ℋ is the hypothesis space – the search space within which a learning algorithm may look for an optimal h.

It is important to realize that every learning algorithm comes with a specific hypothesis space. For instance, when one uses linear regression to solve [eMinEmpRisk], ℋ is the set of all linear functions from ℝ^(K) to ℝ^(M) (I assume you have learnt about linear regression in your first year course on linear algebra). Or, if one sets up a neural network learning algorithm, ℋ is typically the set of all neural networks that have a specific connection structure (number of neuron layers, number of neuros per layer); the networks in ℋ then differ from each other only by the weights associated with the synaptic connections.

The empirical risk is often – especially in numerical function approximation tasks – also called the training error.

Here is an interim take-home summary:

-   The ultimate goal for supervised learning algorithms is to estimate a model f̂ which has a low risk [eDefRisk], that is, which on average gives low-loss (“good”) outputs on “testing” data drawn from the distribution P_(U, Y).

-   The only source of information that the learning algorithm has is the training sample S = (u_(i), y_(i))_(i = 1, …, N).

-   Thus, it appears that the best one can do is to design a learning algorithm 𝒜 which minimizes the empirical risk (“training error”), that is, upon input S the learning algorithm should return the solution of the minimization problem [eMinEmpRisk]:

In the next subsections we will see that the situation is more involved. First, in the kind of complex real-world learning tasks that neural networks usually are used for, algorithms that find exact minimal-training-error solutions do not exist. One only can design learning algorithms that find approximate solutions. Second, if one reduces the learning problem to finding minimal-training-error solutions, one will almost always run into the problem of overfitting. This second complication is by far more painful and important than the first one, and I will address it in the following subsection.

The overfitting problem.

While minimizing the empirical risk is a natural way of coping with the impossibility of minimizing the risk, it may lead to models which combine a low empirical risk with a high risk. This is the ugly face of overfitting.

The overfitting problem is connected to certain properties of learning algorithms which I will collectively refer to as the flexibility of a learning algorithm. The flexibility of a learning algorithm can be defined in several ways, and there are several methods to steer the flexibility of a learning algorithm. Flexibility is not a single, rigorously well-defined concept; it is an entire bundle of aspects which have been addressed in machine learning in many ways. But the general idea is always the same and can be stated in intuitive terms as “a learning algorithm 𝒜 is more flexible than another learning algorithm ℬ if 𝒜 can fit its computed models f̂ more closely to the training data than ℬ can do”. More flexible learning algorithms thus can compute models with lower training error. Maximizing flexibility (by designing learning algorithms that can make come very close to the teacher outputs y_(i)) can lead to doing too much of a good thing: a super flexible learning algorithm may even give zero training error, while performing very poorly on new testing data, rendering the found model f̂ absolutely useless.

Because overfitting is such a fundamental challenge in supervised machine learning, I illustrate its manifestations with four examples. They are copied with slight adaptation of notation from the machine learning lecture notes.

Example 1: polynomial curve-fitting

This example is the standard textbook example for demonstrating overfitting. Let us consider a one-dimensional input, one-dimensional output regression task of the kind where the training data are of form (u_(i), y_(i)) ∈ ℝ × ℝ. Assume that there is some systematic relationship y = f(u) that we want to recover from the training data. We consider a simple artificial case where the u_(i) range in [0, 1] and the to-be-discovered true functional relationship f is y = f(u) = sin (2 π u). The training data, however, contain a noise component, that is, y_(i) = sin (2 π u_(i)) + ν_(i), where ν_(i) is drawn from a normal distribution with zero mean and standard deviation σ. Figure 1 shows a training sample (u_(i), y_(i))_(i = 1, …, 11), where N = 11 training points u_(i) are chosen equidistantly.

[An example of training data (red squares) obtained from a noisy observation of an underlying “correct” function sin(2 π u) (dashed blue line).]

We now want to solve the task of learning a good approximation for f from the training data (u_(i), y_(i)) by applying polynomial curve fitting, an elementary technique you might be surprised to meet here as a case of machine learning. Consider an k-th order polynomial
p(u) = w₀ + w₁u + ⋯ + w_(k)u^(k).
We want to approximate the function given to us via the training sample by a polynomial, that is, we want to find (“learn”) a polynomial f̂ = p(u) such that p(u_(i)) ≈ y_(i). More precisely, we want to minimize the mean square error on the training data
At this moment we don’t bother how this task is solved computationally but simply rely on the Matlab function polyfit which does exactly this job for us: given data points (u_(i), y_(i)) and polynomial order k, find the coefficients w_(j) which minimize this MSE. Figure 2 shows the polynomials found in this way for k = 1, 3, 10.

[Fitting polynomials (green lines) for polynomial orders 1, 3, 10 (from left to right).]

If we compute the MSE’s for the three orders k = 1, 3, 10, we get  respectively. Some observations:

-   If we increase the order k, we get increasingly lower .

-   For k = 1, we get a linear polynomial, which apparently does not represent our original sine function well (underfitting).

-   For k = 3, we get a polynomial that hits our target sine apparently quite well.

-   For k = 10, we get a polynomial that perfectly matches the training data, but apparently misses the target sine function (overfitting).

The modelling flexibility is here defined through the polynomial order k. If it is too small, the models are too inflexible and underfit; if it is too large, we earn overfitting.

However, please switch on your most critical thinking mode and reconsider what I have just said. Why, indeed, should we judge the linear fit “underfitting”, the order-3 fit “seems ok”, and the order-10 fit “overfitting”? There is no other ground for justifying these judgements than our visual intuition — and the fact that I told you beforehand that the correct function is this sinewave! In fact, the order-10 fit might be the right one if the data contain no noise! and the order-1 fit might be the best one if the data contain a lot of noise! We don’t know!! which model is really the best one!

Example 2: pdf estimation

This example comes from the domain of unsupervised learning. In unsupervised learning, the training data are just a collection of points S = (u_(i))_(i = 1, …, N) in ℝ^(K), taken from a distribution P_(U) generated by a random variable U, and the objective is to estimate a model of the distribution P_(U). One typical way to specify such models is by charactering them through a probability density function (pdf) p_(U) : ℝ^(K) → ℝ^( ≥ 0). The overfitting problem arises here just as dramatically as in supervised training. I include this example because it is visually striking.

[Estimating a pdf from 6 data points. Model flexibility grows from left to right. Note the different scalings of the z-axis: the integral of the pdf (= the volume under the surface) is 1 in each of the three cases.]

Let us consider the task of estimating a 2-dimensional pdf over the unit square from 6 given training data points _(i = 1, …, 6), where each u_(i) is in [0, 1] × [0, 1]. This is an elementary unsupervised learning task, the likes of which frequently occur as a subtask in more involved learning tasks, but which is also of interest in its own right. Figure 3 shows three pdfs which were obtained from three different learning runs with models of increasing flexibility (I don’t explain the learning algorithms here — for the ones who know about it: simple Gaussian Parzen-window models where the degree of admitted flexibility was tuned by kernel width). Again we witness the fingerprints of under/overfitting: the low-flexibility model seems too “unbending” to resolve any structure in the training point cloud (underfitting), the high-flexibility model is so volatile that it can accomodate each individual training point (presumably overfitting).

But again, we don’t really know...

[Learning a decision boundary for a 2-class classification task of 2-dimensional patterns (marked by black “x” and red “o”).]

Example 3: learning a decision boundary

Figure 4 shows a schematic of a classification learning task where the training patterns are points in ℝ² and come in two classes. When the trained model is too inflexible (left panel), the decision boundary is confined to a straight line, presumably underfitting. When the flexibility is too large, each individual training point can be “lasso-ed” by a sling of the decision boundary, presumably overfitting.

Do I need to repeat that while these graphics seem to indicate under- or overfitting, we do not actually know?

Example 4: furniture design

Overfitting can also hit you in your sleep, see Figure 5.

[A nightmare case of overfitting. Picture spotted by Yasin Cibuk (2018 ML course participant) on http://dominicwilcox.com/portfolio/bed/ (link now dead), designed and crafted by artist Dominic Wilcox, 1999. Quote from the artist’s description of this object: “I used my own body as a template for the mattress”. From a ML point of view, this means a size N = 1 training data set.]

Again, while this looks like drastic overfitting, it would be just right if all humans sleep in the same folded way as the person whose sleep shape was used for training the matrass model.

THE nonidentical twin curves

There are several ways how to tune the flexibility of a learning algorithm. I will outline some of the most important ones in a moment. But before we consider technicalities, I would like to present THE curve (Figure 6). The x-axis gives a learning algorithm flexibility range, meaning that the further to the right, a more flexible learning algorithm is used. On the y-axis two curves are plotted. For each flexibility adjustment of a learning algorithm (x-axis), these two curves give the risk (testing error) and the empirical risk (training error) of the model found by the respective learning algorithm.

Three points are worth pointing about in this diagram:

-   On the left end (very inflexible learning algorithms), both training and testing error are high – due to the low flexibility of the algorithm the obtained models are just too simple to capture relevant structure in the data distribution.

-   On the right end (super flexible learning algorithms), the training error is small — often, indeed, it can be pushed down to zero — because the high flexibility of the learning algorithm allowed it to fit itself around each individual training point. In human psychology terms: the model has learnt the data by heart. But, the testing error is high: memorizing teaching material by heart is stupid; a good learner extracts the underlying regularities and laws from the data and can transfer that valuable, extracted, condensed knowledge to apply in new (testing) situations.

-   Somewhere in the mid-range of flexibility the testing error has a minimum. This corresponds to learning with an algorithm whose flexibility has been optimially tuned. It will return models that on average will perform best when applied to new input data.

[The generic, universal, core challenge of machine learning: finding the right model flexibility which gives the minimal risk.]

In order to practically find the sweet spot of optimial flexibility, two technical conditions must be satisfied:

1.  One must have an effective method to change the flexibility of learning algorithms.

2.  One must have an effective method to estimate the risk (red curve in the figure).

If one has these two mechanisms available, one can find the sweet spot (green line in our figure) by a systematic sweep through flexibilities, learning models for each flexibility, estimate the risk, and settle for the flexibility that has minimal estimated risk. In the next two subsections I outline how these two mechanisms can be instantiated.

How to tune model flexibility

There are a number of approaches to tune the flexibility of a learning algorithm. In this subsection I list some of the most commonly applied ones (copied with simplifications and cuts from the machine learning lecture notes).

Tuning learning flexibility through model class size

In the polynomial curve fitting example from Section 1.3.1, the model parameters were the monomial coefficients w₀, …, w_(k) (compare Equation [ePolynomal4Sine]). After fixing the polynomial order k, the polynomial p(u) with minimal training error was selected from the set $


It is clear that ℋ₁ ⊂ ℋ₂ ⊂ …. It is also clear that the training error can only shrink when k grows, because the set of candidate solutions ℋ_(k) grows with k, thus there are more candidate solutions to pick from for the optimization algorithm.

Generalizing from this example, we can see one way to obtain a sequence of learning algorithms of increasing flexibility: A model class inclusion sequence is a sequence ℋ₁ ⊂ ℋ₂ ⊂ … of sets of candidate models. If there are algorithms 𝒜_(k) to solve the optimization problem [naiveLearning] for each k, one can practically adjust the model flexibility by trying in turn the model classes ℋ_(j) together with their learning algorithms 𝒜_(k) (j = 1, …, k).

There are many ways how one can set up a sequence of learning algorithms which pick their respective optimal models from such a model class inclusion sequence. In most cases this will just mean to admit larger models with more tuneable parameters for “higher” classes. In polynomial curve fitting this meant to admit polynomials of growing order. In neural network training this means to run neural network training algorithms on neural networks of increasing size.

Using regularization for tuning modeling flexibility

The flexibility tuning mechanism explained in this subsection is simple, practical, and in widespread use. It is called model regularization. Note that the word “regularization” is also often used in a more general way to denote any method for tuning flexibility of a learning algorithm.

When one uses regularization to vary the modeling flexibility, one does not vary the model candidate class ℋ at all. Instead, one varies the optimization objective [eMinEmpRisk] for minimizing the training error.

The basic geometric intuition behind modeling flexibility is that low-flexibility models should be “smooth”, “more linear”, “flatter”, admitting only “soft curvatures” in fitting data; whereas high-flexibility models can yield “peaky”, “rugged”, “sharply twisted” curves (see again Figures 2, 3, 4).

When one uses model regularization, one fixes a single model structure and size with a fixed number of trainable parameters, that is, one fixes ℋ. Structure and size of the considered model class should be rich and large enough to be able to overfit (!) the available training data. Thus one can be sure that the “right” model is contained in the search space ℋ.

The models in ℋ are typically characterized by a set of trainable parameters. In polynomial curve fitting these parameters are the monomial coefficients, and for a fixed neural network structure, it would be the set of all synaptic weights. Following the traditional notation in the machine learning literature we denote this collection of trainable parameters by θ. This is a vector that has as many components as there are trainable parameters in the chosen kind of model. We assume that we have D tuneable parameters, that is θ ∈ ℝ^(D).

Such a high-flexibility model type would inevitably lead to overfitting when an “optimal” model would be learnt using the basic learning equation [eMinEmpRisk] which I repeat here for convenience:

In order to dampen the exaggerated flexibility of this baseline learning algorithm, one adds a regularization term (also known as penalty term, or simply regularizer) to the loss function. A regularization term is a cost function reg : ℝ^(D) → ℝ^( ≥ 0) which penalizes model parameters θ that code models with a high degree of geometrical “wiggliness”.

The learning algorithm then is constructed such that it solves, instead of [eMinEmpRisk], the regularized optimization problem

where θ_(h) is the collection of parameter values in the candidate model h.

The design of a useful penality term is up to your ingenuity. A good penalty term should, of course, assign high penalty values to parameter vectors θ which represent “wiggly” models; but furthermore it should be easy to compute and blend well with the algorithm used for empirical risk minimization.

Two examples of such regularizers:

1.  In the polynomial fit task from Section 1.3.1 one might consider for ℋ all 10th order polynomials, but penalize the “oscillations” seen in the right panel of Figure 2, that is, penalize such 10th order polynomials that exhibit strong oscillations. The degree of “oscillativity” can be measured, for instance, by the integral over the (square of the) second derivative of the polynomial p,
    Investing a little calculus (good exercise! not too difficult), it can be seen that this integral resolves to a quadratic form reg(θ) = θ′ C θ where C is an 11 × 11 sized positive semi-definite matrix. That format is more convenient to use than the original integral version.

2.  A popular regularizer that often works well is just the squared sum of all model parameters,
    reg(θ) = ∑_(w ∈ θ)w².
    This regularizer favors models with small absolute parameters, which often amounts to “geometrically soft” models. This regularizer is popular among other reasons because it supports simple algorithmic solutions for minimizing risk functions that contain it. It is called the L₂-norm regularizer (or simply the L₂ regularizer) because it penalizes the (squared) L₂-norm of the parameter vector θ.

Computing a solution to the minimization task [eRiskWithRegularizer] means to find a set of parameters which simultaneously minimizes the original risk and the penalty term. The factor α² in [eRiskWithRegularizer] controls how strongly one wishes the regularization to “soften” the solution. Increasing α² means downregulating the model flexibility. For α² = 0 one returns to the original un-regularized empirical risk (which would likely mean overfitting). For very large α² → ∞ the regularization term entirely dominates the model optimization and one gets a model which does not care anymore about the training data but instead only is tuned to have minimal regularization penalty. In case of the L₂ norm regularizer this means that all model parameters are zero – the ultimate wiggle-free model; one should indeed say the model is dead.

When regularization is used to steer the degree of model flexibility, the x-axis in Figure 6 would be labelled by α² (highest α² on the left, lowest at the right end of the x-axis).

Using regularizers to vary model flexibility is often computationally more convenient than using different model sizes, because one does not have to tamper with differently structured models. One selects a model type with a very large (unregularized) flexibility, which typically means to select a big model with many parameters (maybe a neural network with hundreds of thousands of synaptic connections).

Tuning model flexibility through adding noise

Another way to tune model flexibility is to add noise. Noise can be added at different places. Here I mention two scenarios.

Adding noise to the training input data.

    If we have a supervised training dataset (u_(i), y_(i))_(i = 1, …, N) with vector patterns u_(i), one can enlarge the training dataset by adding more patterns obtained from the original patterns (u_(i), y_(i))_(i = 1, …, N) by adding some noise vectors to each input pattern u_(i): for each u_(i), add l variants u_(i) + ν_(i1), …, u_(i) + ν_(il) of this pattern to the training data, where the ν_(ij) are i.i.d. random vectors (for instance, uniform or Gaussian noise). This increases the number of training patterns from N to (l + 1)N. The more such noisy variants are added and the stronger the noise, the more difficult will it be for the learning algorithm to fit all data points, and the smoother the optimal solution becomes – that is, the more one steers to the left (underfitting) side of Figure 6. Adding noise to training patterns is a very common strategy. It is one way of doing data augmentation: create artificial, “distorted” versions of the training data and add them to the data used for training.

Adding noise while the optimization algorithm runs.

    If the optimization algorithm used for minimizing the empirical risk is iterative, one can “noisify” it by jittering the intermediate results with additive noise. The “dropout” regularization trick which is widely used in deep learning is of this kind. Again, the effect is that the stronger the algorithm is noisified, the stronger the regularization, that is the further one steers to the left end of Figure 6.

How to estimate the risk of a model

As I mentioned at the end of Section 1.3.5, besides being able to navigate effectively on the flexibility axis of THE twin curve (Figure 6), one also must have an effective means to estimate the risk (testing error) of estimated models f̂, if one wants to locate the best degree of flexibility.

There exist a number of analytical formulas which allow one to estimate the risk of some model f̂ in a mathematical analytical way (for instance, the Akaike information criterion, https://en.wikipedia.org/wiki/Akaike_information_criterion, is relatively popular). However, such analytical estimates of the risk are reliable only if additional conditions concerning the nature of the sampling distribution and the model are satisfied, and these conditions may be difficult to verify. Therefore, in daily practice one uses more often another approach, which is robust and generally applicable and easy to implement and needs no insight into analytical properties of the data distribution nor the model. The price to pay is a potentially high compuational cost. This is the method of cross-validation.

Here is the basic idea of cross-validation.

In order to determine whether a given model is under- or overfitting, one would need to run it on test data that are “new” and not contained in the training data. This would allow one to get a hold on the red curve in Figure 6.

However, at training time only the training data are available.

The idea of cross-validation is to artificially split the training sample S = (u_(i), y_(i))_(i = 1, …, N) into two subsets T = (u_(i), y_(i))_(i ∈ I) and V = (u′_(i), y′_(i))_(i ∈ I′). These two subsets are then pretended to be a "training" and a "testing" dataset. In the context of cross-validation, the second set V is called a validation set. Unfortunately there is no special word for the subset T. It is also called “training set” in the literature, which is confusing because it is a subset of the originally given complete training set. I will call T the reduced training set.

A bit more formally, let the flexibility axis in Figure 6 be parametrized by some appropriate flexibility measure r, where small r means “strong regularization”, that is, “go left” on the flexibility axis. Let the range of r be r = 1, …, l.

For each setting of flexibility r, the data in T is used to train an optimal model  performs best on the validation data. Its flexibility r^(*) is then taken to be the flexibility that marks the “sweet spot” indicated by the green bar in Figure 6. After this screening of degrees of flexibility for the best test performance, a model within the found optimal regularization strength r^(*) is then finally trained on the original complete training data set S = T ∪ V.

This whole procedure is called cross-validation. Notice that nothing has been said so far about how to split S into T and V.

A clever way to answer this question is to split S into many subsets S_(j) of roughly equal size (j = 1, ..., k). Then, for each flexibility value r carry out k complete screening runs via cross validation, where in the j-th run the subset S_(j) is withheld as a validation set, and the remaining k − 1 sets joined together make for a training set. After these k runs, average the validation errors obtained in the k runs. This average is then taken as an estimate for the risk of learning a model with flexibility r. This is called k-fold cross-validation. Here is the procedure in detail:

This procedure contains two nested loops and looks expensive. For economy, one starts with the low-end r and increases it stepwise, assessing the generalization quality through cross-validation for each regularization strength r, until the validation risk starts to rise. The strength  reached at that point is likely to be about the right one.

The best assessment of the optimal class is achieved when the original training data set is split into singleton subsets — that is, each S_(j) contains just a single training example. This is called leave-one-out cross-validation. It looks like a horribly expensive procedure, but yet it may be advisable when one has only a small training data set, which incurs a particularly large danger of ending up with poorly generalizing models when a wrong model flexibility was used.

K-fold cross validation is widely used. It is a factual standard procedure in supervised learning tasks when the computational cost of learning a model is affordable.

I should also mention that cross-validation is not only used for finding the right degree of regularization, but can similarly be used for tuning hyperparameters of a learning procedure 𝒜. The term “hyperparameter” is generally used for all kinds of “knobs to play around with” when tuning a complex learning algorithm. Modern deep learning algorithms have frightfully many tuning options.

Feedforward networks in machine learning

After having lubricated our machine learning gearboxes, we are ready for taking off into the lands of neural networks (NNs). In this section I will present those NNs that are most commonly used in practical machine learning applications: feedforward neural networks trained by the backpropagation algorithm. I will present them in historical order, starting with the famous ancestor of them all, the Perceptron.

The Perceptron

The Perceptron was developed by Frank Rosenblatt, an American psychologist / neuro-biologist / electrical engineer, in the years around 1960 (for instance, ). It was a computational neural architecture inspired by what was known about the human visual processing system at that time, and its workings were demonstrated on visual character classification tasks. Figure 7 gives a diagram of the Perceptron’s architecture.

[Architecture of the Perceptron. Note that Rosenblatt designed and built many variants of the Perceptron; this is just one of them. The “summation unit” and the “response unit” (and possibly the incoming synaptic connections too) taken together are a “neuron” in today’s NN terminology. Graphics taken from .]

In plain English, the Perceptron processes its input as follows.

-   Typical inputs are clean black-and-white images of letters, sensed by an array of photocells (The “retina” in Figure 7 is this array of photocells. Figure 8 A shows one of Rosenblatt’s analog hardware realizations of the Perceptron, the Mark 1). This gives analog measurement values per “pixel” (= photocell) — this version of the perceptron is not a digital but an analog computing system.

-   In a next layer of processing, a number of “associator” neurons each pick up a random selection of the retina values and perform some (arbitrary) Boolean function on them. In modern machine learning language, these associator unites compute local random features from the retinal image.

-   The activation values (denoted y_(j) in Figure 7) of the associator units are propagated forward into a single “summation unit”, where they are summed up — but not before they have been scaled by “synaptic” weights w_(j). Thus, the summation unit together with these weights yields a linear combination of the associator activation values.

-   Finally, the output value of the summation unit is passed through a “response unit”, which delivers the ultimate output of the Perceptron by applying a thresholding operation which results in a 0 or 1 value.

-   The entire processing pipeline of the Perceptron thus transforms retinal sensor patterns into a binary output — which is the format of an image classification system. The Perceptron was thus probed on a binary classification task, for instance letters “A” versus letters “C”.

-   The Perceptron was revolutionary in that it was a learning system. In order to solve the requested image classification task, the synaptic weights must be the right ones. It is not difficult to find a mathematical formula which computes a set of well-working synaptic weights if such a set exists at all. Rozenblatt’s perceptron however “learnt” the right synaptic weights in an incremental, iterative training process. In this process, the network was presented with a series of what we have learnt to call training patterns (u_(i), y_(i))_(i = 1, …, N), where the u_(i) are vectors containing the response values of the photocells, and y_(i) ∈  is a binary indicator, for instance y_(i) = 0 if the input pattern came from an “A” image and y_(i) = 1 if the input came from a “C” image. When during the training process the Perceptron was shown some input and its output was correct, the synaptic weights remained unchanged; if the output was wrong, they were adapted by an incremental weight adaptation rule, the Perceptron learning rule. Rosenblatt could prove that this learning rule would converge to a perfectly performing network if a perfectly working set weights existed at all.

The Perceptron (in its Mark 1 realization) was a remarkable mix of analog signal processing hardware, electrical engineering, biological inspiration, and elementary mathematics. It featured many innovations that today are commonplace for neural networks in machine learning (random nonlinear features, computing outputs by passing the results of a linear combination through a nonlinearity, using image classification as a benchmark demonstration).

[The Mark 1 perceptron — an analog hardware neural network. A: Pattern input: brightly lit B/W images of printed characters sensed by an array of 20 x 20 photocells. B: Wiring — before the invention of printed circuit boards. C: Adaptive weights realized by motor-driven potentiometers (all images from , Chapter 4.1) ]

In plain maths, if one strips off all biologism and does not include the random Boolean operations of the associator units in the learning story (because they remain fixed and are not trainable), the Perceptron simply learns a function

where the argument are vectors u = (x₁, …, x_(K))′ containing the n activations of the associator units, and σ : ℝ → ℝ, σ(x) = 1 if x ≥ 0 else
σ(x) = 0 is a binary thresholding function with threshold at zero. The variables x_(i) are fixed (not trainable) Boolean functions of the raw (image) inputs u.

Note that different from the schema shown in Figure 7 there is no “bias” b inside the thresholding function. It is common practice in today’s machine learning to omit this bias and instead fix one of the inputs x₁, …, x_(K) to always have a value of 1 in all input patterns. This is mathematically equivalent to the bias b inside the threshold function and leads to a more compact notation.

The Perceptron learning rule goes like this:

Given:

    A training pattern set (u_(i), y_(i))_(i = 1, …, N) with u_(i) ∈ ℝ^(K), y_(i) ∈ .

Model initialization:

    set the weight vector w = (w₁, …, w_(K)) to some arbitrary initial value, for instance to some random vector or the all-zero vector.

Cycle through training data and adapt weights:

    Present the training patterns to the Perceptron one after the other, starting again from the beginning when the last pattern has been used. At each presentation of a pattern u_(i),

    -   compute the current output ŷ_(i) = σ(w′ u) of the Perceptron,

    -   if ŷ_(i) = y_(i), do not change the weight vector.

    -   if ŷ_(i) ≠ y_(i), change the weight vector by w ← w + (y_(i) − ŷ_(i)) u_(i).

Stop

    when in one full cycle through all training examples no weight changes were triggered (that is, all training examples are correctly classified).

Today, even a beginner in machine learning would immediately criticize that this method only minimizes the training error, inviting overfitting; and that this seems a very weak and restricted learning algorithm, because it is only applicable to 2-class classification problems, and the search space ℋ for possible solutions f̂ (see Equation [ePerceptron]) contains only very simple functions, namely linear maps followed by a thresholding.

But, at the time when Rosenblatt presented the Perceptron, machine learning as a field did not exist at all (in retrospect, the Perceptron can be seen as one of the starters). The Perceptron bundled an entire collection of 100% new and ingenious ideas, and it looked like a little brain. Here is a sniplet from the New York Times (“New Navy Device Learns by Doing”, NYT July 8, 1958), after a press conference held by Rosenblatt at the US Office of Naval Research on July 7, 1958 (cited after ):

“The Navy revealed the embryo of an electronic computer today that it expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence. Later perceptrons will be able to recognize people and call out their names and instantly translate speech [...], it was predicted.”

Before you feel immensely superior to the 1958 authors and readership of the New York Times, think about what you, today, think about the powers of deep learning. Brain-like, eh? ... let’s wait a few years ...

The hype about the Perceptron did not last long. In a book titled “Perceptrons”, famous AI pioneers pointed out the obvious: with an architecture that boils down to a simple, thresholded linear combination of input values, one can only learn to classify patterns that are linearly separable. That is, in the input space ℝ^(K) where patterns come from, there must exist an n − 1 dimensional linear hyperplane, dividing ℝ^(K) into two half-spaces, such that all patterns of class 0 lie on one side of the hyperplane and all class-1 patterns on the other side. Specifically, Minsky and Papert shone a flashlight on the embarassing fact that Perceptrons cannot learn the XOR function, because it is not linearly separable (see Figure 9). As a consequence of this simple insight (“perceptrons can’t even learn the simplest Boolean functions”), neural networks become disreputable as an approach to realize machine intelligence, and research on neural networks came to a dead halt and remained frozen — until things happened in the early 1980’s that will be related in the next section.

[The XOR stopper for early neural network research. The training dataset consists of merely 4 examples (u₁, y₁) = ((0, 0), 0), (u₂, y₂) = ((0, 1), 1), (u₃, y₃) = ((1, 0), 1), (u₄, y₄) = ((1, 1), 0), but classifying the (0, 0), (1, 1) patterns in one class and the (0, 1), (1, 0) patterns into another is impossible for a Perceptron, because there is no way to draw a line (like the two thin blue ones shown) in the pattern space ℝ² which separates the two classes, that is, all green points lie on one side of the line and all red ones on the other (right panel).]

Multi-layer perceptrons

After Minsky’s and Papert’s shattering blow, neural networks became disreputable in AI (the field “machine learning” did not exist in the year 1969). But research on neural networks did not come to a complete halt. A little outside AI, at the fringes between neuroscience, signal processing and theoretical physics, researchers secretely dared to continue thinking about neural network models... And in the early 1980’s there was a veritable public eruption of new approaches, today often known by the names of their inventors, for instance

Hopfield networks,

    introduced by theoretical physicist John Hopfield , a model of neural networks that can explain how memory traces (of images, for instance) can be stored, retrieved, and noise-repaired in a suitably designed neural network. Hopfield networks are a standard basic model of neural long-term memory to the day. I will devote one session of this course to them.

Kohonen networks,

    also known as self-organizing (feature) maps (SOMs), introduced by engineer Teuvo Kohonen , are a neural architecture for learning low-dimensional representations of high-dimensional input patterns. The principle of self-organizing maps (or something very similar) is apparently active in the way how the mammalian visual cortex learns to represent geometric features in retinal input patterns. I will not treat Kohonen maps in this course (sadly not enough time), but I devoted Section 4.7 in my machine learning lecture notes to them.

The Boltzmann machine,

    introduced by cognitive neuroscientists Geoffrey Hinton and Terrence Sejnowski (subsequently much more clearly explained in ), is a universal, unsupervised neural learning system which can, in principle, learn every kind of distribution from training data. It is mathematically very transparent but computationally extraordinarily expensive and thus of almost no practical use. Besides producing many other groundbraking innovations to neural network science, Hinton continued to think about the Boltzmann machine and finally found a way to cut down the computational cost in the Restricted Boltzmann Machine (RBM), presented to a wider scientific community in an article in Science . In passing, in this article it is mentioned how RBMs can be used to initialize the training of feedforward neural networks for pattern recognition, — in retrospect, these inconspicuous remarks turned out to be the starting shot for deep learning. I will devote one session to Boltzmann machines in this course.

Besides these three neural network models, many others were conceived in the 1970 / early 1980 years, often inspired by physics, neuroscience, or cognitive science (not inspired by AI, since that field was too successful in its own non-neural ways in those days and too proud to reconsider the disreputed Perceptron). These innovative neural network models helped to understand principles of biological and cognitive neural information processing.

But, none of them was really practically useful.

The tides turned in the year 1986, when a two-volume collection of neural network articles was published, “Parallel Distributed Processing” , of which the first volume soon became known (and still is) as the PDP book. This volume contained a number of well-written, detailed introductions to the innovative NN approaches that had been modestly but steadily sprouting in the shadows outside the glaring light of AI research. Suddenly, neural networks were back on stage with a fanfare.

The immense and lasting impact of the PDP book is largely due to only one of its chapters. In this chapter, give a transparent introduction how neural networks could be designed and trained for pattern classification tasks in ways that are much more powerful than what could be achieved with the Perceptron. The dreaded XOR learning task suddenly was just a piece of cake. The added powers of the multi-layer Perceptrons (MLPs), as the neural networks described in this chapter became generally called, arose from two innovations. First, while the classical Perceptron only has a single “summation-response” unit, in MLPs many of such units are coupled together in parallel and serial connectivity patterns. Second, the original Perceptron learning rule (which can’t be applied to MLPs) was replaced by the error backpropagation (or simply “backprop”) algorithm, which made it possible to iteratively adapt the synaptic weights in an MLP in order to minimize a given loss function — for instance, in order to classify images. It must be mentioned that the backpropagation algorithm was not freshly invented by the authors of the PDP book chapter, but had been described by Paul Werbos much earlier in his 1974 PhD thesis — which did not attract attention and remained ignored; and in other fields of engineering, similar algorithms had been discovered even earlier. , Section 5.5, provides a historical overview.

At any rate, after 1986 neural networks lifted off again. And again, there was a hype that soon withered (I will relate this in the lecture). But the decline of that new hype wasn’t as dramatic as what happened to Perceptrons. In the two decades from 1986 to 2006, MLPs remained a respected citizen of the world of AI and machine learning (this field now had come into being). But not more than that — other methods, like support vector machines or Bayesian networks, were more prominent in machine learning. MLPs worked well as far as it went, but that wasn’t too far after all: roughly speaking, they worked fairly well in not too nonlinear signal processing and pattern recognition tasks. Their economical impact was not striking.

After this short historical review (I find history fascinating), I will describe MLPs in sober detail. Much of the material in this section is taken from my machine learning lecture notes.

MLPs are used for the supervised learning of vectorial input-output tasks, based on training samples S = (u_(i), y_(i))_(i = 1, …, N), where u ∈ ℝ^(K), y ∈ ℝ^(M) are drawn from a joint distribution P_(U, Y).

The MLP is trained to produce outputs y ∈ ℝ^(M) upon inputs u ∈ ℝ^(K) in a way that this input-output mapping is similar to the relationships u_(i) ↦ y_(i) found in the training data. Similarity is measured by a suitable loss function. Several loss functions are today standardly used.

An MLP is a neural network structured equipped with K input units and M output units. A K-dimensional input pattern u can be sent to the input units, then the MLP does some interesting internal processing, at the end of which the m-dimensional result vector of the computation can be read from the m output units. An MLP 𝒩 with K input units and M output units thus instantiates a function 𝒩 : ℝ^(K) → ℝ^(M).

Just like how it worked with the Perceptron, an MLP is defined by its structure (often called architecture) and by the values of its “synaptic connection” weights. The architecture is invented and fixed by the human experimenter / data engineer in a way that should be matched to the given learning task. The architecture is (mostly) not changed during the subsequent training. MLPs have a more complex architecture than the Perceptron. A given neuron in an MLP is typically receiving input from, and sending its output to, many other neurons along synaptic connections. Like in the Perceptron, each such connection is characterized by a weight. These weights are iteratively and incrementally adapted in the training process until the network function 𝒩 : ℝ^(K) → ℝ^(M) comes close to what one desired.

It is customary in machine learning to lump all trainable parameters of a machine learning model together in one parameter vector, which is standardly denoted by θ. For a neural network, θ is thus the vector of all the trainable synaptic connection weights. Since the network function is determined by θ, one also writes 𝒩_(θ) : ℝ^(K) → ℝ^(M) if one wishes to emphasize the dependance of 𝒩’s functionality on its weights.

The learning task is defined by a loss function L : ℝ^(M) × ℝ^(M) → ℝ^( ≥ 0). A convenient and sometimes adequate choice for L is the quadratic loss L(𝒩(u), y) = ∥𝒩(u) − y∥², but other loss functions are also widely used. Chapter 6.2 in the deep learning bible gives an introduction to the theory of which loss functions should be used in which task settings.

Given the loss function, the goal of training an MLP is to find a weight vector  which minimizes the risk, that is
where Θ is a set of candidate weight vectors. This minimization will become a subtask embedded in procedures (like cross-validation schemes) which take care of preventing over- or underfitting. For MLPs, however, finding practically working algorithms for “merely” minimizing the training error is already challenging in itself — as we will learn to appreciate.

MLPs are function approximators. “Function approximation” sounds dry and technical, but many kinds of learning problems can be framed as function approximation learning. Here are some examples:

Pattern recognition:

    inputs u are vectorized representations of any kind of “patterns”, for example images, soundfiles, stock market time series. Training outputs y are binary “one-hot” vectors indicating the class of the input. For instance, a training output vector [0, 1, 0, 0] indicates that the current input came from the second of four possible classes. The trained network will usually not produce clean binary vectors but hypothesis (or confidence) vectors that can be understood as “degrees of belief” that the network has in its classification. For instance, a network output [0.1, 0.7, 0.15, 0.15] would indicate that the network most strongly believes the current input to belong to the second class.

Time series prediction:

    inputs are vector encodings of a past history of a temporal process, outputs are vector encodings of future observations of the process.

Denoising, restoration and pattern completion:

    inputs are patterns that are corrupted by noise or other distortions, outputs are cleaned-up or repaired or completed versions of the same patterns.

Data compression:

    Inputs are high-dimensional patterns, outputs are low-dimensional encodings which can be restored to the original patterns using a decoding MLP. The encoding and decoding MLPs are trained together.

Process control:

    In control tasks the objective is to send control inputs to a technological system (called “plant” in control engineering) such that the system performs in a desired way. The algorithm which computes the control inputs is called a “controller”. Control tasks range in difficulty from almost trivial (like controlling a heater valve such that the room temperature is steered to a desired value) to almost impossible (like operating hundreds of valves and heaters and coolers and whatnots in a chemical factory such that the chemical production process is regulated to optimal quality and yield). The MLP instantiates the controller. Its inputs are settings for the desired plant behavior, plus optionally observation data from the current plant performance. The outputs are the control inputs which are sent to the plant.

This list should convince you that “function approximation” is a worthwhile topic indeed, and spending effort on learning how to properly handle MLPs is a good personal investment for any engineer or data analyst. MLPs are the bread and butter of today’s (deep) learning applications, and that is why I introduce them right at the beginning of this course: knowing about them will enable you to define and solve most sorts of semester projects that you might want to launch.

MLP structure

Figure 10 gives a schematic of the architecture of an MLP. It consists of several layers of units. Layers are numbered 0, …, k, where layer 0 is comprised of the input units and layer k of the output units. The number of units in layer κ is L^(κ). The units of two successive layers are connected in an all-to-all fashion by synaptic links (arrows in Figure 10). The link from unit j in layer κ − 1 to unit i in layer κ has a weight w_(ij)^(κ) ∈ ℝ. The layer 0 is the input layer and the layer k is the output layer. The intermediate layers are called hidden layers. When an MLP is used for a computation, the i-th unit in layer κ will have an activation x_(i)^(κ) ∈ ℝ.

[Schema of an MLP with k − 1 hidden layers of neurons. ]

From a mathematical perspective, an MLP 𝒩 implements a function 𝒩 : ℝ^(L⁰) → ℝ^(L^(k)). Using the MLP and its layered structure, this function 𝒩(u) of an argument u ∈ ℝ^(L₀) is computed by a sequence of transformations as follows:

1.  The activations x_(j)⁰ of the input layer are set to the component values of the L⁰-dimensional input vector u.

2.  For κ < k, assume that the activations x_(j)^(κ − 1) of units in layer κ − 1 have already been computed (or have been externally set to the input values, in the case of κ − 1 = 0). Then the activation x_(i)^(κ) is computed from the formula
    That is, x_(i)^(κ) is obtained from linearly combining the activations of the lower layer with combination weights w_(ij)^(κ), then adding the bias w_(i0)^(κ) ∈ ℝ, then wrapping the obtained sum with the activation function σ. The activation function is a nonlinear, “S-shaped” function which I explain in more detail below. It is customary to interpret the bias w_(i0)^(κ) as the weight of a synaptic link from a special bias unit in layer κ − 1 which always has a constant activation of 1 (as shown in Figure 10).

    Equation [eMLPlayerupdate] can be more conveniently written in matrix form, and most conveniently in a version that treats the bias weights as synaptic weights from bias units. Let x^(κ) = (x₁^(κ), …, x_(L^(κ))^(κ))′ be the activation vector in layer κ, let b^(κ) = (w₁₀^(κ), …, w_(L^(κ)0)^(κ))′ be the vector of bias weights, and let W^(κ) = (w_(ij)^(κ))_(i = 1, …, L^(κ); j = 0, …, L^(κ − 1)) be the connection weight matrix for links between layers κ − 1 and κ (including the bias weights, which become the first column in W). Then [eMLPlayerupdate] becomes
    x^(κ) = σ(W^(κ) x^(κ − 1)),
    where the activation function σ is applied component-wise to the vector W^(κ) x^(κ − 1).

3.  The L^(k)-dimensional activation vector y of the output layer is computed from the activations of the pre-output layer κ = k − 1 in various ways, depending on the task setting (compare Chapter 6.2 in ). For simplicity we will consider the case of linear output units,
    y = x^(k) = W^(k) x^(k − 1),
    that is, in the same way as it was done in the other layers except that no activation function is applied. The output activation vector y is the result y = 𝒩(u).

The activation function σ is traditionally either the hyperbolic tangent (tanh ) function or the logistic sigmoid given by σ(a) = 1/(1 + exp ( − a)). Figure 11 gives plots of these two S-shaped functions. Functions of such shape are often called sigmoids. There are two grand reasons for applying sigmoids:

-   Historically, neural networks were conceived as abstractions of biological neural systems. The electrical activation of a biological neuron is bounded. Applying the tanh  bounds the activations of MLP “neurons” to the interval [ − 1, 1] and the logistic sigmoid to [0, 1]. This can be regarded as an abstract model of a biological property.

-   Sigmoids introduce nonlinearity into the function 𝒩_(θ). Without these sigmoids, 𝒩_(θ) would boil down to a cascade of affine linear transformations, hence in total would be merely an affine linear function. No nonlinear function could be learnt by such a linear MLP.

In the area of deep learning a drastically simplified “sigmoid” is often used, the rectifier function defined by r(a) = 0 for a < 0 and r(a) = a for a ≥ 0. The rectifier has somewhat less pleasing mathematical properties compared to the classical sigmoids but can be computed much more cheaply. This is of great value in deep learning scenarios where the neural networks and the training samples both are often very large and the training process requires very many evaluations of the sigmoid.

[The tanh  (blue), the logistic sigmoid (green), and the rectifier function (red). ]

In intuitive terms, the operation of an MLP can be summarized as follows. After an input vector u is written into the input units, a wave of activation sweeps forward through the layers of the network. The activation vector x^(κ) in each layer κ is directly triggered by the activations x^(κ − 1) according to [eMLPlayerupMatrix]. The data transformation from x^(κ − 1) to x^(κ) is a relatively “mild” one: just an affine linear map W^(κ) x^(κ − 1) followed by a wrapping with the gentle sigmoid σ. But when several such mild transformations are applied in sequence, very complex “foldings” of the input vector u can be effected. Figure 12 gives a visual impression of what a sequence of mild transformations can do.

[Illustrating the power of iterating a simple transformation. The baker transformation (also known as horseshoe transformation) takes a 2-dimensional rectangle, stretches it and folds it back onto itself. The bottom right diagram visualizes a set that is obtained after numerous baker transformations (plus some mild nonlinear distortion). — Diagrams on the right taken from .]

MLPs are a member of a larger class of neural networks, the feedforward neural networks (FFNs). FFNs all have an input layer, an output layer, and in between a directed neural connection network which lets the input activation vector spread forward through the network, undergoing all sorts of transformations, until it reaches the output layer. Importantly, there is never a synaptic connection inside the network that would feed some intermediate activation back into earlier layers. Mathematically speaking, the connection graph of an FNN is cycle free (also called acyclical). All FFNs are therefore representing a function from input vectors to output vectors. Within the class of FNNs, the MLPs are the most simple ones, with all layers having the same basic structure. For a given task, this generic simple MLP architecture may be suboptimal (it very likely is!). Much more sophisticated feedforward architectures have been developed for specific tasks. At the end of this section I will highlight one of these, the convolutional neural networks whose architecture is optimized for image recognition.

It is also possible to design neural networks which host cyclical synaptic connection pathways. In such recurrent neural networks (RNNs), a neuron’s activation can ultimately feed back on the same neuron. Mathematically speaking, RNNs are not representing functions but dynamical systems. This is an upper league of mathematics, far more complex, fascinating, and powerful than just functions. Biological brains are always recurrent, and intelligent reasoning has memory — which means it feeds back on itself recurrently. Most of this course will be about RNNs! Only this Section 2 is about FNNs.

Universal approximation and the powers of being deep 

One reason for the popularity of MLPs is that they can approximate arbitrary functions f : ℝ^(K) → ℝ^(M). Numerous results on the approximation qualities of MLPs have been published in the early 1990-ies. Such theorems have the following general format:

Theorem (schematic). Let ℱ be a certain class of functions f : ℝ^(K) → ℝ^(M). Then for any f ∈ ℱ and any ε > 0 there exists an multilayer perceptron 𝒩 with one hidden layer such that ∥f − 𝒩∥ < ε.

Such theorems differ with respect to the classes ℱ of functions that are approximated and with respect to the norms ∥ ⋅ ∥ that measure the mismatch between two functions. All practically relevant functions belong to classes that are covered by such approximation theorems. In a summary fashion it is claimed that MLPs are universal function approximators. Again, don’t let yourself be misled by the dryness of the word “function approximator”. Concretely the universal function approximation property of MLPs would spell out, for example, to the (proven) statement that any task of classifying pictures can be solved to any degree of perfection by a suitable MLP.

The proofs for such theorems are typically constructive: for some target function f and tolerance ε they explicitly construct an MLP 𝒩 such that ∥f − 𝒩∥ < ε. However, these constructions have little practical value because the constructed MLPs 𝒩 are far too large for practical implementation. You can find more details concerning such approximation theorems and related results in my legacy ML lecture notes https://www.ai.rug.nl/minds/uploads/LN_ML_Fall11.pdf, Section 8.1, and a very instructive online mini-tutorial on how these function-approximation networks can be constructed at http://neuralnetworksanddeeplearning.com/chap4.html (pointed out to me by Satchit Chatterji).

Even when the function f that one wants to train into an MLP is very complex (highly nonlinear and with many “folds”), it can be in principle approximated with 1-hidden-layer MLPs. However, when one employs MLPs that have many hidden layers, the required overall size of the MLP (quantified by total number of weights) is dramatically reduced . Even for super-complex target functions f (like photographic image caption generation), MLPs of feasible size exist when enough layers are used. It is not uncommon for professionally designed and trained deep networks to have 20 or more layers. This is the basic insight and motivation to consider deep networks, which is just another word for “many hidden layers”. Unfortunately it is not at all easy to train deep networks. Traditional learning algorithms had made non-deep (“shallow”) MLPs popular since the 1980-ies. But these shallow MLPs could only cope with relatively well-behaved and simple learning tasks. Attempts to scale up to larger numbers of hidden layers and more complex data sets largely failed, due to numerical instabilities, very slow convergence, or poor model quality. Since about 2006 an accumulation of clever “tricks of the trade” plus the availability of affordable powerful (GPU-based) computing hardware has overcome these hurdles.

Training an MLP with the backpropagation algorithm

In this section I give an overview of the main steps in training a non-deep MLP for a mildly nonlinear task — tasks that can be solved with one or two hidden layers. When it comes to unleash deep networks on gigantic training data sets for hypercomplex tasks, the basic recipes given in this section will not suffice. You would need to train yourself first in the art of deep learning. However, what you can learn in this section is necessary background for surviving in the wilderness of deep learning.

General outline of an MLP training project.

Starting from a task specification and access to training data (given to you by your boss or a paying customer, with the task requirements likely spelled out in rather informal terms, like “please predict the load of our internet servers 3 minutes ahead”), a basic training procedure for a elementary MLP 𝒩 goes like follows.

1. Get a clear idea of the formal nature of your learning task.

    Do you want a model output that is a probability vector? or a binary decision? or a maximally precise transformation of the input? how should “precision” best be measured? and so forth. Only proceed with using MLPs if they are really looking like a suitable model class for your problem.

2. Decide on a loss function.

    Go for the simple quadratic loss if you want a quick baseline solution but be prepared to invest in other loss functions if you have enough time and knowledge (read Chapter 6.2 in ).

3. Decide on a regularization method.

    For elementary MLP training, suitable candidates are adding an L₂ norm regularizer to your loss function; extending the size of the training data set by adding noisy / distorted exemplars; varying the network size; or early stopping (= stop gradient descent training a overfitting-enabled MLP when the validation error starts increasing — requires continual validation during gradient descent). is a solid guide for MLP regularization.

4. Think of a suitable vector encoding of the available training data,

    including dimension reduction if that seems advisable (it is advisable in all situations where the ratio raw data dimension / size of training data set is high).

5. Fix an MLP architecture.

    Decide how many hidden layers the MLP shall have, how many units each layer shall have, what kind of sigmoid is used and what kind of output function and loss function. The structure should be rich enough that data overfitting becomes possible and your regularization method can kick in.

6. Set up a cross-validation scheme

    in order to optimize the generalization performance of your MLPs. Recommended: implement a semi-automated k-fold cross-validation scheme which is built around two subroutines, (1) “train an MLP of certain structure for minimal training error on given training data”, (2) “test MLP on validation data”. Note: for large MLPs and/or large training data sets, something like k-fold cross-validation will likely be too expensive. In deep learning, where a single learning run may take two weeks, one uses the cheapest possible cross-validation scheme, called “early stopping”: while the iterations of the model optimization are running, assess the generalization qualities of the model in its current learning state on a validation data set. If the validation error starts to rise again after having first dropped (see THE blue curve in Figure 6), stop the model adaptation and declare your model ready for use.

7. Implement the training and testing subroutines.

    The training will be done with a suitable version of the error backpropagation algorithm, which will require you to meddle with some global control parameters (like learning rate, stopping criterion, initialization scheme, and more).

8. Do the job.

    Enjoy the powers, and marvel at the wickedness, of MLPs.

You see that “neural network training” is a multi-faceted thing and requires from you to consider all the issues that always jump at you in supervised machine learning. It will not miraculously give good results just because it’s “neural networks inside”. The actual “learning” part, namely solving the optimization task [eMLPloss], is only a subtask, albeit a conspicuous one because it is done with an algorithm that has risen to fame.

Iterative model optimization: general principle

This famous algorithm is, of course, the backpropagation algorithm. It is an algorithm for performing a gradient descent minimization. I will first rehearse the general principles and terminology of model optimization through gradient descent. Here is the set-up:

Given:

    Training data (u_(i), y_(i))_(i = 1, …, N) for a supervised learning task, as usual, where u_(i) ∈ ℝ^(K), y_(i) ∈ ℝ^(M).

Given:

    A fixed model architecture parametrized by trainable parameters collected in a vector θ. In our situation, the architecture would be an MLP with fixed structure, and θ would be containing all the synaptic connection weights in the MLP 𝒩_(θ).

    The set of all possible models (with the same architecture, distinguished from each other only by different weights) can be identified with a set Θ of all possible weight vectors θ. Θ gives us the search space for a good model. If the chosen architecture has D trainable weights, this search space would be Θ = ℝ^(D).

Given:

    A loss function L : ℝ^(M) × ℝ^(M) → ℝ^( ≥ 0).

Wanted:

    An optimal model
. The model θ^((n + 1)) is computed by an incremental modification of the previous model θ^((n)). The first model θ⁽⁰⁾ is a “guess” provided by the experimenter.

The hope is that this series converges to a model θ^((∞)) = lim_(n → ∞)θ^((n)) whose empirical risk is close to the minimal possible empirical risk.

Machine learning research (and mathematics and optimization theory in general) has found a number of quite different principles to design iterative model adaptation procedures which lead to a decreasing risk sequence. Examples are the family of Expectation-Maximization algorithms (explained in my machine learning lecture notes if you are interested), or iteration schemes that exploit stability conditions for fixed points of a map; or various sorts of general-purpose stochastic search algorithms like genetic algorithms or simulated annealing (we will study the latter in Section 6.4). Or, finally — gradient descent algorithms. That is the kind of iterative model optimization algorithm that is typically used with neural networks (although, as we will see later in this course, by no means the only one, and also most likely not the one used by Mother Nature to make our brains learn). Gradient descent algorithms come in many degrees of sophistication. Here I discuss the plain vanilla kind.

Performance surfaces.

First a quick recap: the graph of a function. Recall that the graph of a function f : A → B is the set  of all argument-value pairs of the function. For instance, the graph of the square function f : ℝ → ℝ,  x ↦ x² is the familiar parabola curve in ℝ².

A performance surface is the graph of a risk function. Depending on the specific situation, the risk function may be the empirical risk, the risk, or some other “cost”. I will use the symbol ℛ to denote a generic risk function of whatever kind.

Performance surfaces are typically discussed with parametric model families where a candidate set of models can be identified with a candidate set Θ of parameter vectors θ. The performance surface then becomes the graph of the function ℛ : Θ → ℝ^( ≥ 0) which maps each model θ to its risk.

Specifically, if we have D-dimensional parameter vectors (that is, Θ ⊆ ℝ^(D)), the performance surface is a M-dimensional hypersurface in ℝ^(D + 1).

Other terms are variously used for performance surfaces, for instance performance landscape or error landscape or error surface or cost landscape or similar wordings.

Performance surfaces can be objects of stunning complexity. In neural network training (stay tuned ... we will come to that! I promise!), they are tremendously complex. Figure 13 shows a neural network error landscape randomly picked from the web.

[A (2-dimensional cross-section of) a performance surface for a neural network. The performance landscape shows the variation of the loss when (merely) 2 weights in the network are varied. Try to imagine the loss function if it were “plotted” across not only two, but thousands or millions of parameters! Source: http://www.telesens.co/2019/01/16/neural-network-loss-visualization/ ]

Gradient descent on a performance surface

Often a risk function ℛ : Θ → ℝ^( ≥ 0) is differentiable. Then, for every θ ∈ Θ the gradient of ℛ with respect to θ = (θ₁, …, θ_(D))′ is defined:

The gradient ∇ℛ(θ) is the vector which points from θ in the direction of the steepest ascent (“uphill”) of the performance surface. The negative gradient  − ∇ℛ(θ) is the direction of the steepest descent (Figure 14).

[A performance surface for a 2-dimensional model family with parameters θ₁, θ₂, with its contour plot at the bottom. For a model θ (yellow star in contour plot) the negative gradient is shown as black solid arrow. It marks the direction of steepest descent (broken black arrow) on the performance surface. ]

The idea of model optimization by gradient descent is to iteratively move toward a minimal-risk solution θ^((∞)) = lim_(n → ∞)θ^((n)) by always “sliding downhill” in the direction of steepest descent, starting from an initial model θ⁽⁰⁾. This idea is as natural and compelling as can be. Figure 15 shows one such itinerary.

[A gradient descent itinerary, re-using the contour map from Figure 14 and starting from the initial point shown in that figure. Notice the variable jump length and the sad fact that from this initial model θ⁽⁰⁾ the global minimum is missed. Instead, the itinerary slides toward a local minimum at θ^((∞)). The blue arrows show the negative gradient at raster points. They are perpendicular to the contour lines and their size is inversely proportional to the spacing between the contour lines. ]

The general recipe for iterative gradient descent learning goes like this:

Given:

    A differentiable risk function ℛ : Θ → ℝ^( ≥ 0).

Wanted:

    A minimal-risk model

Start:

    Guess an initial model θ⁽⁰⁾.

Iterate until convergence:

    Compute models
    θ^((n)) = θ^((n − 1)) − μ ∇ℛ(θ^((n − 1))).

    The adaptation rate (or learning rate) μ is set to a small positive value.

An obvious weakness of this elegant and natural approach is that the final model θ^((∞)) depends on the choice of the initial model θ⁽⁰⁾. In complex risk landscapes (as the one shown in Figure 13) there is no hope of guessing an initial model which guarantees to end in the global minimum. This circumstance is generally perceived and accepted. There is a substantial mathematical literature that amounts to “if the initial model is chosen with a good heuristic, the local minimum that will be reached will be a rather good one with high probability”.

Gradient descent: stability vs. speed of convergence.

Neural networks — whether old-fashioned Tin Lizzie (look it up on Wikipedia) MLPs or the deep Ferraris of our days — are trained by gradient descent on a performance landscape. While gradient descent seems simple and fool-proof, things actually can get out of hand quickly and wildly. The situation shown in Figure 15 looks deceptively simple; don’t believe that gradient descent works as smoothly as that in real-life applications. In fact, the difficulties are so severe that during the years 1986-2006 between the re-birth of neural networks and the beginning of the deep learning era, they were essentially unsurmountable and forced the neural network community to stay satiesfied with shallow MLPs with 1 or 2 hidden layers.

I should however mention that not everybody in the field was entirely helpless. In particular, Yann LeCun ploughed steadily forward and created a series of better and better and deeper and deeper neural networks for image classification, developing the architecture of convolutional neural networks on his way , and being more able than anybody else to train these beasts by gradient descent. However, the techniques that he applied were so subtle and not 100% documented that other researchers were not generally able to reliably reproduce his achievements.

I will now outline one of the most painful show stoppers for simpleminded gradient descent.

First I note the obvious: differentiable performance surfaces are ususally not linear but curved. This simple fact opens the doors for trouble.

In order to understand why curvature makes gradient descent difficult, I consider a special situation where the main argument comes to the surface most clearly. Assume that your gradient descent optimization already has brought you into the close neighborhood of a local minimum, like the point θ^((∞)) indicated in Figure 15. In the neighborhood of such a local minimum, the curved shape of the performance surface ℛ(θ) can be approximated by the first and second order terms of the Taylor expansion of ℛ(θ) around θ^((∞)). Allow me to skip some maths here (you find it detailed in the ML lecture notes, Section 11.3.4) which amounts to a sequence of coordinate transformations. These coordinate transformations simplify the picture to the scenario shown in Figure 16.

This graphics shows a contour plot of a performance surface over a two-dimensional parameter space Θ = ℝ² after coordinate transformations that move the local minimum θ^((∞)) to the origin and rotate the entire coordinate system such that the main axes of the “trough” of the surface are aligned with the drawing axes.

Assume that during a gradient descent run you have arrived at some parameter vector θ^((n)). In order to determine the next parameter vector θ^((n + 1)) = θ^((n)) − μ ∇ℛ(θ^((n))), you have to decide on a learning rate μ. If you choose μ too large (red arrow in Figure 16), you will jump forward along the direction of the negative gradient so far that you end up “on the other side of the valley” higher than you started — that is, the risk ℛ(θ^((n + 1))) at the next point is larger, not smaller, than at θ^((n)). If you would continue to use this too large learning rate even through the next iterations, you would propel yourself upwards out of the “trough”. The gradient descent algorithm has become instable.

In order to prevent this instability and divergence, the learning rate must be set to a sufficiently small value. Then (indicated by the green arrow in the figure) the gradient descent will continue moving downwards, stably reducing the risk at every step.

[Gradient descent in the neighborhood of a local minimium θ^((n)) (centered at the origin of a shifted coordinate system). Red arrow: gradient descent overshoots with too large learning rate. Green arrow: learning rate small enough for stability. Magenta arrow: weight change direction when a second-order gradient descent method would be used. For more explanation see text. ]

The price to pay for stability is slow convergence. If the safe small learning rate is kept for future iterations, the speed of approach to θ^((∞)) becomes very slow as soon as the iterations reach the bottom of the “valley” (shaded dots in the figure). But you cannot switch to a larger learning rate: the algorithm would immediately become instable.

What does it mean, concretely, for a learning rate to be “too large” such that gradient descent becomes instable? The maths here are actually not very involved (detailed in the machine learning lecture notes). Here I give a summary of the main insights:

-   After the coordinate transformations that were used to produce the situation shown in Figure 16, the geometric shape of the performance landscape (in its Taylor approximation up to order 2) is given by the formula
    
    where θ = (θ₁, …, θ_(D))′ and the quadratic terms λ_(i) θ_(i)² give the shape of the parabolic cross-sections through the performance surface along the axes θ_(i). All λ_(i) are non-negative.

-   In order to prevent instability, the learning rate must be set to a value smaller than  is the largest among the coefficients λ_(i).

Bad news: the more layers a network has, the more common it is that the ratio 
          min

Besides this special case of convergence close to a minimum, there are other geometric scenarios at other places on a performance surface (e.g., saddle points of the performance landscape) which lead to similar inherent conflicts between stability and speed of convergence.

One escape from the instability – slowness dilemma is to go for second-order gradient descent methods. These methods determine the direction vector for the next weigth adaption θ^((n)) → θ^((n + 1)) based not only on the gradient, but also on the curvature of the performance surface at the current model θ^((n)). This curvature is given by the D × D sized Hessian matrix which contains the second-order partial derivatives ∂² ℛ /∂θ_(i) ∂θ_(j). In ideal scenarios (when the second-order Taylor approximation is precise), second-order gradient descent points into exactly the direction to the target minimum (magenta arrow in Figure 16). However, computing the Hessian is expensive and may be subject to numerical problems, and the preconditions for making second-order methods work well (good approximation of surface by second-order Taylor expansion; being close to local minimum) may be badly violated. Numerous variants of second-order methods have been developed, aiming at reduced costs or more robust coping with local geometry. There is unfortunately no general rule of when or whether it is beneficial to use what type of second-order method.

These difficulties are severe. For two decades they had made it impossible to effectively train MLPs on advanced “cognitive” tasks in image interpretation and language processing. Finding heuristic mechanisms to modify plain gradient descent in ways that keeps the iterations stable, while maintaining a sufficient speed of convergence, has been one of the enabling factors for deep learning.

The classical error backpropagation algorithm

I will now describe the classical algorithm for training MLPs by plain gradient-descent — the error backpropagation algorithm.

Given:

    Labelled training data in vector/vector format (obtained possibly after preprocessing raw data) (u_(i), y_(i))_(i = 1, …, N), where u ∈ ℝ^(K), y ∈ ℝ^(M). Also given: a network architecture for models 𝒩_(θ) that can be specified by M-dimensional weight vectors θ ∈ Θ. Also given: a loss function L : ℝ^(M) × ℝ^(M) → ℝ^( ≥ 0).

Initialization:

    Choose an initial model θ⁽⁰⁾. This needs an educated guess. A widely used strategy is to set all weight parameters w_(i)⁽⁰⁾ ∈ θ⁽⁰⁾ to small random values. Remark: For training deep neural networks this is not good enough — the deep learning field actually got kickstarted by a clever method for finding a good model initialization .

Iterate:

    Compute a series (θ^((n)))_(n = 0, 1, …) of models of decreasing empirical loss (aka training error) by gradient descent. Concretely, with
   
    In these iterations, make sure that the adaptation rate μ is small enough to guarantee stability.

Stop

    when a stopping criterion chosen by you is met. This can be reaching a maximum number of iterations, or the empirical risk decrease falling under a predermined threshold, or some early stopping scheme.

Computing a gradient is a differentiation task, and differentiation is easy. With high-school maths you would be able to compute [enablaMLP]. However, the gradient formula that you get from textbook recipes would be too expensive to compute. The error backpropagation algorithm (or simply “backprop” for the initiated, or even just BP if you want to have a feeling of belonging to this select community) is a specific algorithmic scheme to compute this gradient in a computationally efficient way.

Every student of machine learning must have understood it in detail at least once in his/her life, even if later it’s just downloaded from a toolbox in some more sophisticated fashioning. Thus, brace yourself and follow along!

Let us take a closer look at the empirical risk [empRiskMLP]. Its gradient can be written as a sum of gradients
and this is also how it is actually computed: the gradient ∇L(𝒩_(θ)(u_(i)), y_(i)) is evaluated for each training example (u_(i), y_(i)) and the obtained N gradients are averaged.

This means that at every gradient descent iteration θ^((n)) → θ^((n + 1)), all training data points have to be visited individually. In MLP parlance, such a sweep through all data points is called an epoch. In the neural network literature one finds statements like “the training was done for 120 epochs”, which means that 120 average gradients were computed, and for each of these computations, N gradients for individual training example points (u_(i), y_(i)) were computed.

When training samples are large — as they should be — one epoch can clearly be too expensive. Therefore one often takes resort to minibatch training, where for each gradient descent iteration only a subset of the total training sample S is used.

The backpropagation algorithm is a subroutine in the gradient descent game. It is a particular algorithmic scheme for calculating the gradient ∇L(𝒩_(θ)(u_(i)), y_(i)) for a single data point (u_(i), y_(i)). Naive highschool calculations of this quantity incur a cost of O(D²) (where D is the number of network weights). When D is not extremely small (it will almost never be extremely small — a few hundreds of weights will be needed for simple tasks, and easily a million for deep networks), this cost O(D²) is too high for practical exploits (and it has to be paid N times in a single gradient descent step!). The backprop algorithm is a clever scheme for computing and storing certain auxiliary quantities which cuts down the cost from O(D²) to O(D).

Here is how backprop works in order to compute the loss gradient ∇ L(𝒩_(θ)(u), y) for a training example (u, y).

1.  BP works in two stages. In the first stage, called the forward pass, the current network 𝒩_(θ) is presented with the input u and the output  is computed using the “forward” formulas [eMLPlayerupMatrix] and [eOutFromForward]. During this forward pass, for each unit x_(i)^(κ) which is not a bias unit and not an input unit the quantity
    a_(i)^(κ) = ∑_(j = 0, …, L^(κ − 1))w_(ij)^(κ) x_(j)^(κ − 1)
    is computed and stored – this is sometimes referred to as the potential of unit x_(i)^(κ), that is its internal state before it is passed through the sigmoid.

2.  A little math in between. Applying the chain rule of calculus we have
    Thus, in order to calculate the desired derivatives [eBPChainRule], we only need to compute the values of δ_(i)^(κ) for each hidden and output unit.

3.  Computing the δ’s for output units. Output units x_(i)^(k) are typically set up differently from hidden units, and their corresponding δ values must be computed in ways that depend on the special architecture. For concreteness here I stick with the simple linear units introduced in [eOutFromForward]. The potentials a_(i)^(k) are thus identical to the output values ŷ_(i) and we obtain

4.  Computing the δ’s for hidden units. In order to compute δ_(i)^(κ) for 1 ≤ κ < k we again make use of the chain rule. We find
    This formula describes how the δ_(i)^(κ) in a hidden layer can be computed by “back-propagating” the δ_(l)^(κ + 1) from the next higher layer. The formula can be used to compute all δ’s, starting from the output layer (where [edeltasInOutput] is used — in the special case of a quadratic loss, Equation [edeltasInOutputQuadLoss]), and then working backwards through the network in the backward pass of the algorithm.

    When the logistic sigmoid σ(a) = 1/(1 + exp ( − a)) is used, the computation of the derivative σ′(a_(i)^(κ)) takes a particularly simple form, observing that for this sigmoid it holds that σ′(a) = σ(a) (1 − σ(a)), which leads to
    σ′(a_(i)^(κ)) = x_(i)^(κ) (1 − x_(i)^(κ)).

Although simple in principle, and readily implemented, using the backprop algorithm appropriately is something of an art, even in basic shallow MLP training. Here is only place to hint at some difficulties:

-   The stepsize μ must be chosen sufficiently small in order to avoid instabilities. But it also should be set as large as possible to speed up the convergence. We have inspected the inevitable conflict between these two requirements above. Generally one uses adaptation schemes that modulate the learning rate as the gradient descent proceeds. Clever methods for online adjustment of stepsize have been one of the enabling factors for deep learning. For shallow MLPs typical stepsizes that can be fixed without much thinking are in the order from 0.001 to 0.01.

-   Gradient descent on nonlinear performance landscapes may sometimes be crippling slow in “plateau” areas still far away from the next local minimum where the gradient is small in all directions, for other reasons than what we have argued above with the help of Taylor expansions.

-   Gradient-descent techniques on performance landscapes can only find a local minimum of the risk function. This problem can be addressed by various measures, all of which are computationally expensive. For deep networks of large size the local minimum problem seems not particularly problematic. These networks afford of such a brute overfitting potential that on the downhill slide along the negative gradient, the point of overfitting and thus “early stopping” is reached earlier than the local minimum one is heading to. Or, in other words: most local minima represent overfitting models, thus one does not want to reach them.

A truly beautiful visualization of MLP training has been pointed out to me by Rubin Dellialisi: https://playground.tensorflow.org/.

Simple gradient descent with the BP algorithm, as described above, is cheaply computed but may take long to converge and/or run into stability issues. A large variety of more sophisticated iterative loss minimization methods have been developed in the deep learning field, which often succeed in combining an acceptable speed of convergence with stability. Some of them refine gradient descent, others (“second-order” methods) use information from the local curvature of the performance surface. The main alternatives are nicely described in https://www.neuraldesigner.com/blog/5_algorithms_to_train_a_neural_network (retrieved May 2017, local copy at https://www.ai.rug.nl/minds/uploads/NNalgs.zip).

A glimpse at deep learning

Today, neural networks in machine learning means “deep learning” (DL). In my opinion deep learning is a technological revolution like the invention of the steam engine or the transistor. Due to its glamour and enormous industrial funding, deep learning research has spell-bound almost all of the best young ML scientists in the last five to ten years or so. The field is extremely productive and has enormously diversified from its early days when it could be defined as “successfully handling MLPs with more than 3 hidden layers”. This course cannot give a serious introduction to DL. The standard textbook is .

Convolutional neural networks

But I will allow ourselves at least a glimpse at DL, by taking a closer look at convolutional neural networks (aka convnets, or CNNs). This is a deep architecture, pioneered in the 1990’s by Yann LeCun (http://yann.lecun.com/ — this personal academic website of his seems not to have been updated since about six years; this is understandable given that LeCun’s leading roles in DL have propelled him to become VP and Chief AI Scientist of Facebook). However, as almost always in science, LeCun did not invent CNNs out of thin air. Some 15 years before LeCun’s models, the main structural elements of CNN architectures had already been assembled in the Neocognitron of , a historical fact that was properly acknowledged by LeCun but today is rarely mentioned. The Neocognitron was not trained in a supervised way (the backpropo algorithm was no public knowledge then) but with a biologically inspired unsupervised learning algorithm. This condition, combined with the limited compute powers of those days and the fact that Fukushima’s scientific goals were in biological modeling (not machine learning), effected that the Neocognitron has remained under-appreciated in the machine learning community. In turn, Fukushima drew inspiration, and adopted some terminology, from Hubel and Wiesel’s Nobel prize winning research on the early visual processing in the mammalian brain in the early 1960’s.

[Architecture of LeCun’s LeNet 5 convolutional neural network for handwritten character recognition. For explanation see text. Image adapted from ]

The architecture of CNNs is specialized for image processing tasks. Figure 17 sketches the architecture of LeNet 5, the acclaimed common ancestor of all modern deep learning CNNs (see the Wikipedia article https://en.wikipedia.org/wiki/LeNet for detail). The defining feature of CNNs is that they implement a repeated sequence of convolutional and subsampling (aka pooling) layers. The input layer is a 2-dimensional “retina” on which pixel images can be projected as external inputs. Following the schema in Figure 17, I give a brief explanation of the next processing layers (very readable detailed description in the original paper ):

-   The first processing layer C1 after the retina is made of 6 feature maps. Each feature map is a 2-dimensional array of feature detector neurons. Each such feature detector neuron receives its input from a 5 × 5 sized receptive field in the retina. Thus, each such neuron has 25 (plus 1 bias) weights that need to be trained. Within each of the 6 feature maps, every feature detector neuron however has the same weights as all other neurons in that feature map. This weight sharing (aka weight tying) is imposed during the backpropagation training. Mathematically, the transformation from the raw input image to a feature map is a convolution of the image array with the 5 × 5 weight filter, giving this kind of networks its name.

-   The second processing layer S2 reduces the 28 × 28 size of the C1 feature maps to a size of 14 × 14, by creating for each of the six C1 feature maps a subsampled feature map. The neurons in these subsampled maps have 2 × 2 receptive fields in the C1 feature maps. In LeNet5, the four activations in each of the receptive fields are first averaged, then passed to the receiving subsampling neuron with a trainable weight and bias (also with weight sharing imposed within each of the S2 maps).

-   The next layer C3 is again a convolutional layer. Each of its feature maps (of which there are 16 in LeNet5) takes its input from several 5 × 5 sized receptive fields, located in several of the 6 subsampling layers. In Figure 17, I indicated one neuron in a C3 feature map receiving input from two 5 × 5 sized receptive fields.

-   Then, like what was done from C1 to S2, these 16 feature maps are subsampled into 16 subsampled S4 maps.

After these convolution-subsampling layers, LeNet5 has two further MLP-like processing layers and a final output layer which uses a neuron model that is different from the standard summation-sigmoid model [eMLPlayerupdate].

The background ideas that motivated LeCun (and before him, Fukushima) to think out this CNN design:

-   The local feature extraction is biologically inspired;

-   the weight sharing reduces the number of trainable parameters, which leads to a regularization effect in the sense of machine learning;

-   the weight sharing also favors translational invariance for recognition performance (for instance, a local feature which detects a sharp edge like at the top of the letter A would become activated insensitive to shifts of the position on the input letter A on the retina);

-   the subsampling further increases the robustness of the classification performance against input pattern shifts and scalings.

Convnets that were later developed in the DL era had more convolutional layers and subsequent processing stages which are more complex than in their ancestor LeNet5. is an online overview of later CNN architectures.

CNNs can also be designed for 1-dimensional “images”. This makes sense for input data vectors u = (u₁, …, u_(K))′ where the ordering of vector elements has a geometrical meaning. This would be expected, for instance, when the inputs u are energy profiles over a frequency spectrum, or flow velocity measurements along a flow sensor line as in a recent cum laude PhD thesis written in the AI department . In contrast, it would not make any sense whatsoever, for instance, to throw a 1-dim CNN at inputs that are bag-of-words count vectors!

The recklessness of DL: end-to-end training.

A common denominator of today’s deep learning architectures is that they are trained by some version of gradient descent in a supervised learning set-up (well, that is a simplification... there are also unsupervised learning set-ups like auto-encoder learning , semi-supervised learning set-ups , reinforcement learning additions to DL set-ups , or the generative adversarial training principle (tutorial for generative adversarial networks, GANs: )... but ok., well, even today, gradient descent optimization in supervised settings is a core component of DL).

These learning architectures are in most cases, but not always, construed as neural networks of some sort (a notable exception: the Turing-machine inspired, “hybrid computing” architecture of which combines a “controller” neural network with a non-neural memory subsystem which is reminiscent of the tape of a Turing machine). The field is called “deep learning” because the learning architectures typically pull their input through a sequence of many processing layers, sometimes through different submodules which operate in parallel. At any rate, the input patterns become transformed many times before they reach the output interface. All of these transformations are differentiable, making the entire, deep input-to-output transformation differentiable. The successes of DL rest on three enabling conditions, which became available only in the last decade or so:

-   access to a lot of GPU power — affordable hardware and convenient programming libraries,

-   huge datasets for training,

-   and an ever-growing collection of computational “tricks of the trade” to put the gradient-descent monster in shackles.

A catchphrase that one often hears in connection with DL is end-to-end training. This is not a precisely defined concept. In a narrow technical sense, it describes the fact that modern backprop makes it possible to propagate the errors obtained at the output “end” all the way back through the many processing layers until the input “end” is reached. But often the phrase “end to end” is meant to carry a much stronger, bold idea: namely, that DL architectures can learn their tasks on the basis of being fed with the un-preprocessed “raw” input data from real-world data sources.

From the perspective of conventional machine learning wisdom, this is a very reckless attitude. In my machine learning lecture notes I spend a great effort on explaining how important and helpful it is to pre-process “raw” data for dimension reduction, extracting features which are specifically appropriate for the task at hand. Using raw, high-dimensional data is the way to failure, for a number of good reasons that I unfold in those lecture notes.

So, — whom should you believe and follow?

Answer: both views are right — but in different sitations, not at the same time. The critical issue is overfitting.

Flooding a deep learning system with high-dimensional raw data and training it “end to end” will work if (and only if) one has large quantities of data. Together with the clever modern regularization methods for deep networks (plus, possibly, GAN methods), large data volumes minimize the dangers of overfitting. Such convenient conditions (very large data sets, plus high-performance computing facilities and a lot of professional expertise) is what Google, Facebook and their likes can enjoy.

However, the ordinary mortal machine learning professional most often will only dispose of scarce training data, while nonetheless wanting to solve a complex task. A typical non-Google setting is medical image processing. It is a notorious problem in this field that there are never enough training examples. Yet, the targeted input-output tasks (like cancer diagnostics from liver CT scans) are eminently complex. Training a deep CNN “end to end” with a few hundreds of unpreprocessed CT images will not work. Complex data processing pipelines, only some of them being gradient-descent trained neural networks and others informed by human insight for appropriate feature definition, will be necessary to come up with good solutions (for example, ).

A short visit in the wonderland of dynamical systems

If one wants to design artificial cognitive systems, or if one wants to understand natural cognitive systems, there is no way to not consider time as a fundamental dimension of cognition. Seeing, hearing, reasoning, dreaming, speaking, hallucinating, feeling... can you imagine any of these happening not in time?

Feedforward networks are, mathematically speaking, functions. They map an input pattern to an output pattern – a static argument-value relationship. No time involved.

Biological brains are not feedfoward networks but recurrent networks (RNNs). A recurrent network is a neural network that has at least one connection feedback cycle. I will now demonstrate that neural network science isn’t easy. Consider a simple RNN made from only two neurons that are recurrently connected with each other and each of them also with itself (“self-feedback”). Not giving this system any external input, nor a bias, its state update equation is
x(n + 1) = tanh (W x(n)),
where x(n) is the 2-dimensional neural activation vector at time n = 0, 1, 2, … and W is the 2 × 2 weight matrix of the synaptic connection weights which feeds the activations x(n) of the two neurons back to determine the next state x(n + 1). We set

and also consider W₂ = 3.3 ⋅ W₁, W₃ = 4.3 ⋅ W₁, that is, W₂, W₃ are just scaled versions of W₁. We carry out three simulation runs where we iterate [eSingleNeuronSelffeeding] for 1000 timesteps. In the first run we use W₁, in the second run we use W₂, in the third run we use W₃, plugging them into [eSingleNeuronSelffeeding]. Each run is startet at time n = 0 from some arbitrary random initial state x(0). After a short initial transient where the arbitrary initial state is “forgotten” (or “washed out”), the temporal sequence x(n) stabilizes into a pattern which characteristically depends on W₁, W₂ or W₃, respectively. Figure 18 shows what we get:

-   In the run using W₁, all activity “dies out” and the network gets locked in a stable, unchanging state x(n) = x(n + 1) = ….

-   With W₂, the network starts to oscillate, here with a period 4, that is x(n) = x(n + 4) = x(n + 8) = …. Other periods (powers of 2) could also be obtained by using other scalings of W₁, not shown.

-   When the weight matrix is scaled above some critical value (which here is about 3.55), all regularity breaks down and the behavior becomes chaotic. There are no periodicities; patterns will never repeat exactly.

[Running the 2-neuron network with weight matrices W₁ (blue, top plot and first plot in bottom row), W₂ = 3.3 ⋅ W₁ (green, second plot and second panel in bottom row), and W₃ = 4.3 ⋅ W₁ (red, third plot and last panel in bottom row). The last 50 time steps from the 1000 step simulation are shown. First three plots show the development of the activations of the two neurons plotted against time. The panels in the bottom row give another visualization of the same traces: here the two axes of the panel correspond to the two components x₁, x₂ of the state vectors x(n), giving a 2-dimensional representation of the temporal evolution. ]

Biological neural networks exhibit the same kinds of characteristics. Stable states occur, for instance, in your motor control circuits of your neck muscles when you keep your head steadily upright. Periodic activation sequences occur extremely often — for instance in the neural ganglia, embedded on the surface of your heart, which generate heartbeat signals; or in central pattern generator neural networks in your spine that generate a basic walking pattern for your leg muscles; or when you memorize a number by mentally repeating it; or in an epileptic seizure. Numerous occasions have been hypothetically proposed for chaotic activation patterns, for instance as being the carrier of complex sensory representations or as the background activation in an awake, alert, resting state.

The dizzy dancing of just two coupled neurons already is hard to follow even with a mathematician’s eye. But that’s only 2 (two) neurons. You own about 85 billion neurons (or they own you). The door to understanding their dance is the study of recurrent neural networks. And the key to open that door is the theory of dynamical systems (DS). In this section I will treat you to a crash course in DS essentials.

What is a “dynamical system”?

Short answer: everything that exists.

Because “existing” happens in time and anything that happens in time is a dynamical system - by definition of that term.
Long answer: given in the Encyclopedia of Complexity and Systems Science . Just buy (5200 Euros) and read (10,000+ pages).

[Some (only some) sorts and aspects of dynamical systems. Each item comes with its own collection of dedicated mathematical formalisms. ]

Physicists and mathematicians have been developing formalisms for modeling dynamical systems (DSs) since several hundred years, followed in the 20th century by theoretical biologists, control engineers, economists, neuroscientists and machine learners. Since everything under the sun is a dynamical system, it is clear that there cannot be a single unified mathematical theory of dynamical systems — it would be a theory of everything and as complex as the mystifying world around us. Formal models of dynamical systems thus always focus on a particular class of real-world (or computer-simulated) systems and analyse a particular aspect of them. Figure 19 gives a coarse overview of main aspects that have been studied.

Since brains are made to understand the world, brains have to try to capture all aspects of the world; and since brains are made from recurrent neural networks, recurrent neural networks are — or ultimately will be, if evolution carries on — as complex as the real world. Thus, all the aspects of dynamical systems listed in Figure 19, plus many more, are relevant for the study of recurrent neural networks. Close your eyes and look inside...

[Basic concepts for describing a dynamical system. ]

It is clear that in this course we can only scratch the surface at a few places. I start with introducing some basic concepts and terminology (summarized in Figure 20):

Time.

    There are two main formalizations of the arrow of time. Discrete time jumps forward in “time steps” or “increments”, mathematically captured by segments of the integers ℤ. A discrete-time DS model can be specified for the entire timeline ℤ with infinite past and future, or it can be started (typically at time n = 0) and run forever which yields a timeline equal to ℕ, or it can be modeled for a finite interval [0, 1, …, N]. In formulas one mostly uses the symbol n to denote discrete time points. Figure 20 is based on a discrete-time view. The main alternative is continuous time where time points are real numbers, typically denoted by the symbol t ∈ ℝ.

    One often says that a DS evolves, and one speaks of the evolution of a DS. This is a figure of speech that has nothing to do with biological evolution. It simply means that the system changes its states as time goes on. Even if one monitors a simple pendulum, physicists or mathematicians will speak of the temporal evolution of this system, meaning nothing more fancy than that the pendulum simply swings.

States.

    A fundamental assumption, if not dogma, of physics is that at any time t, a physical system is in a state, often denoted by x(t). Many different mathematical objects can serve as states – in fact, any sort of mathematial object can be used when modeling a system state. The most common state models in physics are real-valued or complex-valued state vectors (examples: the state of a pendulum is given by the 2-dimensional vector x(t) = (ω(t), ω̇(t))′ where ω(t) is the current angle and ω̇(t) is the current angular velocity; or the state of digital microchip is a binary vector that indicates which of the logic gates on the chip are currently switched on or off) or functions (example: a function w(t) : [0, 1] × [0, 1] → ℝ² describing the wavy surface of a square pool of water at time t in terms of wave hight and vertical wave velocity — this is an example of a function that characterizes a vector field). But far more complex state models can be considered as well. For instance, in cognitive modeling in AI formalisms, the mind-state of a reasoning agent would be a large set of logic formulas which represent the agent’s current set of beliefs and perceptions. According to the standard view held in physics, the state x(t) represents a complete characterization of the system at time t — nothing more than x(t) could or would be needed be known about the system, everything that could possibly inform us about the system’s future behavior is contained in its state. In this view, at any time t one could, in principle, make a “cross-section through reality” and get a world state. Anything that happened in the past can influence the future only through some traces that are part of the current state.

    The set of all possible states that the DS might visit is called the state space. Physicists also like to call it the phase space. I will use the symbol 𝒳 for it.

Update operator.

    This is the mathematical rule which specifies how a system state changes as time moves on. There are many namings for update operators, for instance transition function, system law, or (in AI) inference rule. Basic and often used formats in discrete time models are
    x(n + 1) = T(x(n)),
    which describes a deterministic temporal evolution of states by an iterated map T, or
    P(X_(n + 1) = x_(j) | X_(n) = x_(i)) = T(i, j),
    which gives the probability that at time n + 1 a stochastic Markov system is in state x_(j) if at time n it is in state x_(i), in a simple set-up where there are only finitely many possible states x₁, …, x_(K) and T here becomes a K × K Markov transition matrix. In continuous, deterministic, vector-state models (the most common model in physics), the gradual change of state is characterized by a ordinary differential equation (ODE)
    
      
    If you are not familiar with the  notation, be patient, it will be explained later in this section. I just mention in passing that when states are functions one needs partial differential equations (PDEs). They would become necessary if, for instance, one wishes to model the propagation of neural activation waves on the cortical surface. ODEs and PDEs can be generalized to capture stochastic dynamics, yielding stochastic [partial] differential equations, — just to drop the words.

Inputs.

    Many DSs of interest are receiving external input signals. This is particularly relevant for recurrent neural networks: after all their main purpose is to process incoming information. Animal brains are bombarded with sensory information. Thus in neural network research one almost always consider scenarios where the DS of interest — an RNN — is continually “driven” by an input signal. There is no standard notation for input signals. I will follow a common usage in the field of signal processing and control and mostly write the symbol u for input signals. Inputs u(n) can be anything, for instance just a single number per timestep (example: a voltage applied to an electric motor), or a vector (example: a million-dimensional vector made from the pixels in the image frames of a video input stream), or a symbol (like the letters of a text that is letter-wise read into an online text translation RNN). The evolution equations [eIteratedMap], [eMarkovtransition], [eODE] become extended by input terms and now appear as
    
        x(n+1)  =  T(x(n), u(n)), 
        P(X_ = x_j 
         =  T_a(i,j), 
        
        
    respectively.

    In mathematical parlance, DSs that have no input are called autonomous dynamical systems. One has to be a bit watchful about who uses this term, since in the field of robotics and intelligent agents, when one speaks of “autonomous systems” or “autonomous agents”, the intention is to highlight that these systems develop their action planning all by themselves. But of course these robots or “agents” receive sensory input all the time, so they are not autonomous systems in the understanding of mathematics.

    Another point worth mentioning is that physicists are traditionally interested in exploring their systems (a pendulum or the solar system or the helium atom) in laboratory conditions where they go to great lengths to isolate their experimentally observed system from external influences. They want to understand the “pure” system just as it is for itself and use the rather deprecatory word perturbation for the undesired external input. Since the mathematical theory-building of DS has been motivated and often pioneered by physicists, still most textbook treatments of DSs concentrate on autonomous systems. This is a pity for modeling neural networks as DSs, because the bulk of available theory is limited to autonomous systems only, and it is not at all trivial to extend that huge, traditional body of theory toward input-driven systems, RNNs in particular. Mathematical research for non-autonomous DSs is in its infancy.

    In contrast, in the field of control engineering one always has considered input-driven systems. This was by necessity: if one wants to control a system (the latter being called “plant” in control engineering), one has to give control input to the system — if you want to steer your car (the plant) you have to turn the steering wheel. A very rich body of theoretical knowledge about input-driven dynamical systems has thus grown over the last hundred years or so. But this theory was almost exclusively built around linear models of plants, that is system models where the functions T in [eIteratedMapU], [eODEU] are linear.

    Thus, even today we still have an unsatisfactory theory coverage of input-driven DS. Physics and mathematics give us beautiful formal theories of nonlinear autonomous systems, and control engineering of linear non-autonomous ones. But brains and RNNs are nonlinear and non-autonomous.

Outputs and observations.

    The states of a real-world DS are often not directly or fully accessible. One cannot experimentally measure all the electric potentials of all neurons in a brain (which are part of the full brain state), nor can one monitor every molecule of air in meteorological modeling of the earth’s atmosphere. All that one can do is measure or observe a finite, often small number of measurables (synonym: observables). For instance, one can plant a few hundred measurement electrodes into a brain (state of the art today – but the human brain has billions of neurons); or one can monitor the atmosphere with a few thousands of weather observation stations. Some technical or computational systems have built-in measurement mechanisms, in which case one often speaks of system output. In particular, artificial neural networks have designated output neurons. In mathematical formalism, measurements (which are “pulled out” from the system one might say) and outputs (which are yielded by the system “voluntarily” one might say) are treated in the same way. Typically a DS model is extended by an output function O which at any time point extracts the measurement from the system state, yielding pairs of equations
    
        x(n+1)  =  T(x(n), u(n)) 
        y(n)   =  O(x(n)), 
      
    
        P(X_ = s_j 
         =   T_a(i,j) 
        P(Y_n = y_k 
        O(i,k), 
      
    
        
        y(t)   =  O(x(t)), 
      
    where here for the stochastic system [eMarkovtransitionUO] I gave a stochastic version of output generation through a stochastic output matrix O whose rows are probability vectors.

Trajectories.

    A sequence of states …, x(n − 1), x(n), x(n + 1), … is called a trajectory (more specifically: state trajectory; also called orbit) of a DS. In continuous-time models, a continuous “sequence” becomes formalized as a function  which assigns to every continuous time t ∈ ℝ a state x(t) ∈ 𝒳, where 𝒳 is the state space.

    Note that a DS is not the same as a trajectory. A DS is a mechanism which typically can generate many different trajectories. In a deterministic DS, the trajectory depends on the initial state. For instance, in the iterated-map DS (Equation [eIteratedMap]) with right-infinite time starting at n = 0 and state space 𝒳 = ℝ, defined by T(x(n)) = x(n)², if the initial state is x(0) = 0, one gets the trajectory 0, 0, 0, …; if the initial state is x(0) = 1 one obtains 1, 1, 1, …; and for x(0) = 2 one finds 2, 4, 16, ....

    In stochastic DSs one will get different trajectories even when they are all started in the same initial state.

    If the DS is observed with an output function O, one obtains an output sequence O(x(0)), O(x(1)), O(x(2)), … together with the state sequence x(0), x(1), x(2), …. It is possible but not customary to call such output sequences “trajectories” too. The word “trajectory” is usually reserved for system state histories. For output sequences I would recommend to use the word output signal, following the terminology in signal processing and control.

The zoo of standard finite-state discrete-time dynamical systems

When scientists (of all sorts, in all fields) are faced with a very complex system that they want to understand (brains, societies, molecules, RNNs), a good strategy is to start analyses with simple and small formal models of the respective target system. It is clear that highly simplified models cannot capture the full richness of the system under scrutiny, but one can hope that simplified, highly abstracted models do capture some of the essential, fundamental properties of the target system. In this section I present the most commonly used formalisms for DS modeling which are simple in the sense that

-   discrete time is used,

-   the state space is assumed to be finite: 𝒳 = ,

-   possible input and output values likewise come from finite sets which I will denote by 𝒰 and 𝒴 respectively.

Such models are often called finite-state models. Note that “finite” does not mean small. A digital computer can be modeled (and is modeled in theoretical computer science) as a finite-state system. A state x(n) of a digital computer is a binary vector which assigns the on- or off-switching states to all the Boolean gates and memory cells that make up the computer’s circuitry. I have no idea how many gates a modern PC has, but certainly many millions. Say, one million. Then there are 2^(1, 000, 000) different possible states: |𝒳| = 2^(1, 000, 000), a number that by far exceeds the number of atoms in the universe. But it’s finite.

I will now take you on a fast guided tour through the zoo of finite-state models.

Deterministic finite-state automata (DFAs)

DFAs describe how a system with states x₁, …, x_(K) is deterministically “switching” or “jumping” fron one state x(n) to the next state x(n + 1) under the controlling influence of an input u(n) given at time n. This is captured by a transition function T : 𝒳 × 𝒰 → 𝒳, (x(n), u(n)) ↦ x(n + 1). The transition function can be coded by a lookup table or illustrated in a transition graph (Figure 21).

[A 3-state, 2-input DFA. ]

DFAs are the first things that computer science students learn about in their introductory Theoretical CS classes. In CS, DFAs are the simplest model of an input-driven computing system. Except for the output functionality (which DFAs lack), every clocked digital microchip can be seen as a DFA. The wording “automaton” (the singular of “automata”, which is a plural word) comes from the intuition of seeing a DFA as a little “machine”.

Often in a DFA one additionally specifies a starting state . There are other optional embellishments, like specifically designated final or accepting states that in CS modeling signify the successful completion of a computing job.

DFAs are used in every field of science, not only in theoretical CS. For instance, gating proteins in the membrane of biological cells can be switched (by chemical or electrical inputs) between a finite number of geometric configuration states which in turn determine which molecules can pass from the outside of the cell to its inside through these gating proteins. Or a high-level “cognitive” robot control program can be set up as a DFA where each state corresponds to an assumption of the robot in which external situation it currently is, with switching induced by sensory input signals.

Nondeterministic finite-state automata (NFAs)

A DFA is deterministic because if at time n it is in state x(n), and it receives input u(n), then the transition function T unequivocally determines in which state x(n + 1) = T(x(n), u(n)) the DFA will be at time n + 1. This can be relaxed by allowing a choice of possible next states. The transition function then generalizes from T : 𝒳 × 𝒰 → 𝒳 to the new type 


[A 3-state, 2-input NFA. ]

In a NFA, the next state is not uniquely determined by the current state x(n) and the input u(n). It is only specified which next states are possible: they must be from the set T(x(n), u(n)). Thus, even when the system is started always from the same starting state, many trajectories are possible. It may also happen that T(x(n), u(n)) = ⌀. Then the trajectory comes to a halt in a dead end, since there is no possible next state (as happens in Figure 22 with T(p, B) = ⌀).

NFAs yield simplified models of RNNs as in the following example. The original state space of an RNN with K neurons that uses the system equation

  
      
is 𝒳 = [ − 1, 1]^(K). This continuous hypercube of side length 2 can be discretized as one wishes, for instance by partitioning it into smaller hypercubes with side length 1 in an obvious manner. These sub-hypercubes, of which there are 2^(K) many, are taken as the finitely many elements of a discrete state space 𝒳^(*) = . Then, if it is known that at time n a continuous-valued trajectory of the original system [etanhRNNwithInput] lies within hypercube x^(*)(n) and the input is u(n), one can infer that at time n + 1 the original trajectory must lie in one of the hypercubes from the set

  
      
The continuous-valued inputs u(n) can be discretized in a similar manner, which in turns requires an adaptation of [eDiscreteRNNasNFA] (exercise). Both discretizations together give a NFA model of the RNN.

NFAs and their generalizations play a major role in theoretical computer science, among other in setting the stage for the famous P =? NP problem, which is believed by some (including me) to be the deepest unsolved problem of mathematics at large, period .

I emphasize that non-determinism is not the same as stochasticity. NFAs and other non-deterministic models of DSs only make statements about which trajectories are possible at all, not how probable they are.

Probabilistic finite-state DS — aka (discrete) Markov chains

We now turn to probabilistic finite-state models. The simplest one is the Markov chain (MC), or more precisely, the finite-state Markov chain. A Markov chain consists again of a finite state space 𝒳 = . There is no input in this elementary kind of model. Transitions from one state x(n) to the next state x(n + 1) are now stochastic, and the transition probabilities are conditional probabilities of the form
P(X_(n + 1) = x_(j) | X_(n) = x_(i)),
which can be assembled in a K × K transition matrix T (also called transition kernel), as already indicated in [eMarkovtransition]. At starting time n = 0, the MC system can be in any of its states, with a probability distribution given by a starting distribution written as a probability vector p₀ (Figure 23 shows a 3-state example).

[A 3-state Markov chain. ]

Trajectories are random state sequences. In formal mathematical notation, the (random) state in which this system is at time n is described by a random variable X_(n) — that is, for every time point n one has a separate random variable. Random trajectories can be generated as follows:

1.  At starting time n = 0, select the first state by a weighted draw from p₀.

2.  Later times: if the trajectory point x(n) = x_(i) ∈ 𝒳 at time n has already been determined, the next state x(n + 1) = x_(j) is picked from 𝒳 according to the probabilities P(X_(n + 1) = x_(j) | X_(n) = x_(i)) which can be found in the i-th row of the transition matrix.

This generation process can be continued for an arbitrary duration. Among all initial trajectories of length L, a specific trajectory x_(i₀), …, x_(i_(L − 1)) has the probability of being generated given by

  
  
   =  
        P(X_
  
   =  

A Markov chain is a particularly simple kind of a stochastic process. Generally, a (discrete-time) stochastic process is a sequence (X_(n))_(n ∈ ℕ) of random variables which all take their values in the same state space 𝒳. Markov chains are special in that the probability of what is the next state x(n + 1) only depends on the previous state x(n). In general stochastic processes, the probability of x(n + 1) to appear in a trajectory may depend on the entire initial part of the trajectory, requiring some formalism (of which there are many) that can capture all the conditional probabilities
P(X_(n + 1) = x(n + 1) | X₀ = x(0), X₁ = x(1), …, X_(n) = x(n)).

Stochastic processes where the probabilities for the next state only depend on the previous state are generally said to have the Markov property; they are called Markov processes. Markov processes can have infinite or continuous state spaces, and they can also be defined for continuous time. They play a big role in physics, because by that dogma of physics that I mentioned earlier, all that can be inferred or known about the future of a physical system is contained in its present state — which is just another wording for the Markov property. According to physics, in particular quantum mechanics, the world is a Markov process, though a more complex one than what we see in Figure 23.

In the field of stochastic processes, one often calls trajectories realizations or paths of the process.

Moore and Mealy machines

These two kinds of finite-state DS are obtained when DFAs are augmented by an output mechanism. The ingredients of DFAs are kept (state space 𝒳 =  is added. While the underlying DFA is evolving, a synchronous output sequence y(n) is generated

-   in Moore machines: by letting a state x ∈ 𝒳 “emit” an output when this state occurs in a DFA trajectory — this is specified by an output function of type O : 𝒳 → 𝒴,

-   in Mealy machines: by emitting the outputs not from the states, but from the transitions of the underlying DFA, specified by an output function of type O : 𝒳 × 𝒰 → 𝒴.

Figure 24 shows a simple example.

[A 3-state Moore and a 3-state Mealy machine.]

This input-to-state-to-output set-up leads to a fundamental machine learning task: Given an observed paired input / output sequence

   u(0), u(1), u(2), 
   y(0),
identify (“learn”) a Moore or Mealy machine which can generate this observed input-output paired sequence. This is the elementary situation of modeling in the natural sciences: (i) experimentally manipulate a target system by giving inputs to it, (ii) observe some measurables of the system, then (iii) try to explain the observed input-to-output relations with an underlying system model. It is not difficult to find some Moore or Mealy machine that can perfectly explain a given input-output “training” sequence. The challenge is to find a minimal such machine, that is, the smallest possible — following the principle of power and elegance called Occam’s razor that the simplest explanations for some riddle of nature are the best.

Probabilistic finite-state output-observed DS — aka hidden Markov models

In the same way as DFAs can be augmented by adding an output mechanism, one can add such an item to Markov chains. Since here the general perspective is a stochastic one, the output mechanism also takes on a probabilistic format. Thus a hidden Markov model (HMM) is defined by a “hidden” (unobservable, invisible) Markov chain defined as in Section 3.2.3, plus, for every state x ∈ 𝒳, conditional probabilities
P(Y_(n) = y(n) | X_(n) = x(n))
which declare how probable it is to observe (measure) a “visible” outcome y(n) ∈ 𝒴 =  at time n when the ongoing random stochastic process is in state x(n) (see Figure 25). These probabilities are called emission probabilities. They are usually all collected in an emission matrix E of size |𝒳| × |𝒴| whose i-th row collects the emission probabilities of all possible observations y ∈ 𝒴 if the process is in state x_(i).

[A 3-state, 2-observables hidden Markov model.]

Here and in Figure 25 I used a finite set of possible observation values. HMMs can be generalized to continuous observation spaces 𝒴 ⊆ ℝ^(n) by introducing continuous emission distributions on 𝒴, conditional on the hidden states.

In machine learning and in the natural sciences at large, HMMs are a model of choice when one wishes to capture a situation where the states of a stochastic real-world DS cannot be directly or completely measured, and all that one has available are a few measurement devices that can observe the ongoing process. For example in a chemical factory, what happens inside a reactor vessels will be monitored only by a temperature and a pressure sensor, which does not at all fully capture all the chemical reactions, turbulent flows, chemical gradients etc. inside the reactor. Or, closer to the theme of our lecture, what happens inside a brain can be observed by the few dozen channels of an EEG scalp electrode array. The measurement process itself is typically plagued by stochastic uncertainties (“measurement noise”), hence the need to introduce emission probabilities.

In machine learning, HMMs have been playing a central role in the developement of speech recognition technologies. The underlying hidden Markov process models what happens in a brain of a speaker, and the observable signal is the speech signal coming out of the mouth of the speaker owning the brain. Since about 2010 HMMs have been superseded by recurrent neural networks in speech recognition applications. The role of HMMs in the natural sciences however is irreplaceable by neural networks because HMM models allow a scientist to rigorously analyze the obtained models, for instance making probabilistic predictions about the long-term future evoluation of a process. HMMs furthermore are still un-replaceable in computer linguistics and computational linguistics, because they allow the modeler to connect assumed cognitive mechanisms (the hidden Markov chain part) with observable speech or text output in a transparent way. RNN models do not allow this because they are blackbox models which are so complex and so devoid of mathematical theory that an analysis of system properties is virtually impossible.

From a machine learning perspective, the learning task is the following: Given a sequence y(0), y(1), …, y(L) of observations made from an invisible “hidden” Markov process, learn (estimate, identify) a HMM which can explain the observed sequence of measurement values. The tutorial text of gives a beautiful introduction to the theory and practical algorithms of HMMs (almost 30K Google cites, as of February 2021). This classic piece of tutorial scientific literature has been a major booster for the widespread use of HMMs.

Probabilistic input-driven finite-state DS — aka controlled Markov chains

While HMMs augment Markov chains with output, one can also give an input signal u(n) to a MC. One can think of this input as a way to “steer” the ongoing stochastic state evolution, hence these input-driven Markov chains are also called controlled Markov chains. For every possible input u ∈ 𝒰 =  one has a separate Markov transition matrix T_(u_(i)). When at time n input u(n) is given, the probabilities to transit to the next state are the ones found in the associated matrix T_(u(n)) (example in Figure 26).

[A 3-state Markov chain controlled by two inputs]

The impact of inputs is thus quite dramatic: they change the entire Markov chain transition system from one timestep to the next. For machine learning this means that if one wishes to learn a model of a controlled Markov chain from data, one often has to estimate a large number of large matrices — which does not only sound hard, it is hard and needs large volumes of training data and considerable computing resources.

In the field of reinforcement learning, a subfield of machine learning, controlled MCs are further expanded by assigning a reward to every transition x_(i) → x_(j) under an input u_(k). The underlying interpretation here is that inputs u(n) are actions of an intelligent agent carried out in a randomly behaving environment (modeled by the underlying Markov chain, whose states x(n) correspond to external situations the agent can be in). The objective in reinforcement learning is to let the agent learn a policy, that is an action-decision-making rule π : 𝒳 → 𝒰 which tells the agent which action to choose next when it finds itself in a situation x(n). A policy is good to the extent that when it is applied at every time step, the accumulated reward is maximized. In this context (controlled Markov chain plus reward plus policy) one speaks of Markov decision processes. Markov decision processes, in which the policy functions π : 𝒳 → 𝒰 are learnt by deep neural networks, play a leading role in today’s deep reinforcement learning research. You will likely have heard of machine learning systems beat human champions in chess, Go, and even advanced computer games — those successes are based on Markov decision processes combined with deep neural networks.

Probabilistic input-driven, output-observed, finite-state DS — aka Partially Observable Markov Decision Processes

When one adds control inputs to a HMM, one obtains the most complete and complex kind of model that I want to show you in this zoo (Figure 27). These models capture how a stochastic DS, which can only be observed through stochastic observables, behaves in time when it is steered, controlled, influenced, modulated or perturbed by external inputs.

[A 3-state Markov chain controlled by two inputs and observed by two outputs]

Stochastic models of this kind play a leading role in designing and training of artificial intelligent agents like autonomous robots, software agents, or game characters. These models are the agent’s world model: the states x_(i) ∈ 𝒳 are the states (situations, conditions) of the external environment, for instance locations (for a mobile robot), or tool loads and tool tip poses (for a manipulator robot arm), or the mental states of opponents (for competitive game creatures). The inputs u_(j) ∈ 𝒰 are the possible actions that the agent can execute. The observations y ∈ 𝒴 are what the agent can see, hear or otherwise sense in its environment. In the fields of reinforcement learning, rewards and policies are added to the picture, and the whole thing is called a Partially Observable Markov Decision Processes, or POMDP for short (pronounce “Pomm-Dee-Pee”). My favourite classical (accessible but not reflecting the deep learning state of the art) tutorial texts are for the case without rewards and policies and for the full reinforcement learning picture. It is always a good idea to read seemingly outdated classical texts because when the field is fresh and young, authors still explain things that in later texts are taken for granted. The word / acronym POMDP is also used for the input-driven, output-observed, finite-state DS without the reward and policy ingredients; the usage of terminology is not entirely consistent. Like with Markov decision processes, the discrete input and observation spaces that I displayed here can be extended to continuous-valued ranges, which today is typically effected by installing (deep) neural networks in the overall machinery. This leads far out of the scope of our lecture.

Cellular automata

Cellular automata (CA) are magic. On the one hand, they are as simple as DFAs — even simpler — and on the other hand, they can model almost every real-world DS, from the spread of animal populations to whirlwinds to chemical reactions to the color patterns of wild animals from zebras to seashells.

The states x of a CA are 1-, or 2-, or generally d-dimensional regular rectangular grids whose grid cells each contain a symbol from a local state set 𝒬 = q₁, …, q_(K). In the one-dimensional case (as in Figure 28), this grid corresponds to the line ℤ of integers; in two-dimensional CAs it corresponds to ℤ × ℤ, etc. In mathematical formalism, a state x thus is a map x : ℤ^(d) → 𝒬. Besides using the infinite, unbounded grids ℤ^(d) one can also define CAs on finite rectangular grid regions. In the terminology of CAs, states are mostly called configurations or generations or global states to distinguish them from the local states q ∈ 𝒬. I will use the term “configuration” here.

[A 1-dimensional CA with two local states (red, green) and a neighborhood width of three.]

The evolution of a CA creates a sequence of configurations x(0), x(1), … in a deterministic manner. The initial configuration has to be defined by the experimenter. The next configuration x(1) at time n = 1 is computed with the help of a local transition function 
    local.

The evolution of 1-dimensional CAs can be nicely represented in a graphical format where each row of an evolution graph corresponds to one configuration, and time progresses from the top to the bottom. Figure 29 shows four (famous) examples which exhibit four different qualitative kinds of CA rules, today called the Wolfram classes of CAs after their discoverer, Stephen Wolfram. In all the four plots, the first configuration is a random binary pattern. In class 1 CAs, all “activity” quickly dies out. In class 2, a stationary repetitive (cyclic in time) pattern evolves. Class three CAs generate evolutions that look “just like noise”, and in fact such CAs were used by Wolfram as random number generators in early versions of the Mathematica software package that he developed. Class 4 includes CAs whose evolution leads to the “self-organization” of increasingly globally structured patterns. Universal Turing machines can be embedded in such class 4 CAs, leading to the arguably simplest know models of universal computing systems , see also the nice Wikipedia page on this subject.

[The four classes of 1-dimensional CAs identified by Steven Wolfram. Image taken from .]

In two dimensions, one cannot plot entire evolutions but must be satisfied with snapshots of individual configurations. Figure 30 shows a few.

[Some configurations occurring in some 2-dimensional CAs. Pictures retrieved from www.collidoscope.com/modernca/welcome.html some years ago — not online anymore. But there are other online CA simulation engines that invite playing with these amazing systems.]

The evolution of spatial patterns in continuous substrates is usually modeled with partial differential equations (PDEs). By discretizing the state space, a PDE can be approximated by a CA. Cellular automata thus can be used as computationally cheap and mathematically analyzable tools to study spatial pattern formation in general. One of my personal favorites is the simulation of the geometrical patterns that appear on the surface of exotic seashells as the shell is growing (Figure 31). Note that a seashell is growing through expanding its “lip front” and winding it around its shell body while it is growing. The geometrical patterns are generated at the 1-dimensional growing front. The biochemical mechanism can be described by a reaction-diffusion PDE which, when approximated by a CA, yields striking similies of the biological original.

[A real seashell (left) and a CA-based simulated one. Image taken from .]

CAs have been first defined by John von Neumann, who used them to develop a model of a self-replicating computing engine .

Cellular automata are being studied and used in several communities for different purposes and in different ways:

In theoretical computer science

    as an elementary model of parallel computing architectures,

in general complex systems sciences

    to study spatio-temporal pattern evolutions,

in the mathematical study of dynamical systems

    as one of the most simple kinds of spatiotemporal dynamics.

For computational speed when simulating CA evolutions, the configurations of CAs should not be handled by a von-Neumann computer, because this would mean that all the cells of one configuration would have to be updated one after the other due to the serial processing imposed by the CPU. In order to exploit the inherent parallelism in CAs, one often implements them on FPGAs; also dedicated microchips are available for running two-dimensional CAs efficiently .

In the field of neural networks, a specific kind of 2-dimensional CAs has been branded as Cellular Neural Networks by no lesser scholar than Leon Chua, who subsequently became more prominently famous as a pioneer of memristors, another technology innovation related to neural networks. Let me just squeeze in the remark that in recently blossoming research, memristors are being explored as electronic nanoscale devices that can directly implement trainable neural synapses in innovative neuromorphic microchips (... my own research happens to be strongly involved here ...). Back to cellular neural networks (acronym CNN, confusingly the same as CNN = convolutional neural network). They are made from cells whose local transition functions are Boolean gate functions. Calling these systems “cellular neural networks” seems a far cry to me, but it is in line with the earliest computational interpretation of the brain by . In this pioneering work, which can be seen as the foundation paper for what later became the field now known as computational neuroscience, the authors abstracted biological neurons to Boolean gates too, and applied results from automata theory, which in turn builds on DFAs, to analyze brain functions. — You see, everything in the world of science seems somehow connected to everything else, and this is one of the reasons that this subsection grew so long. Another reason is that I have a personal relationship with CAs: they were the topic of my diploma thesis.

Attractors, Bifurcation, Chaos

As you saw in the previous sections there are many kinds and aspects of dynamical systems, and as many mathematical formalisms. But if you open any introductory textbook on dynamical systems, it will almost surely focus on a single type: , where x ∈ ℝ¹, x ∈ ℝ², or x ∈ ℝ³ — that is, autonomous, continuous-time systems of at most three dimensions, specified by ODEs. There are two main reasons for this didactic preference. First, history: such DS are the ones that have been needed by physicists and explored by mathematicians ever since Newton. They are the most deeply investigated sort of DS. Second: plottability. The trajectories of these systems can be beautifully visualized, making it easy to develop vivid intuitions about the remarkable things that can happen in such systems.

The concentration on such low-dimensional ODE systems is less restricting than one might suspect. It turned out that there are a number of key phenomena which, on the one hand, can be nicely studied in these ODE systems, and which on the other hand are universal and re-appear in most other sorts and formalisms of DSs. I like to call these universal phenomena the ABC of dynamical systems: Attractors, Bifurcations and Chaos. The modern, general theory of DS revolves very much about ABC. Understanding this trio is a door-opener to understand what may happen in pendulums, brains and artificial recurrent neural networks. Not understanding them means that the door stays locked. Thus I will spend some pages on explaining the basics of this ABC.

There are two textbooks that I want to recommend. The first is — written for mathematicians and physicists by an MIT prof who once got the MIT best teacher award, and his skills to explain and motivate show in this book. The second is . It is the most beautiful science book that I know – period. Abraham is a DS math pioneer and Shaw is a graphical artist. The book contains not a single formula — everything is explained in wonderful drawings, and yet it guides you into some advanced corners of DS theory in a rigorous way, no hand-waving. Often re-printed, always out of print. If you can get hold of a used copy for anything less than 200 Euros, buy it (I just saw a used hardcover exemplar offered on Amazon for more than 1000 Euro). I purchased copies several times and gave them away as presents on very special occasions. You can “read” (not the right word for bathing in graphical art) it on one weekend like any graphical novel.

Welcome on stage: Whirl

For most of the rest of this Section 3.3 I will be playing with a single 2-dimensional ODE system, because it gives nice pictures that highlight the A and the B in ABC, attractors and bifurcations. I will call it the Whirl system. For starters I want to introduce Whirl in some detail, explaining some basic concepts.

The two state variables of Whirl are x and y. Sometimes I will put them together in a single 2-dimensional state vector z = (x, y)′. The ODEs that specify the temporal evolution are

  
                 - y 
  
                 + x 
These equations look a bit frightening maybe — we will soon see how they can be dramatically simplified, but for the moment let us stick to this basic formulation.

In these equations, ẋ and ẏ are the customary shorthands for , the derivatives of the state variables x, y with respect to time — in other words, the temporal change rates of these state variables. One could also correctly write these equations as

  
                 - y(t) 
  
                 + x(t),
bringing to the surface that these equations hold at any time t in the evolution of the system. However, it is common usage to omit the t’s.

The right-hand sides of [eWhirlx], [eWhirly] each are both functions (I called them f and g) of x as well as of y: the change rate ẋ(t) at any time t depends on both the current value x(t) and the current value y(t). One says that the state variables x, y are coupled.

Yet another way to write these equations is to present them in vector form:

where


[The vector field yielded by Equations [eWhirlx], [eWhirly]. Left: A large-area view. The blue arrows show the vectors resulting from h(z) for some points z on a grid. Three trajectories, each started at time t = 0 in the points z(0) marked by green circles, are evolved for a duration of 2 time units, that is for t ∈ [0, 2]. Right: a zoom around the starting point of one of the trajectories. ]

Now let us go graphical. Note that h : ℝ² → ℝ² is a function which assigns to every vector z ∈ ℝ² another vector h(z) ∈ ℝ². Such functions are called vector fields.

In the 2-dimensional case, vector fields can be nicely visualized. Figure 32 (left) shows the vector field coming out of our system equations. Every point z = (x, y)′ ∈ ℝ² is associated with a vector (blue)  of the blue vectors. Generally in our example system, the speed is higher when the trajector is further away from the origin.

In this system there is a special point, namely the origin z = (0, 0)′. For this point (and only for this point) the speed is zero:  are called fixed points of a DS. They can lie anywhere in state space, not only at the origin as in Whirl.

Now I will disclose how these system equations [eWhirlx], [eWhirly] can be simplified (and we will continue our journey with the simplified representation). The trick is a coordinate transform to polar coordinates.

If you are not familiar with polar coordinates, here is a quick explanation. A point z ∈ ℝ² can be specified in many ways. The most common is to specify it by its Euclidean coordinates, z = (x, y)′, which is what I did above. Another representation is to specify z by its distance r from the origin (its “radius”, hence the symbol r); and the rotation angle φ of the line leading from the origin to z (Figure 33).

[Left: Representing a point z ∈ ℝ² in Euclidean (green) and polar coordinates (red). Right: According to the decoupled ODEs [eWhirlr], [eWhirlphi] in polar coordinates, the motion of a state z is the sum of two independent motion components in orthogonal directions, one along the radius beam and the other along the circle on which z lies. ]

Now, if in our Whirl system a state z(t) changes its x and y coordinates with the change rates given by [eWhirlx], [eWhirly], also its polar coordinates r, φ are changing. Omitting the derivation, the equivalent laws of change for r, φ are given by the ODEs

  
  
These formulas look certainly simpler than their equivalents in (x, y) coordinates. The main advantage of the new version over the old is not a simpler look, but the fact that the new coordinate variables (r, φ) are decoupled. The rate of change ṙ of the radius only depends on the current radius, and the rate of angular change doesn’t even depend on anything, since it is constant. In this decoupled system we can analyze the dynamics in the direction of the radius independently from the angular dynamics. A trajectory point z(t) moves in the r direction by coming closer to the origin or moving away from it. And simultaneously, z(t) rotates anti-clockwise around the origin, with constant angular velocity φ̇ = 1. It’s as if z(t) sits on a circling radar beam like on a rail on which z(t) can glide back or forth (Figure 33 right).

A cyclic attractor and a repellor

Now that we have made friends with the Whirl system, we will investigate it more closely (using the polar coordinate representation) and learn a lot.

The vector field shown in Figure 32 has some interesting geometrical properties which determine the long-term evolution of trajectories.

[ Two long-term trajectories (black) and two trajectories evolved for a shorter time (red). Starting points are marked by small green circles. Four short-term trajectories started close to the origin. An elementary phase portrait rendering of the Whirl system, showing attractors in green and repellors in red, plus two exemplary trajectories.]

Figure 34 shows two trajectories that are started from different initial points and which were computed for a long timespan (black), and two trajectories that were computed for a shorter timespan (red). Observation: all these trajectories converge to the unit circle, whether started from its inside or outside. A state z(t) that exactly sits on the unit circle will forever rotate along that circle counterclockwise with an angular velocity of φ̇ = 1. One says that the unit circle is an attractor of this DS, or more specifically, a cyclic attractor. Cyclic attractors indicate the presence of oscillatory (or periodic) behavior of a DS. Our Whirl system is a case of an oscillator system.

Generally and somewhat loosely speaking, in an n-dimensional dynamical system , where z ∈ ℝ^(n), an attractor is any subset A ⊆ ℝ^(n) which has the properties (i) that trajectories started “near” A will converge to A if evolved for infinite time t → ∞, and (ii) that any trajectory started in A will remain forever in A, and (iii) that A is minimal, that is, no subset of A has properties (i) and (ii). From a purely mathematical perspective, attractors are sets of states, and one may speak of attracting sets if one wishes to emphasize this fact. Attracting sets can have many other geometrical shapes besides circles — we will see some other types later. A general, precise definition of attractors in dynamical systems needs concepts from topology and is beyond our scope, but this intuition of “letting all nearby trajectories converge” will be good enough.

Now consider Figure 34. It shows a few trajectories that are started close to the origin. They all move away from it (and in the long run will converge to our cyclic attractor). The origin acts as a repellor. Like attractors, repellors R are sets of states — here, the repellor set contains a single point only, namely the origin R = . It is a point repellor. Repellors are the opposite of attractors. They are (again a little loosely speaking) minimal state sets characterized by the property that any trajectory that is started near the repellor will move away from it.

Figure 34 graphically collects the essentials of the Whirl system, by showing the repellor (red) and the attractor (green) and a few instructive trajectories, without rendering the vector field. Such graphical “thumbnails” of dynamical systems are called phase portraits. There is no precise definition of what a phase portrait is — you may call any graphics that highlights the dynamics of a DS a “phase portrait”. Attractors and repellors and a few trajectory lines are typically drawn in phase portraits, but further elements can also be added.

[The radial component of the Whirl dynamics.]

Now we take a closer look at the Whirl equations [eWhirlr] and [eWhirlphi] and investigate why, exactly, we get the dynamical behavior that we have qualitatively described above. The rotational component φ̇ = 1 needs no deep thinking — it tells us that every trajectory is rotating around the origin with constant angular velocity. Everything that is interesting happens in the radial component ṙ =  − r³ + r. Figure 35 plots ṙ against r. We see that ṙ is equal to zero when r = 0 or r = 1. That is, when a state z has a radial component of 0 or 1 — that is, when it lies in the origin or on the unit circle — then ṙ is zero, which means that z will not move in the r direction. Or said in educated terms, r = 0 and r = 1 are fixed points of the radial dynamics ṙ. And whenever 0 < r < 1, the motion ṙ is positive: when z has a radial component between 0 and 1, its radial component will grow with time — the trajectory moves closer to the unit circle. Conversely, when r > 1, then ṙ < 0 which means that states outside the unit circle will also move closer to it by reducing their radial component.

Furthermore, the fixed point r = 0 is repelling: the slightest perturbation by pushing the trajectory from 0 to any 0 + ε will initiate a state motion away from the origin. And the fixed point r = 1 is attracting: all values of r around it move toward r = 1. For fixed points, instead of calling them repellors or attractors, one also says that they are instable or stable fixed points. — Once this radial ṙ is understood, the Whirl phase portrait (Figure 34) becomes “obvious” (start running when a mathematician says this word! ) — it’s just the ṙ dynamics on a rotating radius beam.

[Another ṙ and the resulting phase portrait. The radial rate of change function. Resulting phase portrait. The two basins of attraction for the central point attractor and the cyclic attractor with radius 0.6]

Once one has the knack of it, it is easy to create more complex DSs in the same spirit. Figure 36 shows a ṙ function that passes through the zero line four times, at r = 0, 0.3, 0.6, 0.9 and the resulting phase portrait. The constant rotational motion φ̇ = 1 was kept. This DS has a point attractor in the origin and a cyclic attractor with diameter 0.6, and two cyclic repellors with diameters 0.3 and 0.9. No further explanation should be necessary.

Two more broadly used concepts: Given an attractor A, its basin of attraction (or attractor basin) is the set  of all states z which will eventually converge to A. Figure 36 shows the basins of the two attractors in this system. The basin for the point attractor in the origin is the set . — Segments of trajectories that are not close to an attractor are called transients. This term is used in a loose manner because it is not mathematically clear what “close” means. It’s more an intuitive term used when one wants to emphasize that a state motion is not (yet) well described by the controlling influence of an attractor by which the trajectory might eventually be captured.

Attractors are, I think, the most important and decisive phenomenon in mathematical models of real-world dynamical systems. The reason: if you look at any real-world DS — for instance a stone (yes! it’s a dynamical system! its atoms oscillate like mad!), or your wristwatch, or your brain — then this system most likely has already existed some time before you look at it. But a system whose dynamics has evolved for some time will be in, or close to, an attractor. That is the very definition of attractors. Thus what we observe when we look around are all attractors, and never repellors. While from a pure-math perspective, repellors and attractors are symmetric concepts (a repellor in $
h(
- h(

The Whirl system has simple, low-dimensional system equations, and only a single attractor. This may have given you the impression that attractors are somehow special and few. But DSs that are not as simple and small as Whirl may host very large numbers of attractors. This holds in particular for recurrent neural networks. In Section 5 we will meet with a kind of RNN called Hopfield networks in which we can embed very many attractors by training the network. Each attractor corresponds to an input image that the network has learnt to memorize and that it can remember and retrieve when it is shown a new test image that is similar enough to the stored image.

Structural stability

Now we empty a glass of wine, get a little disinhibited, and add some crazy stuff to our equations for the radial and angular motions:


  
  

We have added another vector field to our original one — we have perturbed our good old Whirl, even quite strongly. The two state variables r, φ are now coupled, and it seems impossible to predict how the changed system will behave. We plot a phase portrait of the new system and compare it to the portrait of our original Whirl system (Figure 37).

[The original Whirl system, drawn once again (left) and the perturbed system (right).]

You should be surprised! The perturbed system doesn’t look very different from the original one! It also has a cyclic attractor — which looks a bit wobbled but it’s still a cyclic attractor with an unstable fixed point in the middle. The main structural elements have been preserved: the two phase portraits are qualitatively equivalent. Intuitively, if you imagine that the original phase portrait had been printed on a rubber sheet, you could smoothly stretch that rubber sheet and thereby deform the original portrait until it exactly matches the perturbed one. More precisely, there exists a smooth, bijective mapping  which, when applied to any trajectory in the Whirl system gives a trajectory in the Wobble-Whirl system, and vice versa. If such a “rubber sheet transformation” between two phase portraits exist, one says that the two concerned systems are structurally similar.

I didn’t play a sophisticated math trick here. Any added crazy vector field, provided it is not too strong, will perturb our original Whirl system in a way that the new phase portrait again will have an unstable fixed point somewhere, surrounded by a cyclic attractor.

Here is an important concept: a DS 𝒟 is called structurally stable if it has this surprising robustness property, namely that if its vector field is perturbed by any arbitrary added vector field whose maximal “vector amplitude” is not greater than some bound B, then the perturbed system 𝒟^(*) will be structurally similar to the unperturbed one. Rephrased in the plainest take-home wording: A structurally stable dynamical system will stay “essentially the same” if its system equations are kicked and twisted a little.

There also exist structurally instable DS. Their system equations have to be exactly set up in the right fashion with exactly the right vector field — the slightest change to its vector field will change the system’s qualitative properties. Here is an example:


  
  

[Left: the pure circling dynamics of the system [eCircDSr]  [eCircDSphi]. Right: Almost the same system, but slightly perturbed.]

You can easily see from the system equations [eCircDSr]  [eCircDSphi] how the phase portrait will look like: all trajectories are perfect cycles (Figure 38 left). But this needs extraordinary precision in the vector field: a trajectory must hit its own tail again exactly after one rotation. A slight perturbation of the vector field will make the returning trajectory miss itself, resulting in inward or outward spiraling. Figure 38 shows the latter case.

The two phase portraits in Figure 38 are not rubber-sheet equivalent: one cannot pull and shear a rubber canvas without rupturing it such that circles become spirals. Notice that besides the circling vs. spiraling behavior, there are other qualitative differences between the two systems. The pure circling system has no attractor and no repellor, while the perturbed system shown in the figure has a repellor at the origin.

If a DS 𝒟 has the property that the slightest change of its vector field will change qualitative properties of its phase portrait, one says that 𝒟 is structurally instable.

The mathematical study of structural (in)stability of DSs is involved and since about half a century an important research subject, with a full picture far from being known. Some important classes of dynamical systems have been shown to be generically structurally stable. This means, loosely speaking, that if one writes down a randomly selected system equation for a DS from that class, it will be structurally stable with probability 1. If you want to dig deeper, check out the Wikipedia article on structural stability, or even deeper-digging, the Scholarpedia article on Morse-Smale systems, an important large class of generically structurally stable DS which includes many real-world physical systems.

I find structural stability not only mathematically intriguing, but also very reassuring indeed. We would not exist, and neither would the universe exist, if structural stability were not a very typical property of DSs. If most real-world DS were structurally instable, the slightest perturbations would change their behavior dramatically. Imagine what would happen if the many dynamical systems in our universe — from the hydrogen atom to planetary systems, with plants and animals in between — would not be structurally stable — pouuffff! bouummmm!

Structural instability is splendidly described in a classical work of German literature, the Simplicius Simplicissimus by Hans Jakob Christoffel von Grimmelshausen, written 1668. This novel recounts the life and adventures of the vagabond Simplicius in the Thirty Year’s War. In one episode (Book 6, Chapter 9) the hero meets a ... a what? That is the question, because that strange being met by Simplicus is structurally instable:

I was once walking around in the forest and listening to my idle thoughts when I found a life-sized stone statue lying on the ground. [...] it began to move by itself and said: “Leave me alone. I’m Soonchanged.” [...] Then he took the book which I happened to have with me and, after he had changed himself into a scribe, he wrote the following words in it: “I am the beginning and the end, and I am valid everywhere. [...]” After he’d written this, he became a large oak tree, then a sow and then quickly a sausage, and then some peasant’s dung. The he changed himself into a beautiful meadow of clover and, before I could turn around, into a cow-pie; then he became a beautiful flower or sprout, a mulberry tree and then a beautiful silk rug and so on till he finally changed back into human form [...] Then he changed himself into a bird and flew quickly away. (, translation by Monte Adair)

A bifurcation

We are now ready to call on stage another key actor in the DS show: bifurcations. We again consider the Whirl equations, but add a scaling parameter a to the radial component, leaving the constant rotation unchanged:

  
  
Such parameters that scale or otherwise modulate certain components in a system equation are called control parameters. There can be several control parameters in the equations of a DS, which one can lump together in a control parameter vector a. If one wants to discuss DS under the influence of control parameters a, one often writes

to make it explicit that the right-hand side contains control parameters.

[Whirl phase portraits and ṙ functions for several settings of the control parameter a.]

So far we have investigated the Whirl system in the case a = 1 only. Interesting things happen when we consider other choices for a. Figure 39 shows Whirl phase portraits for the settings a =  − 1,  − 0.2, 0, 0.2, 1, and the associated plots of the ṙ function. Here is a summary of findings that we can make:

-   When a < 0, phase portraits show a point attractor in the origin, with trajectories whirling toward it. I mention without proof that these systems are all structurally stable.

-   When a > 0, phase portraits show a point repellor in the origin and a cyclic attractor that lies on a perfect circle with a diameter equal to the zero crossing point of the ṙ curve. Again all of these systems are structurally stable.

-   When a = 0, and only for exactly this unique value of the control parameter, we find again a single point attractor in the origin. However, there is a qualitative difference between the case a = 0 and the cases a < 0 which is not immediately visually apparent. Namely, in the cases a < 0, when one would measure the length of any trajectory, from some arbitrary starting point until the trajectory line meets the point attractor, then this length is finite, and when the trajectory spirals toward the origin from some starting point, it revolves around the center only finitely many times. In contrast, the length of any trajectory in the a = 0 case, measured from some starting point toward the center, is infinite; and trajectories revolve around the origin inifinitely often as they come closer. The trajectories shown in Figure 39 for a = 0 have been computed for a very long but not infinite time, thus they don’t reach the center and the plot leaves a little unvisited area around it. The mathematical cause for this behavior is that in the a = 0 case, the slope of ṙ at r = 0 is zero. — Finally I note that the a = 0 system is structurally instable. The slightest perturbation of this system will change its behavior either to a < 0 or the a > 0 type, i.e. one would get either a finite-length spiraling-in or a repellor + cyclic attractor type of system.

-   The a = 0 system is not rubber-sheet equivalent to the a < 0 systems, because, intuitively speaking, one would have centrally “unwind” the a = 0 rubber canvas infinitely often to get a 1-1 mapping to a a < 0 phase portrait, and that cannot be done by a continuous map (it would have a singularity in the origin).

What we see here is an example of a bifurcation. Bifurcations occur in many ways, and there are many types of bifurcations. The general scenario for bifurcations looks as follows:

-   Consider a system  with control parameters.

-   Typically, when the control parameters a are changed by small amounts, the resulting systems will be structurally similar, and each is structurally stable.

-   However, for some isolated, critical values a^(*), the system  will be structurally instable.

-   If one considers a sequence of control parameters a₀, a₁, …, a^(*), …, a_(N) which passes through a^(*), then systems with a “on the left” of a^(*) will be structurally stable and similar to each other, and so will be the systems with control parameters “on the right” of a^(*). But the systems on the left vs. on the right are not structurally similar.

This can be summed up a bit loosely as when a control parameter passes through a critical value, we observe an instantaneous change in qualitative structural properties of the DS. Such abrupt changes of system behavior are called bifurcations.

Bifurcations occur in real-world systems everywhere. They are not confined to systems described by ODEs. Here are two examples:

My old-fashioned wall-clock

    that I inherited from my grandmother will stop ticking at the point when the metal spring that I should have wound up (but forgot) unwinds to a critical slackness. The control parameter here is the force of the spring, and the bifurcation is one between a system with a cyclic “ticking” attractor (spring force  > a^(*)) and a system with a stable fixed point (spring force  < a^(*), clock stands still).

A dripping faucet.

    This is my favourite example — a classic in the DS literature. You can try it out for yourself, but you need a faucet that can regulate the water very finely, and the outlet nipple should be small — it doesn’t work satisfactorily with bulky rusty old faucets.

    [A dripping faucet harnessed for the service of science (picture taken from )]

    The control parameter is the degree by which you open the faucet. When you open it just a tiny bit, you will observe a regular, slow, periodic formation of water drops coming out in equal time intervals: drip – drip – drip ... : a periodic attractor. When you slowly turn the faucet further open, the dripping will get faster but stay regular periodic — until a bifurcation suddenly appears: when the faucet is opened across a critical value, the dripping sound will suddenly change to a double periodic pattern (your ears will immediately notice the difference): drip-drop – drip-drop – drip-drop ... : you witness a period-doubling bifurcation. Now continue (needs a really finely operating faucet) and slooowly open the faucet further. All of a sudden, the double periodic pattern will double again: drip-drop - dribble-plop – drip-drop - dribble-plop – drip-drop - dribble-plop ... . If you continue opening the faucet beyond another certain critical point, suddenly all periodic regularity in the dropping rhythm will be lost, and you have entered the chaotic regime of the faucet system (see Section 3.3.6). The physical reality of this system is very complex, involving water volume shapes, surface tension, physical interactions between the material of the faucet nozzle and water, etc., and only approximate mathematical models are within reach. This system has fascinated DS researchers a lot, and numerous experimental studies paired with mathematical analyses revealed an enormously complex picture . Not even the most innocent everyday phenomena can escape the bite of science.

Bifurcations in RNNs

Bifurcations occur not only in ODE based continuous-time system models but also in many others — in fact they must occur whenever a modeling formalism allows one to define system equations with control parameters. Recurrent neural networks in machine learning are usually specified in discrete time with iterated maps. Re-visiting our 2-neuron intro example from the beginning of this long section, now written with a control scaling parameter a,

  
                                            1a -1a 
                                        
we saw that this system changes its qualitative behavior if a is changed. In fact, this RNN behaves like a perfect water faucet, going through a sequence of period doublings and entering chaos if a is increased.

Now brace yourself for some disturbing news. RNN models have huge numbers of control parameters: namely, every single synaptic weight can be regarded a control parameter. In the simplest version of an RNN with N neurons, x(n + 1) = tanh (Wx(n)) with x ∈ [ − 1, 1]^(N), all the elements in the connection matrix W are control parameters. It is clear that, if already an RNN as simple and small as [eDemoRNNDS] can go through a splendid bifurcation cascade under the influence of a single control parameter, much stranger things can happen in larger RNNs with hundreds, thousands or (in modern Deep Learning RNNs) even millions of control parameters.

During the process of training an RNN (we will learn the details of that soon in our course) these parameters are all gradually adapted by gradient descent algorithms akin to the ones that we saw in Section 2.2.3. One starts from some randomly initialized RNN with a weight matrix 


Chaos

There are three basic kinds of attractors. The two simpler ones we have already met: point attractors and cyclic attractors. Now I will cast a short glimpse at the third kind, chaotic attractors, also sometimes called strange attractors. The mathematical study of chaotic attractors has given rise to an entire subbranch of DS theory, chaos theory. It is a very rich (and indeed strange) field, and here I can only present a few basic concepts in a rather informal manner.

I will tell my story using the super-classical demo example, the Lorenz attractor. It is defined by the 3-dimensional ODE

  
  
  

This is a system with three control parameters σ, 𝜚, β. I only discuss the classical setting of σ = 10, 𝜚 = 8/3, β = 28. The system is structurally stable: small variations of these values would not change the qualitative properties of the phase portrait. The dynamics is characterized by a single, global attractor, which lies within the cube shown in Figure 41A. From all starting points that one might choose in ℝ³, trajectories will converge to this Lorenz attractor.

[A: A 50-time-unit trajectory on the Lorenz attractor in its 3D state space. B: A 200-time-unit trace of the x(t) component of Lorenz states. C: Three plots of the x(t) components of state trajectories (x(t), y(t), z(t)) started from three different but closely neighboring starting points. D: The three pairwise absolute distances between the three x-trajectories shown in the panel above, in logarithmic scale.]

The diagram in Figure 41A shows a trajectory that lies directly in the attractor. I always think of the Lorenz attractor dynamics as a fly circling around two light bulbs: for a while, the poor thing circles around one bulb in slowly widening cycles, then for some reason loses hope and switches to the other bulb... and so on and on.

This attractor is, like all attractors, defined as a subset of points A ⊂ ℝ³. The geometry of this attractor set A is indeed ... strange. It consists of infinitely many curved lines which fold into the volume cube in an infinitely finely organized way that defies every plotting resolution. If I would have run the simulation shown in Figure 41A infinitely long, I would have gotten a line of infinite length contained in A which would never exactly meet itself — and this infinite line would only be one among infinitely many other such lines, all belonging to the set A. The geometry of A cannot be described with the familiar concepts of geometry. It needs a new sort of geometry theory, the theory of fractal geometry. The study of fractal geometries was boosted by the graphics powers of modern digital computers and gives rise to beautiful pictures — and allows computer game engineers to generate virtual sceneries full of trees, clouds and rippled waters, not to speak of broccoli, familiar objects around us all of which have fractal geometries.

The defining hallmark of chaos is however not its fractal geometry but its effective unpredictability. Consider the plot of 200 time units length of the x state component shown in Figure 41B. The spiraling rotations in the two lobes of the attractor are nicely visible as “upper” and “lower” oscillation episodes. The number of revolutions that the trajectory takes within one of the lobes seems random. In the shown plot, picking the “upper” oscillations, we count 4 - 1 - 5 - 1 - 2 - 1 - 8 - 1... In fact, using such apparently random features of chaotic trajectories, one can design random number generating algorithms, and chaotic dynamics have been proposed for data encryption schemes. But wait... isn’t the evolution of an ODE system deterministic? Yes it is. If one fixes the initial starting point (x(0), y(0), z(0))′, the future evolution ((x(t),y(t),z(t))′)_(t ∈ [0, ∞)) is uniquely and perfectly determined, without any randomness. And here is the catch: one cannot effectively determine the starting (x(0), y(0), z(0))′ with the infinite precision that real numbers need. In all practical computations, one only has finite-precision numerics; and in all empirical measurements one can measure a physical quantity only up to a residual measurement error. This necessarily remaining indeterminacy of the initial condition, regardless how small, renders the long-term continuation of the trajectory effectively unpredictable. This phenomenon is brought to the surface in the plots in Figure 41C,D. Panel C shows three x(t) trajectories that were started from almost, but not quite, the same starting states. The three starting states were about 0.004 space units distant from each other, that is about 1/10,000 of the diameter of the entire attractor. Up to a simulation time around 5 time units, the three trajectories stay so close together that their separation from each other is not visible in the plotting resolution. Thereafter, they visibly separate from each other and continue in apparently entirely individual futures. Panel C shows how the pairwise distances between the three shown trajectories grows as time goes on. Note the log scale of the plotted distances: the mutual distance grows exponentially on average (red shaded arrow) until complete separation (blue shaded arrow) is reached. This exponential separation of nearby trajectories is the essential fingerprint of chaotic dynamics.

Many real-world systems are apparently chaotic. A large class of real-world systems whose chaotic properties have been intensely studied are turbulent flows in fluids and gases. Turbulence in flows manifests itself in many ways, the most visible being the creation of eddies (local “whirls”) of all sizes. The earth’s atmosphere and oceans are turbulent flows. This means that long-term weather forecasting is impossible: a complete, precise determination of the global atmosphere’s state, down to every molecule, is obviously impossible. Here is a thought experiment, the famous butterfly effect, you will probably have heard of it: consider two complete, totally precise simulations of our planet’s atmosphere, both started from almost the same starting state. The only difference between the two starting states is that in one of the two simulations, a butterfly somewhere in Chine flaps its wings once, and it doesn’t do that in the other simulation. This tiny difference will blow up, exponentially invading the future, and lead to entirely different weather conditions after a few weeks, with a tornado striking Groningen in one simulation and calm sunshine in the other.

Chaos is easy to create in continuous-time RNNs  or in discrete-time RNNs x(n + 1) = tanh (a W x(n)) when the recurrent weight matrix W is fixed and the single scaling control parameter a is increased. Intuitively, when a grows beyond some critical value, the network-internal re-circulation of activation gets so strong that the network over-excites itself and “goes crazy”. Figure 18 shows that this can happen in RNNs with merely two neurons.

Chaos apparently plays important roles in biological brains. The respective literature is bottomless. Chaos has been detected or hypothesized on all levels of the anatomic and functional neural processing hierarchy, from a single spiking neuron to daydreaming and schizophrenia. In two successive articles, first give a methodological introduction to chaos concepts and the difficulties of its experimental verification (more detailed than this lecture note section — very recommendable reading), and then survey the wide range of theories and findings .

So far, so good ...

(Note: This section only gives some added background information, will not be tested in exams) I could only give you a first tasting of dynamical systems. Basic DS knowledge is indispensable for anyone who works with recurrent neural networks. In case that you will be professionally working with them at some point, you will undoubtedly be pulled deeper into the fields of DS. Here are some of the bites that you might have to chew:

Discrete-time, continuous-valued dynamics.

    ABC phenomena occur in iterated map systems z(n + 1) = h(z(n)) like they did in ODE systems and are equally important there. But in discrete time they are more complex and cannot be so nicely visualized. The reason for the added difficulty is that trajectories which solve ODE system equations can never cross each other, while discrete-time trajectories can do that. Consider for example the one-dimensional, two-state DS with state space 𝒳 =  with the master equation x(n + 1) =  − x. It only has two possible trajectories, depending on whether one starts in  − 1 or 1, but these two trajectories cross each other at every timestep. In a certain sense, continuous-time ODE systems are a very small subset of discrete-time systems: one can approximate every ODE system arbitrarily well by discrete-time systems, but not vice versa. Such discrete-time approximations are, in fact, realized by every numerical algorithm that solves ODEs on a digital computer — all the apparently continuous-looking trajectory plots that I pictured in this section were, alogorithmically speaking, created by discrete-time iterated maps and only plotted with the dots connected.

Signal processing and control.

    This large, classical and powerful engineering discipline has developed its own rigorous methods arsenal to deal with timeseries, which are called signals in that field. A main tool there is to decompose signals into frequency mixtures by Fourier or Laplace transforms, and do all analyses in this transformed frequency domain. These methods are broadly used in theoretical neuroscience and whenever artificial neural networks get into the hands of engineers.

Stochastic processes.

    I only briefly pinpointed stochastic DSs by the discrete-time, discrete-value models of Markov chains, hidden Markov models and POMDPs. But stochasticity is everywhere in biological neural networks and is present in almost all real-world signals that one encounters as inputs in practical applications of RNNs. At some point, an RNN modeler will need to apply general methods of stochastic process theory.

Information theory.

    Artificial and biological neural systems use their dynamics to “process information” in some way — that’s what they are made for. This will sooner or later force an RNN modeler to read a textbook of information theory.

Network theory

    is a rather young, interdisciplinary research field which considers dynamics that evolve on graphs (called networks in that context, not to be confounded with neural networks — “networks” is a far more general concept). Many important real-world systems can be best modeled as a collection of active elements which exchange signals, messages, materials or forces along connecting channels. Standard examples are power grids (active elements: power plants, transformation stations, end-users; connecting channels: power cables); social networks (active elements: people; connecting channels: communicated messages); metabolic networks (active elements: proteins and other biomolecules in living cells; connecting channels: chemical reaction pathways); or, yes!, neural networks. The specific modeling powers of network theory come from the combination of graph theory (the global connectivity patterns between the active elements) with DS theory (the local dynamics of the active elements), which opens new ways to study the emergence of global, network-wide dynamical phenomena.

Non-autonomous dynamical systems.

    I presented ABC theory only for ODE systems that are not driven by external input. But neural networks normally operate by processing input data (recall Equations [eIteratedMapU]-[eODEU]). Input-driven DS, which are a case of what mathematicians call non-autonomous DS (because their dynamics cannot be understood only from within the system itself), are quite hard to analyze. In particular, developing adequate generalizations of ABC phenomena that work in input-driven scenarios is a mountain that mathematicians have only begun to climb. This is in my view the most painful gap in today’s math toolboxes when one wishes to really understand brains and artificial RNNs.

Recurrent neural networks in deep learning

The deep learning (DL) revolution began with feedforward neural networks, especially CNNs. Wait... that is not quite true; history isn’t that simple. The pioneering paper of , which (not only) I consider as a kickoff work for DL, was about a special sort of RNNs, the restricted Boltzmann machine (RBM). We will treat RBMs later in this course. In that pioneering work, the powers of RBMs were demonstrated on data compression tasks. Only in passing it was also mentioned that RBMs can be used to initialize the backprop learning for MLPs — the rest is history. After creating excitation and much respect (because it isn’t easy at all to work with RBMs) in the early years of DL, RBMs vanished from the focus of attention again, though they are still being used and explored.

When it comes to recurrent neural networks, the DL stage is nowadays reserved for Long Short-Term Memory (LSTM) networks and their derivatives. Like convolutional neural networks, LSTM networks have a history that began before the DL revolution. Essential ideas were introduced already in the diploma thesis of and became fully worked out in . However, LSTM networks only rose to the domineering role that they have today after reliable backpropagation algorithms became available through DL research. In our present time, virtually all advanced machine learning tasks which need to cope with timeseries data are handled with LSTM networks or close relatives of them. In particular, all speech recognition and text translation engines are based on such RNNs.

In order to understand LSTM networks, one has to make friends with two separate algorithmic and architectural techniques: (i) a generalization of the backprop algorithm from feedfoward networks to recurrent networks, called backpropagation through time (BPTT), and (ii) a special kind of complex neuron model, the LSTM unit. I will treat both topics in turn, but before I do that, I will explain what it means to carry out a supervised learning task with temporal data.

Supervised training of RNNs in temporal tasks

I restrict this presentation to discrete-time dynamics, where time is represented by integers …, n − 1, n, n + 1, …. Deep learning RNNs always use discrete time. In contrast, biological neural systems, and some of the recent developments in neuromorphic computing which I will hint out in the last session of this course, are based on continuous time.

A basic format of an RNN

One of the most basic kinds of RNNs (simpler than LSTM networks which I will explain later) is given by the following update equations:
  
                      
where

-    or forever,

-   Equation [eDefRNNupdate] specifies how the network activation state is updated from one timestep to the next, and Equation [eDefRNNoutput] specifies how the output vector is computed at time n,

-   x(n) is the vector of activations of the neurons inside the RNN — I will generally denote the number of neurons in an RNN by L, so x(n) ∈ ℝ^(L),

-   W is the L × L matrix containing the synaptic connection weights w_(ij) ∈ ℝ giving the strength of the connection from neuron j to neuron i,

-    is an L × K dimensional real-valued matrix containing the weights from K input neurons into the RNN — I will generally denote the dimension of the input signal u(n) by K,

-   b ∈ ℝ^(L) is the vector of biases,

-   σ is a sigmoid function (like in MLPs, typically the tanh, the logistic sigmoid, or the rectifier function), which is applied element-wise on the vector 
          in

-   y(n) is the output signal generated at time n — I will generally denote the dimension of the output signal by M,

-    is an M × L sized output weight matrix containing the weights of synaptic connections from RNN neurons to output neurons,

-   f is a wrapper function applied to the linear “readout” ; often again a sigmoid, but often also just the identity function.

Sometimes one wishes to feed back the generated output signal y(n) into the RNN. Equation [eDefRNNupdate] is then extended to the format

where  is an L × M sized matrix for feeding back the generated output signal y(n) into the RNN. Such output feedback is required when the task is not an input-to-output transformation where the output is determined by the input signal, but where instead the output is actively generated by the RNN, even in the absence of input. Such tasks occur specifically in robot motion generation.

In order to get the dynamics [eDefRNNupdate], [eDefRNNoutput] started, at time n = 0 the recurrent network state x(0) is set by the user to some predetermined initial state, often the all-zero state. For later times, Equation [eDefRNNupdate] is used to compute network states x(1), x(2), ….

[Schema of a basic RNN. Here an example is shown that has one-dimensional input and output signals, and hence only one input neuron and one output neuron. This particular RNN has been trained to generate a sinewave output whose frequency is controlled by the input: high input value gives high frequency output. Output feedback is needed here. Activation signals from a few internal neurons are shown. Image taken from http://www.scholarpedia.org/article/Echo_state_network.]

Figure 42 illustrates the wiring schema of such a basic RNN. Many variations of the system equations [eDefRNNupdate], [eDefRNNoutput] are in use. Also, more complex architectures than the one shown in the figure are used. For instance, in hierarchical RNNs, several recurrent sub-modules are stacked on top of each other. Or the signals from inside the RNN are propagated through a MLP in order to obtain a more complex “readout function” than what you can get with [eDefRNNoutput].

Some basic types of temporal tasks for RNNs

Temporal supervised learning tasks come in a number of quite different kinds. The common denominator is that the training data contain timeseries.

There are two basic types of tasks, depending whether the used data come from stationary or non-stationary dynamical systems:

Stationary dynamical systems

    are not changing their kind of signals as time goes on. Whether you observe a stationary system at early times, say from time n = 0 to time n = 100, will give you the same kind of signals as if you would observe it from time n = 10000 to time n = 10100. Examples are oscillations, monitoring a wind turbine, human or robot walking, or a video showing the surf on a beach.

    The training data typically consists in a single, long sequence 
    
      
      
          
    which is close (in some appropriate loss function metrics) to the testing output, that is $
      

Non-stationary dynamical systems

    use and generate signals whose characteristics changes with time. Observing such a system at different times will yield different signals. For instance, the microphone signal recorded when a human utters the word “zero” will have a spectral composition which in the beginning reflects the phonological nature of the letter “z”, and at the end the phonological nature of the vocal “o”. Or a stock index will just continue to grow (lucky investor!) throughout the observation period.

    The training data for non-stationary tasks typically consist of many individual recordings of a limited-time observation, for instance in many sound recordings of people uttering the word “zero”. Formally, such training data are doubly indexed: S = (u^((i))(n), y^((i))(n))_(i = 1, …, N; n = 1, …, n_(i)), where i is the index of the training sequences and there are N such sequences, and n_(i) is the length of sequence i.

Stationary and non-stationary signal-generating systems can be deterministic or stochastic. Most real-life systems are stochastic.

The stationary vs. non-stationary distinction is not clear-cut. Whether a system is considered stationary or non-stationary depends on how long it is observed. For instance, when a pair of dancers performing a waltz is monitored for an extended time, the different waltz figures repeat and the signal can be considered stationary. However, the entire waltzing performance can be segmented into short figures (which are individually trained in dancing lessons), where each figure consists in a specific non-stationary sequence of motions. The same holds for speech (words are non-stationary, but a long monologue can be considered stationary), weather (during a day there will be a non-stationary evolution of temperature and wind, but over the years everything repeats — ignoring climate change), and many other interesting signals. In order to learn RNN models for such mixed signals (short-term non-stationary, long-term stationary) one may pursue two strategies: either try to learn one large, complex RNN which is capable of changing its “mode” in shorter time intervals; or try to learn several smaller, simpler RNNs, one for each typical non-stationar subpattern, and then combine them into a compound multi-RNN system where the individual RNNs are activated / deactivated in turn.

I will now present a few distinct scenarios to illustrate the richness of temporal learning set-ups. They all come in both stationary and non-stationary versions.

Dynamical pattern generation tasks.

    Sometimes one would like to train a RNN such that it generates a temporal pattern in its output units, without needing any input. For example, in legged robots, one wishes to have so-called central pattern generator (CPG) networks which send activation signals to the motors of the robot, such that a walking pattern is created (or a hand waving pattern, or staircase climbing, or any other motor pattern that is needed). Neurobiologists have found evidence that such CPGs are implemented by RNN circuits in the spinal chord. If reading these lecture notes leaves you with the energy to watch a youtube video, click on https://www.youtube.com/watch?v=DkS_Yw1ldD4 to see a simulated human with 61 degrees of freedom (“muscles”) performing a sequence of motions that are generated by a pattern-generating RNN. Or in automated music composition, one wishes to have an RNN whose output is an improvised piece of music.

    The training data for pattern generation tasks consists just of the desired output sequence, 

    It is not obvious however how “similarity” is measured when there is no input in the testing phase. When the training signal  can not directly be compared with a “known correct” output. Figure 44 illustrates this difficulty. The teacher pattern is a chaotic signal generated by solving the Mackey-Glass equation (an equation describing the temporal change of the amount of white blood cells in leukemic patients ; this has become a popular benchmark signal for RNN training). There is no general recipe for quantifying the accuracy of pattern generation learning. The only way is to define measurable characteristics of the signal (for instance mean amplitude, Fourier spectra, or conditional probabilities for continuations of the signal given an previous few values) and compare the values of these characteristics measured in the original training signal vs. the generated ones.

    [Illustrating the difficulty of assessing the accuracy of a learnt pattern generator. Green: Original (training) pattern. Magenta, blue: patterns generated by two trained RNNs. Top three panels: pattern signal against time. Bottom: time delay embeddings. The blue pattern generator matches the teacher better than the magenta one. This comes out visually clearly in the delay embedding plots, while it would be hard to judge the matching accuracy from the plots against time. ] [Illustrating the difficulty of assessing the accuracy of a learnt pattern generator. Green: Original (training) pattern. Magenta, blue: patterns generated by two trained RNNs. Top three panels: pattern signal against time. Bottom: time delay embeddings. The blue pattern generator matches the teacher better than the magenta one. This comes out visually clearly in the delay embedding plots, while it would be hard to judge the matching accuracy from the plots against time. ]

Pattern detection tasks.

    Sometimes one wants to identify certain patterns when they occur within an ongoing observed process. For example, in cardiological monitoring, one wants to detect moments when a certain pathological abnormality appears in a ECG signal; or in speech-based car control, the car’s driver voice recognition system must identify moments when the driver says “brake”; or the control system of a nuclear fission reactor must trigger an alarm action when the plasma shows signs of instability (a very tricky task where machine learning methods are considered a key for making nuclear fusion based energy possible — query “nuclear fusion plasma disruption neural network” on Google Scholar!).

    The training data here often is prepared in the format 
            

    [The seizure prediction scenario. The input is a multi-channel EEG recording. Image taken from . ]

Timeseries prediction tasks.

    Sometimes one wants to look into the future. This task is particularly popular among people who want to get rich by forecasting financial timeseries, and also among students searching for theses topics which will help them to become the kind of people who get rich. Other applications exist, too, for instance weather forecasting, windspeed prediction for wind energy farming (a theme with an extensive machine learning literature, check out Google Scholar on “wind speed prediction neural network”), or in epidemology where one wants to understand the spreading dynamics of a pandemic.

    The concrete kind of training data and best suited RNN architectures vary greatly across application scenarios. Here I illustrate a scenario that involves highly nonstationary and stochastic timeseries: financial forecasting. Figure 46 gives an impression of the apparent difficulty of the task. In fact it is almost impossible to predict financial timeseries; the best one can hope for is to be a tiny little bit better than just repeating the last observed value and call it a prediction.

    [Four samples out of a financial forecasting competition dataset that comprised 111 such timeseries (http://www.neural-forecasting-competition.com/NN3/). The competition task was to train a neural network which could predict all of the 111 series for another 18 steps into the future. ]

    A typical approach for timeseries prediction is to train a model that can predict just one step into the future, on the basis of past observations, and then feed back this prediction and iterate. Concretely this works as follows. The training data consists in a number of example timeseries
    S = (u^((i))(n), y^((i))(n))_(i = 1, …, N; n = 1, …, n_(i)) = (d^((i))(n − 1), d^((i))(n))_(i = 1, …, N; n = 1, …, n_(i)),
    where i is the index of the training example and d^((i))(n) is the n-th value in the i-th training timeseries (there were 111 such timeseries in the example task illustrated in Figure 46). The output timeseries desired from the network is thus equal to the input timeseries shifted one step ahead. An RNN is set up with a single input and a single output node. After training (when the network has learnt to predict all the training series as well as it can), the network is used to forecast the future by iterated one-step prediction in the following way:

    Given:

        a new testing timeseries d(1), …, d(k).

    Wanted:

        a prediction d(k + 1), …, d(k + h) for a prediction horizon h.

    Phase 1: initialization:

        The RNN is primed by running it with d(1), …, d(k) as input:
        
          
          = 1, 
        The last obtained output $
              out

    Phase 2: iterated prediction:

        Feed network outputs back as input, that is, compute for n = k + 1, …, k + h
        
            
            
            
            
              out
          

    For a serious effort to obtain good predictions, this basic scheme must be considerably refined (check out for a case study). But that is a common machine learning wisdom: on any nontrivial learning problem, better and better results are achieved by investing more and more work, insight, and computing power.

System modeling tasks.

    Sometimes — in fact, often — one wants to simulate how a physical system responds to external perturbations or human control input. However, simulation models from first physical principles can be too expensive to be practical. For instance, while the Navier-Stokes equations can be used to exactly model the airflow around an aircraft, which would give a perfect model of how an aircraft responds to pilot commands, the numerical integration of these equations with the necessary precision is far too expensive to be useful for extensive explorations of an aircraft’s behavior. In such situations one desires a computationally cheap model of the aircraft that faithfully replicates the flight responses of the aircraft to pilot steering input. When the target system behavior is very nonlinear and has memory effects, RNNs are a good candidate to yield simulation models. Figure 47 shows an example of an aircraft model realized by an RNN.

    [Modeling the response of an aircraft (top three panels showing the aircraft accelerations in the three spatial dimensions) to the pilot’s action on elevator, aileron and rudder (bottom). The neural network predictions (red lines in upper three panels) show a good agreement with actual aircraft flight dynamics (blue lines, measured from actual flight experiments). Figure taken from . ]

    Other intensely investigated examples of modeling the responses of complex systems to input are oceanic flows, local weather dynamics (and more generally, all sorts of turbulent fluid dynamics), all sorts of industrial manufacturing pipelines, robot motion, power grids, etc., etc. — basically, every complex system that one wants to control, understand, or predict through simulations is a candidate for RNN modeling.

    The training data here are of the generic format 
          max

This collection of RNN application scenarios is only indicative. There are many more, for instance tasks of data compression, denoising, channel equalization (cancelling echos in radio antenna signals), nonlinear control, game playing — and soooo many more. Since the real world evolves in time, virtually all real-world data sources are temporal; no wonder that our brain, the best world modeling engine that we know, is recurrent.

Backpropagation through time

Training an RNN (e.g. of the basic format given in Equations [eDefRNNupdate] and [eDefRNNoutput]) means to find weight matrices 

    

  n

How the loss function is defined depends on the specific scenario.

In stationary tasks, it is often sufficient to measure the mismatch between outputs at individual time steps. The loss function then is of the same form as in MLP training:
L : ℝ^(M) × ℝ^(M) → ℝ^( ≥ 0).
The risk is then defined as the expected loss 



In nonstationary tasks we find a greater variation. For simplicity I only consider the case where all training and testing sequences have the same length k. Often one is interested in the network output only at the end of processing an input sequence. This happens, for instance, in single-word recognition tasks. In other cases one wants to collect some evidence throughout the entire sequence. Generally speaking, one often wants to weigh mismatches between network outputs  and targets y(n) differently at different times. A loss function then would look like
L(((
  0,
where L₀ is a single-timepoint loss function and a_(n) are time-dependent weighting factors.

After a loss function has been decided, one carries out an RNN training project in the same way as for MLPs, by

1.  installing an optimization algorithm which minimizes the empirical risk (“training error”), that is, an algorithm which solves the problem
    
      
      L(
      
    where θ is the vector of all trainable parameters in the weight matrices 
      

2.  while at the same time attempting to ensure a good generalization by embedding the training error minimization runs in some outer optimization loop for finding a good level of regularization through cross-validation.

For MLPs, step 1. is done with the backpropagation algorithm. This algorithm crucially depends on the fact that the network topology is “feedforward”, that is, there are no connection cycles.

For RNNs, finding weights which minimize the training error is more difficult. A number of algorithms are known which are based on different mathematical principles. The “best” of these algorithms does a true gradient descent on the performance surface and is suitable for online learning (adapting the RNN continuously while a never-ending stream of training data is arriving). This real-time recurrent learning (RTRL) algorithm has been introduced 30 years ago by . Sadly, RTRL is too expensive for most practical exploits, having a cost of O((K + L + M)⁴) per update step. Yet, interest in RTRL has recently been rekindled in a subfield of deep learning called continual learning, where the objective is to find learning schemes that enable “life-long” training of RNNs. If you are interested, a tutorial introduction to RTRL and other RNN training algorithms is .

Today, the training error minimization task [eMinimizeRNNLoss] is almost always solved by the backpropagation through time (BPTT) algorithm. The idea is to “unfold in time” a recurrent neural network into a feedforward neural network, by assigning an identical copy of the RNN to each timestep and re-wiring the internal connections (in the weight matrix W) such that they feed forward into the next copy in time. Figure 48 illustrates this idea.

[Basic idea of the BPTT algorithm: a recurrent net (left) is identically replicated for every time step and network-internal connections are rewired forward to the next copy (right).]

The unfolded network is free of connection cycles, which makes it possible to train it with a special version of the backpropagation algorithm. The difference to the MLP version of backprop is that corresponding weights in different “time layers” must be identical. This is easy to accomodate and I do not present the adapted error backpropagation formalism here.

The BPTT scheme looks straightforward, but it comes with its own new problems.

The first problem is that the stack of temporal copies (right side in Figure 48) must have a finite depth to enable the backpropagation algorithm. Call this depth h (for “horizon”). The unfolded RNN then will be a finite, cycle-free network which yields an input-to-output mapping from input sequences u(n), …, u(n + h − 1) to output sequences 
  = 0, 

For all we know, the human brain solves this multi-timescale problem by exploiting a host of different physiological and architectural mechanisms which yield a large compendium of different memory mechanisms, supported by different neuronal circuits and physiological effects, ranging from ultra-short term memories in the millisecond range, over a spectrum of short-term and working memory loops from seconds to minutes to hours, to long-term memory mechanisms that operate in the range of the human lifetime. Neuroscience, cognitive science, AI and machine learning so far has only given us a very partial understanding of such memory subsystem cascades. This is an active, interdisciplinary research area in which I am personally involved (check out the EU project MeM-Scales, “Memory technologies with multi-scale time constants for neuromorphic architectures”, https://memscales.eu/).

The second problem is known as the vanishing gradient problem. It also occurs in a mirror version as exploding gradient problem. This problem comes to the surface when error gradients are back-propagated through many layers. The deeper a network, the more serious the problem. Unfolding RNNs in time tends to end up with particularly many time-slice layers (up to several hundreds), making them much deeper than commonly used deep MLPs. Thus the vanishing gradients are particularly disruptive in RNN training.

I will demonstrate the mathematical nature of vanishing gradients with a super-simple RNN. It has a single input unit with activations u(n), a single output unit with activations y(n), and the main RNN itself consists of a single linear unit with activation x(n), and no bias. This leads to the following embryonic RNN equations:

  x(n+1)  =  w 
               u(n+1), 
  (n)  =  
The task is a pure memory task. The training data is given by 
u(n-h))n = 0, 

We now want to find values for 


For this simple RNN all gradients can be computed easily without invoking the backpropagation algorithm. During the gradient descent, the unfolded network is aligned with length-h windows of the input/output teacher signal, from times n to n + h. The only task-relevant signal propagation pathway goes from the teacher input u(n) at time n to the teacher output y(n + h) = u(n) at time n + h. All other input-unit to output-unit pathways in the unfolded network lead from input signals that are uncorrelated with the output signal at the respective time, and their contributions to the gradient will average to zero as the unfolded network is moved forward through successive time windows [n, n + h] of the training data. The only gradient component that may not average to zero is the one on the pathway from the earliest input node to the latest output node in the unfolded network (see Figure 49).

[A single-unit embryo RNN for demonstrating the vanishing/exploding gradient. Training this network of depth h = 3 on the pure memory task with memory depth h, the only input-node to output-node pathway that leads to a gradient contribution which does not average out to zero is along the green line. The orange pathway, for instance, connects an input signal u(n + 1) to a teacher output signal y(n + 2). Since the two are uncorrelated and the entire network is linear, this pathway gives a gradient component which averages out to zero over time.]

For the quadratic loss, this gradient component of the loss with respect to the weigth w is equal to

  
   =  
        
        w(y(n+h)
        - u(n)
        
        
        
        
        
        out)^2
  
   =   
        
        
        
        out  
        h 
where y(n + h) is the teacher output, ŷ(n + h) the network output, and  are all the Other Gradient Components stemming from other input-output pathways and which give zero contributions in temporal averaging.

The critical term in this gradient is w^(h − 1). I think you can smell the danger: if w < 1 and h is large, this term will shrink toward zero — the gradient vanishes; and when w > 1, the gradient will explode with the depth h.

In fully grown-up RNNs the analysis of vanishing/exploding gradients is not so simple, but the basic mechanism is the same: since all time-slices of the unfolded RNN are identical copies, gradient components arising from pathways that span large temporal horizons are repeatedly either quenched or expanded at every timestep. If the learning task includes the exploitation of long-term delayed input-to-output effects (long memory), the vanishing / exploding gradient problem will make it practically impossible to let the network find and encode these long-term effects during learning.

You can find a more detailed presentation of the vanishing gradient problem in the deep learning textbook , Section 10.7. The conclusion drawn by the authors at the end of that section is “... as we increase the span of the dependencies that need to be captured, gradient-based optimization becomes increasingly difficult, with the probability of successful training of a traditional RNN via SGD [= stochastic gradient descent] rapidly reaching 0 for sequences of only length 10 or 20. [...] the problem of learning long-term dependencies remains one of the main challenges in deep learning.”

LSTM networks

Long short-term memory (LSTM) networks are today the best answer to both the problem of memory over several timescales, and the problem of vanishing gradients.

First a note on the name. “Long short-term...” seems like a contradiction in terms. This terminology has the following background. In neuroscience, different neural mechanisms have been identified (or hypothesized) to take care of memorizing on different timescales. There is a fine-grained terminology which we will not further unfold here. But there is also a very coarse summary distinction between “long-term memory” (LTM) and “short-term memory” (STM). LTM is physiologically realized (according to current neuroscience dogma) by changes in synaptic connection strenghts. The name of your father and mother, for instance, is encoded in your LTM system by very stable synaptic connection patterns which have been burnt into your brain in childhood — according to textbook dogma. (In parentheses I point out that dogmas in any science may be swept away by revolutionary ideas and discoveries; in fact it becomes increasingly clear that the idea of long-term persistent synaptic strengths is a gross simplification .) In contrast, “short-term memory” is a lump naming for any memory mechanism where information is retained for limited periods of time by neuronal activity patterns, without inducing synaptic weight changes. Thus, speaking of “long short-term memory” means memory mechanisms where information is preserved over limited, but possibly rather long times, on the basis of neuronal activation dynamics in RNNs, without synaptic weight adaptation.

In machine learning, “LSTM networks” refers to a specific RNN architecture introduced in stages in the pre-deep learning era by Hochreiter, Schmidhuber and Gers . The original motivation was to attack the vanishing gradient problem. It soon turned out that the proposed solution, now called LSTM networks, at the same time helped to train RNNs to cope with multiple-timescale tasks. Today, LSTM networks and their descendants, RNNs with gated recurrent units (GRU networks, ) are the dominating sort of RNNs used in deep learning.

The basic ideas behind LSTM networks are intuitive, but I am not aware of a transparent mathematical analysis of why and how, exactly, LSTM networks do function so well as they do. The explanations given in Section 10 in the deep learning “bible” are kind of handwaving. In the following parts of this section I will try to explain the LSTM mechanism through a worked-out baby LSTM which extends the embryo RNN example from Figure 49. The extended version is shown in Figure 50.

[A baby LSTM. For explanation see text.]

For this simplified demonstration, I consider a memory learning task where the input is 0 almost always, except for a few times when it jumps to some nonzero value. The target output is to recall the input from h timesteps before. In the example in Figure 50 the prediction horizon is h = 3. The teacher output is thus almost always zero except at times n + 3 when an the earlier input u(n) was nonzero.

The baby LSTM set-up involves three input signals u(n) ∈ ℝ, s(n) ∈ . The input u(n) plays the same role as in the embryo example before: the task for the network is to identically recall u(n) at a later time n + h in the output, that is, y(n + h) should be equal to u(n). The binary inputs s(n), r(n) (for “store” and “read”) are auxiliary inputs to control this memorizing task. Both are equal to 0 most of the time. At rare times the store signal jumps to s(n) = 1. Then, the network should “store” the current value of u(n). At the future time n + h, the “read” input jumps to r(n + h) = 1, and the output should read the stored value u(n). Figure 50 shows a case where h = 3. At all other times r(n) is zero, and the output should be 0 too.

LSTM networks have “neurons” that are quite unlike the neurons that we have seen so far. In the LSTM literature these neurons are called (memory) cells, and such a memory cell is embedded in an intricate control circuitry that has little in common with biological systems. The memory cell together with its surrounding control circuitry is called a memory block in the core historical paper . In later literature by other authors, often the entire circuitry is often called an LSTM “cell”. I will stick to the terminology of and use the term “memory cell” only for the central, memory-preserving neuron, and use the “memory block” for the entire circuit.

I will give full equations for LSTM memory blocks later and for the time being only present a simplified set of equations for our baby LSTM network, which is made from a single (simplified) memory block.

The memory block has a memory cell with activation state c(n) ∈ ℝ at time n. The update equation for c(n) and the output equation are

  c(n+1)  =  1 
               c(n)), 
  y(n)  =  r(n) 
The “store” signal s(n) acts as a multiplicative gate which only lets the input u(n) affect the memory cell at the rare times when s(n) = 1. At such times, the memory cell state is set to  at the next time step n + 1.

Similarly, the “read” signal r(n) allows the output unit to read out from the memory cell an output value of  only at times when r(n) = 1; at other times the output reading is zero.

Furthermore, as long as there is no “store” input signal, that is, as long as s(n) = 0, the memory cell retains it previous value c(n + 1) = 1 ⋅ c(n).

The only two trainable weights in this system are . If one uses again the quadratic loss, one will find that the squared error at times when r(n) = 0 is zero (because the network output and the teacher output are both zero, hence no error), and at times where r(n) = 1 is

  
                         out
                    =   (u(n-h) - 
                         out
                         in 
                    =   u^2(n-h) 
                         out
                         in)^2.
At these times, the gradient of the error with respect to the two trainable weights is

  
  in 
                                  out
                                  in) 
                                  out
  
  out  
                                    out
                                    in) 
                                    in
Obviously, gradient descent along this gradient will lead to a point where  The important insight here is that this gradient does not vanish or explode when h gets larger. In fact, the gradient is independent of h.

The key to this independence of the error gradient from the memory depth h is that, as long as s(n) = 0, the activation state c(n) is identically copied to the next time step by virtue of the update rule c(n + 1) = 1 ⋅ c(n) + 0 ⋅ … in [eBabyLSTM1]. The weight w that we had in the embryo RNN [eUpdateEmbryo], which made gradients vanish or explode if not equal to 1, is fixed to be equal to 1 in [eBabyLSTM1].

LSTMs are, by and large, a sophisticated extension on this this trick to carry memory information forward in time through a linear memory cell update c(n + 1) = w c(n), where w is equal or close to 1.

An LSTM network is an RNN which consists of several memory blocks and possibly other, “normal” RNN units. The memory blocks and normal units may receive external inputs and/or recurrent input from other blocks/units within the network.

LSTM memory blocks are made from 5 specialized neurons which have different update equations. The central neuron is the memory cell with state c(n). The remaining four neurons are

an input neuron

    with state u(n). This corresponds to the input neuron u(n) shown in our baby LSTM. The input neuron may receive external input (as in the baby LSTM) and/or input from other blocks or normal units within the network. The input neuron is a “normal” unit with an update function
    u(n + 1) = f(W^(u) x^(u)(n) + b^(u)),
    where f is any kind of sigmoid (tanh, logistic sigmoid or rectifier function), x^(u)(n) is a vector of signals composed of external inputs and/or outputs from other blocks or normal neurons in the network, W^(u) is a vector of input weights, and b^(u) is a bias vector;

an input gate neuron

    with state 
          input(n)
    
          input(n+1) = 
            
          g
          g
    where σ is always the logistic sigmoid and  is is a vector of signals composed of external inputs and/or outputs from other blocks or normal neurons in the network;

an output gate neuron

    with state 
          output(n)
    
          output(n+1) = 
            
          g
          g
    where again σ is always the logistic sigmoid and  is is a vector of signals composed of external inputs and/or outputs from other blocks or normal neurons in the network; and

a forget gate neuron

    with state 
          forget(n)
    
          forget(n+1) = 
            
          g
          g
    where again σ is always the logistic sigmoid and  is is a vector of signals composed of external inputs and/or outputs from other blocks or normal neurons in the network.

The central element in a memory block is the memory cell c(n). Its update equation, which corresponds to [eBabyLSTM1], is

  c(n+1) = 
      forget(n+1) 
      input(n+1) 

The main extensions compared to the baby LSTM are that the memory cell does not necessarily preserve its previous value with a factor of 1, but may “leak” some of it by the multiplication  with the forget neuron value.

The output y(n) of a memory block is (analog to [eBabyLSTM2]) is given by

  y(n) = 
      output(n) 
This output of an LSTM block is not necessarily external output, but may be used internally as input to other memory blocks and “normal” units in the LSTM network.

Notice that the three gate units must have the logistic sigmoid as their squashing function. This ensures that the gating values 
    output(n), 

Figure 51 illustrates the LSTM block that I described.

[A standard LSTM block layout. For explanation see text.]

LSTM networks are trained with BPTT. All the weights 



Numerous variations of this memory block architecture are in use. For instance, describe memory blocks that may contain several memory cells, and they also admit the signals c(n) as inputs to the gating neurons of the same block. Significantly simplified versions of LSTM blocks, called “gated recurrent units”, are also explored and used. A discussion of these variations is given in , Section 10.10. There appear to exist no clear universally best winners among the large assortment of possible variations.

Today many high-level programming toolboxes for deep learning are available. They all incorporate read-made LSTM modules with automated BPTT adaptations, such that the end-user does not have to care about programming the details. One just plugs together LSTM units with normal neurons as one deems fit, hits the “train” button, and hopes for the best. It often works. When it doesn’t — which also happens to happen — it is good to understand how LSTM blocks function, which now you do.

Hopfield networks

An obvious core functionality of biological brains is long-term memory (LTM). You can remember your own name as well as details from your parent’s house as well as impressions from the beach of your last summer vacation — and more. In fact you carry with you a rich repository of memories from your previous life’s experiences — you carry with you all the things that make you. The more you start thinking seriously about all the things that you “know” (= have in your memory), the more you will find that you know very very much indeed. How can your little brain store all of that?

The question of human long-term memory has kept psychologists, cognitive scientists, neuroscientists, psychiatrists, AI researchers and even physicists busy since the beginnings of each of the respective branch of science. It is one of the most heavily thought-about scientific riddles, and it is far from being solved.

One thing is clear: memorizing is different from storing items in a shelf. The car of your parents, which you can very well remember in much detail, is not placed as a little model car somewhere in your brain. A neurosurgeon would not be able to find it after opening your skull. Here are some of the research riddles (non satisfactorily cleared) that surround long-term memory:

-   How is LTM distinguished from other forms of memory, like short-term memory, long-short-term memory, working memory?

-   Is there a unique dedicated mechanism in the human brain for LTM, or is it a complex system with many functional modules? Connected with this question, what are commonalities / differences between LTMs in humans vs. dogs vs. frogs vs. honeybees vs. little worms? (They all have their peculiar LTM capacities.)

-   To the extent that long-term memory “items” are “stable”, there must be something physical in brains that is stably persisting through the tides of time. For decades, the dogma in neuroscience (and machine learning, for that matter) had it that LTM memory traces are physically realized through synaptic weights. Memorizing something for good means that some synaptic weights are set and not changed thereafter. Well, this simple dogma is dissolving in these days. It can’t be that simple:

    -   Human memories change over time. A classical, super fascinating experimental psychology study (21K citations) reveals that over the decades of a human life, what one thinks one had clearly “memorized” for good — continually changes, even dramatically. My wife and I are both aware of this and sometimes marvel that, when she is super-convinced that the skirt her sister wore at our wedding was blue and long, and I am dead sure that it was red and short, we look at photos and find it was yellow and of medium length. If you sometimes spend time recalling your childhood days, you might be interested in a recently published study reporting from a long-time study where parent-children interactions were recorded over decades, and where it was found that, whether grown-up children remember their childhood relations with their parents as positive or negative, this depends more on their current mood and current relationship with their parents than on the factual emotional experience twenty years before.

    -   On the microanatomical level, it is becoming clear that neural synapses are incessantly changing, even becoming deleted or re-growing, at amazingly high rates .

    -   A long-standing conundrum in artificial NN / machine learning research is that if an MLP is first successfully trained on some task A, leading to a specific formation of synaptic weights, and then subsequently trained again on another task B, it will learn task B but in the process modify synaptic weights such that task A is no longer mastered. This is called the problem of catastrophic forgetting. Attempts to overcome it have only very recently become halfway successful, forming the active field of continual learning within deep learning (review: ). Again, what is being found out in this field is incompatible with the dogma of that LTM memory traces are realized through fixing stable synaptic weights.

-   If your memory of a car isn’t like putting a little toy car on your brain shelves, the question is how are memory items neurally encoded? If you think of your grandmother, does this mean that a specific grandmother neuron is activated? Like always in neuroscience, there is solid evidence in support of this hypothesis, and equally solid evidence against it (https://en.wikipedia.org/wiki/Grandmother_cell). An alternative view is that memories are encoded in a distributed way: thinking of your grandmother is the effect of a complex neural activation pattern that involves large portions of your brain.

-   Given some long-term encoding scheme, by which physiological/anatomical processes are memory traces actually formed? In engineering terms: what is the “writing” mechanism? In machine learning, one does the “writing” by the backprop algorithm which nobody believes is biologically feasible (plus, it suffers from catastrophic forgetting). The human brain might need ... sleep! in order to transform memory items, which are provisionally recorded during the day, using non-persistent mechanisms (presumably in the hippocampus), into persistent traces in LTM.

-   Ok., assuming it is understood how memory items are encoded, and how they are written, how are they retrieved when you “recall” them? Recalling leads to two subproblems: addressing and decoding:

    -   How to you mentally “point to” memory items? how do you “know” what to retrieve? In a digital computer, addressing is done by pointers to physical memory registers. But equivalents of C pointers are unlikely to be implemented in biological brains. Instead, it seems more plausible that brains use content addressing. This means that in order to access what you have stored about your grandmother, you need to start with some fragments of your grandmother memories — for instance, her name, or you think of her house, or of a family gathering. This way of thinking about mental addressing is known as associative memory models. Building theories about how associative memories function has been a mainstream activity in neural networks research for decades. It has led to a mountain of models documented in a wide-spanning literature.

    -   The decoding problem is the twin of the encoding problem.

-   Finally, it is unclear what a “memory item” is. When you remember your grandmother, you will be recalling different aspects of her in different recall contexts. Apparently it is quite a simplification to think of well-circumscribed memory “items”. This immediately leads to the highly disputed problem of the nature of what concepts are and how they are neurally represented. Again, a vast literature on this topic exists in psychology, AI and philosophy, and no consensus is in sight. A most instructive, influential, and readable book is (30K Google Scholar cites), and it has a nice title too: “Women, fire and dangerous things” The title outlines the semantic contents of a single concept in an Australian Aboriginal culture and language.

This bundle of riddles is obviously of great interest for cognitive neuroscience. But it is also of central importance for today’s research in computer science and AI:

-   In deep learning, the problem of continual learning, i.e. the problem of organizing the incremental growth of the representational repertoire of an artificial neural network, is not satisfactorily solved. The magnificent achievements of deep learning come in the form of networks specialized each on a specific task. A face recognition network cannot be further trained to also recognize cars, let alone to control a robot arm. Partial solutions are emerging in these days, and continual learning is an active topic in the APS group here in the AI institute (https://scholar.google.com/citations?user=VFr_XuYAAAAJ, ).

-   In an emerging field called (among many other namings that are floating around) neuromorphic computing, one tries to design novel types of computing microchips which are inspired by neural networks. I will give an introduction to this field in the last session of this course. One of the main goals for this line of research is to enable in-memory computing. The idea is that in biological brains and many artificial neural networks, there is no distinction between a “processor” (CPU) and a “memory” (RAM). All computing should be done directly at the locations and encoding level of the memory. This would obviate the infamous von-Neumann bottleneck, that is the read/write channel between the CPU and the RAM. This bottleneck eats up most of the time and energy in conventional digital computing technologies and enforces a serial execution of computational operations. The promise of in-memory computing is to enable a thoroughly parallel way of computing directly on the hardware level of memory traces, thereby saving orders of magnitude of energy and time. So far, a generally useful way for in-memory computing has not been found.

Among the many and diverse models of neural long-term memories, there is one which stands out: Hopfield networks ( – 21K Google Scholar cites. John J. Hopfield was born in 1933 and is still scientifically active - his homepage is https://pni.princeton.edu/john-hopfield). It is simple, mathematically transparent, reasonably powerful, deeply analyzed and almost completely understood. In machine learning it spun off an entire family of energy-based neural network models, among which the Boltzmann machine, which in turn was instrumental in getting deep learning off the ground. In the cognitive neurosciences it served and still serves as a foundational reference model for associative neural memories.

An energy-based associative memory

Before I start explaining the formalism and learning algorithm for Hopfield networks (HNs), I will outline what it is meant to achieve.

In a nutshell, a HN can be trained to store a finite number of patterns and let them become retrieved by content-adressing through an auto-association process. In the context of HNs, a pattern is a binary vector ξ ∈ . In textbook introductions to HNs, one mostly uses patterns ξ whose entries are arranged in a 2-dimensional “pixel” array, which allows one to display a pattern graphically as a black-and-white pixel image. Given a finite and not too large number of training patterns ξ₁, …, ξ_(N), they can be encoded (“stored”, “learnt”) in a Hopfield network, which is a recurrent neural network of a very special design. In the context of HNs we also call these training patterns fundamental memories (after they have been stored).

In order to recall one of the stored ξ_(i), the HN is presented with a cue pattern u ∈ ^(L). The cue pattern must have the same dimension as the stored patterns. The cue is typically a corrupted version of one of the stored patterns. “Corrupted” means that the cue pattern agrees with the corresponding stored pattern in some pixels and is different on other pixels. The differences between the cue and the targetted stored pattern can be very substantial. Figure 52 shows two examples.

[Cueing a HN by corrupted patterns leads to a retrieval of the uncorrupted, stored pattern. Left: corruption by noise (leading to pattern restauration upon retrieval). Right: corruption by omission (leading to pattern completion functionality). Images taken from .]

This functionality is quite suggestive of some aspects of human LTM. We are also able to recall memory items from corrupted cues, where “corruption” can mean many things, in particular that only some fragments of the memorized items are needed to recall its full content. Furthermore, according to some theories in cognitive psychology (very much disputed though – but all cognitive psychology theories are very much disputed), humans represent and encode conceptual items in the form of prototypes, that is, “clean”, “characteristic” examples. According to protype theories of conceptual representations (introductions: Part III in ), if you see a house or read the word “house”, you would mentally access a representation of an “ideal” house. As always in the cognitive and neurosciences there is substantial evidence in favor of prototype theories (if asked repeatedly to draw a house, your drawings will all look similar, and similar to the house drawings of other members of your social community), and much evidence against it (you can also be asked to draw a large house, an old house, Bilbo Baggin’s house, or a termite’s palatial nest — and all these drawings will look different). These cognitive prototypes would correspond to the fundamental memories in HNs.

Here are the main design principles for the associative pattern recall functionality in Hopfield nets:

1.  Every state x of the HN corresponds to one possible pattern ξ ∈ . We can therefore identify HN states with patterns. We will use this convention in the remainder of this section and often write ξ for patterns as well as for HN activation state vectors, using the notations ξ and x interchangeably. The first notation is more suggestive when we discuss the interpretation of states as patterns, the second is more intuitive when we discuss the computational mechanics inside a HN, because x is the natural notation for the state of a dynamical system.

2.  Each state x of HN has a well-defined energy E(x) ∈ ℝ. Note that negative energies are possible, unlike in physics. We will nonetheless see later in this course that the connections between HNs and the concept of energy in physics can be made precise (Hopfield is a theoretical physicist).

3.  During the storage learning process, every pattern from the learning set becomes associated with a locally minimal energy. This gives (after learning) and “energy landscape” over the space ^(L) of all binary patterns, where the fundamental memories are placed at local minima. See Figure 53.

4.  Recall is started by presenting a (corrupted) input pattern u, which is set to be the initial state of a state trajectory which evolves according to the recurrent dynamics of the HN. The trajectory leads through a state sequence which at every step reduces the energy and thereby necessarily ends in a local minimum — the fundamental memory is retrieved.

[Schematic of energy landscape over the pattern/state space of a HN. The pattern space is here artificially rendered as a 2-dimensional Euclidean space; this is only for an intuitive visualization as the pattern space ^(L) does not have an Euclidean topology. Fundamental memories ξ₁, ξ₂, ξ₃ mark local minima of the landscape. Upon input of a (corrupted) pattern u, the recurrent dynamics of the HN lead through a state sequence which at every step reduces the energy, until the nearest local minimum is reached. Image retrieved from www.ift.uib.no/∼antonych/protein.html (no longer accessible). ]

Given a HN in which a set ξ₁, …, ξ_(N) of patterns has been stored, and given an input cue pattern u, the discrete-time state update dynamics of the HN will lead to a sequence of patterns/states u = x(0), x(1), …, x(m) = x(m + 1) = x(m + 2) = … = ξ_(i) which at every update yields a state with smaller or same but never larger energy, until at some time m (which depends on the initial cue) the sequence becomes stationary — no further energy reduction is possible, one has landed at a local energy minimum, that is, at a fundamental memory ξ_(i). Since the pattern/state space ^(L) is finite, this must happen after some finite time with probability 1. Figure 54 (left) shows such a pattern sequence.

[Schematic of recall trajectories in a HN. The energy landscape is rendered as a contour plot. Three fundamental patterns here correspond to three pixel images of digits 6, 3, 2. Since there are three local minima, that is, three point attractors, there are three basins of attraction. They are marked by orange boundary lines. Left: cues u lying in the basin of attraction of ξ₂ are attracted toward ξ₂. Right: a concrete 7-step trajectory leading from cue u to ξ₂. Note that the clean gradient-descent (red) lines would look much more jittery in the actual HN dynamics because it is stochastic. The Hopfield dynamics is not a gradient descent dynamics – there is no gradient in the discrete state space of HNs. Pixel images taken from . ]

Under the action of HN state update dynamics, the fundamental memories act as point attractors. The set of all cues u that are ultimately attracted by a fundamental memory ξ_(i) is the basin of attraction of ξ_(i). One may say (and AI theoreticians and cognitive scientists indeed say) that all the patterns in the basin of attraction of ξ_(i) are instances of the category (or concept or class) represented by ξ_(i). Hopfield networks thus give a specific formal model of cognitive conceptual spaces. According to the HN model, concepts are characterized by “prototypes” ξ_(i), and instances u of a concept are more or less similar to the prototype according to the number of HN state update steps it takes to move from the instance u to the prototype, that is, the number of steps until the local minimum corresponding to ξ_(i) is reached.

HN: formal model

A HN is a recurrent neural network without input and output units. If the task is to store and recall L-dimensional patterns, the HN will itself have exactly L neurons which serve equally as input, “internal”, and output units.

The formal definition of an L-dimensional HN is simple: it is fully specified by an L × L sized, real-valued, symmetric weight matrix W which has zeros on the diagonal. Here is a little example with L = 4:

  W = 
                                                                      0
                                                  1    0.1 
                                                                      1   0    -2 

The most noteworthy thing here is that the weight matrix is symmetric. This means that the connection weight of the synaptic link between neurons i and j is the same in both directions, w_(ij) = w_(ji). Connections are undirected in HNs — which is biologically unrealistic but opens the doors to “energy”-based computing.

A state of a HN is an L-dimensional binary vector with entries from .

A key idea about HNs and some other powerful neural network models that we will meet in the next section is that each state is assigned to a real-valued quantity that is called the energy of this state. In HNs, given a state x = (x₁, …, x_(L))′ ∈ ^(L), the energy of this state is defined by

  E(
    jw_

Neural network models in which an energy of states is defined lead to a tight and mathematically rigorous connection between neural network dynamics and statistical thermodynamics. Such models and the way of thinking behind all of them leads to a subfield of neural network models called energy-based models. I will not dig deeper into the links between HNs and theoretical physics at this point but leave that for later sections, when we will meet other, more powerful energy-based models.

A side fact: Hopfield is a theoretical physicist. While his work certainly has had a strong impact in theoretical neuroscience and machine learning, it also has triggered a whole school of research within theoretical physics. This line of investigation is hardly perceived outside physics, especially not in neural networks research — one of these strange facts about the disconnectedness of social subcommunities in the sciences.

The rule for the state update dynamics is a stochastic rule in HNs. If at time n the state is x(n) = (x₁(n), …, x_(L)(n))′, the next state x(n + 1) is obtained as follows:

1.  Randomly select one neuron x_(i).

2.  Compute its activation value at time n + 1 by
    x_(i)(n + 1) = sign(∑_(j ≠ i)w_(ij) x_(j)(n)).
    where “sign” is the signum function sign : ℝ → ,  sign(z) = 1 if z > 0 else sign(z) =  − 1. In the rare case that ∑_(j ≠ i)w_(ij) x_(j)(n) = 0, set x_(i)(n + 1) = x_(i)(n).

3.  Set x(n + 1) = (x₁(n), …, x_(i − 1)(n), x_(i)(n + 1), x_(i + 1)(n), …, x_(L)(n))′, that is, update only the activation of the neuron x_(i).

Note that x(n + 1) = x(n) is a possible outcome of this update operation.

It can be shown (very simple exercise, do it) that such a state update always leads to a reduction of state energy E(x(n + 1)) < E(x(n)) provided that x(n + 1) ≠ x(n), that is, if the update flips the state of the selected neuron. With probability 1 such a random update sequence will end in a local energy minimum state after a finite number of steps.

Geometry of the HN state space

In Figures 53 and 54 I pretended that the state space of a HN has a continuous Euclidean geometry, that is, it is a volume in ℝ^(L). This is not true, and I used that geometry only because it gives intuitive graphics. The state space of a HN has a discrete structure, because it is made from isolated points x ∈ ⁴.

Figure 55 shows the graph geometries of the spaces ^(L) for L = 1, 2, 3, 4. These are the geometries of what is known as the L-dimensional hypercubes.

[The hypercube geometries for the HN state spaces up to L = 4.]

Figure 56 attempts to visualize a 6-dimensional hypercube, with a few state energies and pattern images included.

[The hypercube state spaces for L = 6. Some (non-negative) energies of states are schematically indicated. In any pair of neighboring states, one of the two states has a lower energy than the other. The HN update rule always moves to a neighboring state with lower energy, or repeats the current point. A “de-noising” sequence of a corrupted starting input of a digit “3” is indicated for intuition, but note that these digit images have 10 × 12 pixels and thus should be correctly shown in an L = 120 dimensional hypercube. Furthermore the graphic is incorrect in that per update step (one move along one edge in the graph) should change only a single pixel.]

Training a HN

The learning problem for a HN is this:

Given N L-dimensional training patterns ξ₁, …, ξ_(N), find a weight matrix W which creates an energy landscape that has the training patterns located at the local minima, and every local minimum corresponds to one of the training patterns.

Achieving this goal is not always possible as we will see, but the conditions when it is possible are well understood.

There are two methods of finding a weight matrix W satisfying the learning objective (if it exists). The first method is very fast and simple: there is an analytical formula that directly computes W from the training patterns ξ₁, …, ξ_(N). The second method is iterative-incremental and may appear unnecessarily time-consuming, but it is biologically plausible and could be used by real brains (whose neurons cannot compute the analytical formula of the first method). I will present the formalism of both and after that explain in intuive terms why they work.

Analytical solution of the learning problem

If a weight matrix W exists which solves the learning problem, it can be written (and computed) as

  W = 
  N 
where I is the L-dimensional identity matrix.

Proving that this formula places the fundamental patterns at local minima (if possible) requires concepts from dynamical systems that we did not cover (namely Lyapunov functions, you can find a full treatment in Chapter 14 of the textbook ).

Note that this formula may yield negative as well as positive or zero “energies” for a state. This is unlike energy in physics, which is always non-negative. If one would wish to come closer to physics, one could add a constant offset B to all energy values, that is, redefine , to make all possible energies non-negative. This would however be a purely cosmetical embellishment which would not change the relative differences between energies of states, and it is only these differences that enter the HN dynamics and the location of fundamental patterns.

Two technical but not very important details in [HNWanalytical]. The prefactor 1/L is not mathematically necessary: any nonnegative factor multiplied into W will only linearly scale the energy landscape but not change the locations of the minima. The factor 1/L is included as a reverence to biological plausibility: without it, the total sum of inputs w_(ij) x_(j) (see [eHNneuronUpdate]) hitting a neuron i would grow with the size L of the network, leading to unrealistic large impacts in large networks. With the prefactor 1/L, the energy levels expressed in [eHNenergy] are normalized, i.e. they do not grow with network size.

Second, the term  − NI simply sets all self-connections w_(ii) to zero, as demanded by our HN model (notice that each of the matrices ξ_(i) ξ′_(i) has all 1’s on the diagonal).

Iterative solution of the learning problem

An iterative method to obtain a weight matrix W which solves the learning problem (if a solution exists) goes like this:

Given:

    a training dataset ξ₁, …, ξ_(N) of L-dimensional patterns.

Initialization:

    create a random initial weight matrix W(0) (symmetric with zeros on diagonal).

Loop:

    -   At update step k, present one of the training patterns to the network (picked at random or in cyclic order), say ξ = (ξ⁽¹⁾, …, ξ^((L)))′ is presented. The weight matrix before entering step k is W(k − 1).

    -   Update all weights w_(ij)(k − 1), where i ≠ j, by
        w_(ij)(k) = w_(ij)(k − 1) + λ ξ^((i)) ξ^((j)),
        where λ > 0 is some small learning rate, obtaining W(k).

Stop

    when you reach a condition of a previously defined stopping criterion. For instance, you can stop when the largest weight (in absolute value) hits a predefined ceiling; or when a test set of corrupted input patterns is recalled correctly; or when the energies of the training patterns are smaller than the energies of all patterns that are similar to training patterns except at one flipped vector entry; or (best) when the pairwise ratios of weights in the sequence W(k) appear to converge up to a predefined, small residual change.

Notice that the incremental update [eHNincWupdate] can be written in matrix form as
W(k) = W(k − 1) + λ(ξ ξ′−I).

From this finding it is easy to conclude that this incremental learning rule will converge to a weight matrix that is the same as the one obtained from the analytical solution [HNWanalytical], up to a scaling factor which grows larger and a residual error in matrix entry ratios which decreases the more the longer you run the iterative computation. The local formulation given in [eHNincWupdate] is meant to underline biological plausibility: in order to effect the change of a weight w_(ij), only information that is locally available at this synaptic link is needed.

A general note: in any neural network architecture any learning rule which, in order to determine a change of a weight w_(ij) needs non-local information which is drawn from neurons other than neurons i and j, is deemed biologically impossible. Biological synapses can be adapted only on the basis of information that is locally available at that very synapse. Specifically, the backpropagation algorithm is non-local.

Why it works. The idea of Hebbian learning

It is not difficult to get an intuition why the rule [eHNcombinedUpdate] does its job. This is a straightforward mathematical argument: the weight adaptation [eHNcombinedUpdate] lowers the energy of the training pattern ξ which is used in that step. Please check this claim by yourself from the definition [eHNenergy] of energy – I leave it as an easy exercise. The intuition is thus that we incrementally and repeatedly change W by making it to yield lower energies for the training patterns. It is however nontrivial to show that this does not at the same time lower energies for patterns outside the training set more than for training patterns.

The learning rule [eHNincWupdate] also instantiates a general learning principle that is believed to be ubiquitously effective in biological brains. This is called Hebbian learning. Because Hebbian learning is mentioned in the NN literature in many places and in many contexts, I will expand a little on this topic.

Donald O. Hebb (1904-1985) was a psychologist / neuroscientist (he started out as a teacher of English) who gave us one of the main guiding principles for understanding how neural circuits can represent conceptual information. In his book The Organization of Behavior , he developed a theory of biological neural learning mechanisms that could explain how the human brain can memorize, recall and re-generate perceptual and conceptual patterns to which it is repeatedly exposed at learning time. To explain this capacity, he developed a theory of cell assemblies. A cell assembly can be thought of as a group of neurons that are mutually exciting each other through positive (“excitatory”) synaptic connections. These mutually exciting connections arise in a learning process. If some perception (for instance, a child seeing its mother’s face) is repeatedly made, and at each presentation a certain subset of neurons in, say, a visual processing area of the brain is simultaneously activated by this perception, then these repeatedly co-activated neurons will form mutually excitatory links. In Hebb’s wording, a memory trace is formed: the perceptual experience becomes encoded in the cell assembly. This assembly can then function very much like a trained HN: if some of its neurons are excited by sensory input or input from other brain areas, the entire assembly tends to self-excite. Hebb stated this learning principle in a paragraph that has become one of the most often cited sentences in neuroscience:

“When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is increased.”

In the folklore of neuroscience, this principle has been shortened to the catchphrase

“Cells that fire together, wire together.”

Hebb was, in the first place, a psychologist, and did not use mathematical formalism to make this learning mechanism more precise. He wanted to model the biological brain and tried to relate his principle to what was known at his time about neural microanatomy. He also tried to relate his principle to a large spectrum of findings in cognitive and developmental psychology that were known at his time. He certainly would never have thought of simplifying a neuron to a mathematical variable that can only take the values  − 1 and  + 1, and also the idea to build artificial neural systems was alien to him — that idea would start its rise but 10 years later, in the form of the Perceptron.

The Hopfield network is nonetheless an obvious instantiation of “Hebbian learning”. I conclude this excursion by emphasizing that today there is a large spectrum of rigorously formalized neural learning mechanisms which are Hebbian in their core. They give a spectrum of different answers to problems which arise when one starts working out the consequences of the cell assembly idea. I mention just two:

-   The mutual excitatory connections of cell assemblies must in some way be complemented and compensated by inhibitory connections — otherwise the entire brain would burst into flames by global spread of self-excitation.

-   Neurons can only excite one another by sending spikes. A spike lasts only about one millisecond. But sensory impressions or the activation of conceptual items in one’s mind may last seconds — three orders of magnitude longer. This mismatch in timescales makes it necessary to develop ingenious, nontrivial schemes to account for fine-grained temporal relationships between the spike events in neurons within an assembly.

The first problem is solved in the HN model by giving equal citizenship rights to positive and negative neural activations, and positive and negative synaptic weights. The second problem does not arise in HNs because there are no spikes in the simple neuron model used in HNs.

Limitations

Above I inserted the cautionary clause “... if possible ...” at various places. In fact, it is not always possible to store N training patterns in an L-neuron HN, such that after learning there are exactly N local minima in the energy landscape which correspond to the precise training patterns. Several things can go wrong:

-   The fundamental memories are only approximately equal to the original training patterns, that is, they will differ from the training patterns in a few bits.

-   Not all training patterns can be stored — the storage capacity of a HN is limited. Specifically, consider the ratio N/L of the number of training patterns over the network size. This fraction is called the load of training a HN. The following facts are known for HNs:

    1.  For a load N/L > 0.138 (I give only first three significant digits, precise number is known), storing training patterns using [HNWanalytical] breaks down entirely: none of the training patterns will be stored, not even approximately.

    2.  A HN works really well only for loads 0.03 < N/L < 0.05, in the sense that the training patterns end up in local minima that correspond to perfect copies or only slightly altered versions of the original patterns.

-   Besides at the locations of the stored training patterns, other local minima are created in the performance landscape which do not correspond to training patterns. Such “false memories” are called spurious states in the HN literature.

There is one thing that always “goes wrong”: if ξ is a fundamental memory, then also the sign-inverted pattern  − ξ is a fundamental pattern. HNs cannot distinguish between patterns and their sign-inverted versions (why? find out for yourself, it is an easy one).

All of this is not so good news, inasmuch as these limitations more or less render HNs useless for practical applications.

On the other hand, the reasons why these limitations occur, and which of them strike how badly at what load levels, are almost completely known. The corresponding mathematical analyses have been carried out, mostly by theoretical physicists, in the 1980’s and now form a classical body of rigorous insight into the conditions of storing information in a neural network.

In the remainder of this subsection I document these findings in some more detail.

Bit errors in fundamental memories

If the storing formula [HNWanalytical] places a training pattern ξ exactly at a local minimum, it becomes a point attractor under the update rule [eHNneuronUpdate]. This means that if one cues the trained HN with a test input pattern u = ξ′ which is equal to ξ except for a small number of flipped bits — that is, ξ′ is located in a neighborhood of ξ — then the energy minimization induced by [eHNneuronUpdate] will let the sequence of patterns end up in ξ, as we would wish.

However, often the storing formula [HNWanalytical] leads to a slight misplacement of the local minimum associated with a training pattern ξ. The created local minimum ξ̂ will be very near to, but not exactly equal to, the original pattern. As a consequence, if the HN is cued with the exact pattern ξ, the state update dynamics will move it a little, going down the “energy valley”, until it ends in ξ̂. Some bits in the original pattern ξ will become flipped — these bits are unstable.

How many bits in a pattern will be unstable, that is, how precisely can a HN recall the training patterns? This depends probabilistically on the load N/L. Here is the main result:

Proposition 1. If N patterns are stored in an L-dimensional HN using the formula [HNWanalytical], the probability that a given bit i among the L bits of a training pattern ξ is flipped when the HN is cued with ξ is
P
  

Here Φ : ℝ → ℝ is the so-called error function associated with the one-dimensional Gaussian distribution, visualized in Figure 57.

[The error function Φ(a) gives the area under the pdf of the standard normal distribution on the left-hand side of a. ]

This proposition does not imply that, if the trained HN is cued with ξ, the state update will come to rest at a pattern ξ̂ close to ξ. It may occur that after ξ is used as input, the flipping of some bits in ξ triggers a bit flip avalanche and the state update leads far away from ξ. The training pattern is entirely unstable and is not located near a local minimum of the energy landscape. In the worst case, all bits in ξ become randomly flipped, resulting in a maximal 
0.5

How bad this avalanching becomes depends on the load. In the limit of large L, at a load of N/L ≈ 0.138, every training pattern becomes maximally unstable. Avalanches start to occur for loads n/L > 0.05. Figure 58 shows the growth of the avalance effect with the load.

[Percent of changed pattern bits under iterated network update, vs. load (here denoted by α). Figure and result from .]

The result illustrated in Figure 58 is an example of a phase transition — you remember that from the dynamical systems primer.

Spurious states

The textbook of describes in detail a computer experiment where the patterns were 10 × 12 black/white pixel images (the patterns shown in the right panel of Figure 54 were copied from that section in the Haykin book). This makes L = 120. A small set of training patterns consisting of merely N = 8 images was used. It contained pixelized versions of the digits 0,1,2,3,4,6,9, plus a pattern showing a 5 × 6 sized black square in the upper left corner (Figure 59).

[The 8 training patterns from a HN computer demo presented in . The slight jitter visible at some of the black-white boundaries are artefacts from my postprocessing of photocopies from the Haykin book. ]

When testing the trained HN with about 43,000 different cues obtained by randomly flipping one fourth of the pixels in one of the training patterns, the network frequently ended in a local energy minimum (attractor) state which was not one of the 8 training patterns. Such “wrong memories” are called spurious states in the HN terminology. Figure 60 shows 108 such spurious states. Most likely there are many more — it is not possible to exhaustively search a pattern space of 2¹²⁰ many different patterns! Haykin cites without further explanation findings of to the effect that there are three kinds of spurious states (see Figure 60), one of which is always present: namely, the sign-inverted training patterns always become local energy minima if the original training patterns do so.

[Some spurious states found in the demo in the Haykin book. ]

Summary of imperfections related to load

Here is an overview of what goes wrong or right in HNs depending on the load. I collected these findings from .

-   For all loads N/L: stable spin glass states exist — these are states that look “just random” and which are uncorrelated to all training patterns.

-   For N/L > 0.138: spin glass states are the only stable ones.

-   For 0 < N/L < 0.138: stable states close to desired fundamental patterns exist.

-   For 0 < N/L < 0.05: pattern-related stable states have lower energy than spin glass states.

-   For 0.05 < N/L < 0.138: spin glass states dominate (some of them have lower energy than pattern-related states).

-   For 0 < N/L < 0.03: additional mixture states exist, with energies not quite as low as the pattern-related states.

In summary, a HN works really well only for a load 0.03 < N/L < 0.05.

Miscellaneous notes

HNs have been deeply explored, and many more interesting facts could be related about them. I conclude this section with some ad hoc comments.

Local stochastic vs. deterministic global update.

    We used a stochastic local update: in [eHNneuronUpdate] a single bit was randomly selected to consider whether it would be flipped in a state update. Alternatively, one could consider an update where all bits of a pattern would be subjected to [eHNneuronUpdate] in one global update. Interestingly, this is not equivalent to the local stochastic update. A dramatically different update dynamics would result, with no guarantee to lead to local energy minima.

    Such a decisive difference between local stochastic and global deterministic state update rules is not confined to HNs, but is a common finding in all kinds of dynamical systems that have states made of discrete bits.

    Specifically, digital computer chips use a global deterministic update rule. This necessitates a global clock which on your notebook computer typically runs at a Gigahertz clock rate. In contrast, in the emerging field of neuromorphic microchips (I will say more about them in the final lecture), no global clock is available and the update dynamics is necessarily local and stochastic, driven by local physical laws. This is one of the reasons why the computational mathematics in such unconventional microchips is entirely different, and much less understood, than the mathematical theory for classical digital computing — which every student of computer science learns in lectures on theoretical CS.

Variations.

    The HN model that I described in this section is but one specific version among many other formalizations. For instance, one could use state values . This is a minor variation which leads to an entirely equivalent theory. A much stronger modification is to admit continuous values of the neurons, say from the continuous interval [0, 1]. The quality of findings is similar to what we saw in this section, though details and formalism differ. The Haykin book discusses both discrete and continuous HNs.

Importance of symmetric synaptic connections.

    The symmetry of the weight matrix W is crucial for HN theory. If one would admit asymmetric connections w_(ij) ≠ w_(ji), energy functions could no longer be defined; the intuition of state updates reducing energy would evaporate; besides stable fixed points, other kinds of attractors (for instance cycles) would emerge and the “use case” of storing patterns as point attractors would collapse. It is a characteristic of the larger family of energy-based models of neural computation to have symmetric weights.

Heteroassociative networks.

    HNs are alse referred to as autoassociative networks. This terminology is motivated by the fact that the HN update dynamics “associates” a fundamental memory ξ with itself: ξ(n + 1) = ξ(n). In heteroassociative networks, one wants to learn associative sequences of patterns. The cognitive modeling motivation to consider such sequences is the idea of a flow of thought: if you think of a rose, the next moment you might be thinking of love, then of tears, and so on. If your brain were a HN, once you think of a rose, you will continue to think of the rose over and over again...

    Designing and training a heteroassociative network, one would start with a training dataset that is made of sequences of patterns. In a simple case that would be a circular sequence ξ⁽¹⁾, …, ξ^((N)) = ξ⁽¹⁾. The learning formula [eHNcombinedUpdate] would be replaced by

    W(k) = W(k − 1) + λ ξ^((i + 1)) ξ^((i))′,
    and (if convergence is achieved) the weight matrix would become non-symmetrical. A substantial literature on such heteroassociative networks exists (of course).

HN pro’s.

    Many things are good and influential and insightful about HNs:

    -   It is a simple model which can be mathematically almost fully analysed.

    -   HNs are not biologically immediately unrealistic in that they have an incremental learning rule which is local.

    -   HNs have strongly influenced how neuroscientists think about long-term memory.

    -   Deep connections to other fields of computational physics exist (spin glass models).

    -   HNs are robust against “brain damage” (not discussed in this section), that is, if in a trained HN some weights are modified, the overall performance deteriorates only gradually.

    -   HNs have historically helped to salvage neural network research after the “neural network winter” caused by the shattering insight that Perceptrons cannot solve the XOR task. J. J. Hopfield was traded over several years as a Nobel prize candidate.

HN con’s.

    A number of aspects of HNs are maybe not so good:

    -   The memory capacity is small. It is however unclear whether the capacity of biological brains is larger, or how it can be quantified in the first place.

    -   All the “mathematically nice” results only hold for uncorrelated fundamental patterns — an unrealistic assumption for real-world patterns. A HN would have difficulties to store two rather similar patterns separately.

    -   Various imperfections — pointed out above in Section 5.5.

    -   HNs are not technically useful.

    -   HNs have strongly influenced how neuroscientists think about memory. This may be bad if biological brains turn out to work entirely different.

Moving toward Boltzmann machines

One of the central models of a “cognitive” neural network is the Boltzmann machine (BM). The Boltzmann machine is a must-know architecture for machine learners, AI afficionados, cognitive scientist and neuroscientists alike because

-   it gives a computational model of memory, concept representation, learning and reasoning in hierarchical cognitive systems,

-   it can be used in machine learning as a universal learning “machine” for almost any probability distribution,

-   it can be understood as a universal statistical inference (“reasoning”) device because it admits to compute conditional probabilities of any sort (“what is the probability I see a horse given that this is the input picture?”, “what is the probability that tomorrow it will rain given that today it was cold and windy?” — for a conditional image completion task watch https://www.youtube.com/watch?v=tk9FTdKOL5Q),

-   it connects neural networks to statistical physics in a deep and insightful way,

-   it is a classical and instructive prototype representative of a large class of computational architectures in machine learning and theoretical physics, the class of energy based models of neural computing,

-   it helped paving the way for deep learning,

-   it is mathematically very transparent, and looks so natural and elegant that nobody (at least, no mathematician or theoretical physicist) can remain untouched by its charms.

This sounds as if there must be a drawback. Indeed there is. Training and using a Boltzmann machine is computationally very expensive. This has barred a widespread use in machine learning (except for the early years of deep learning, say between 2006 and 2012, when the Boltzmann machine was competing with MLPs and CNNs for realizing deeply layered neural learning systems). Although in those exciting years the freshly born DL community was using a computationally streamlined version, the restricted Boltzmann machine (RBM), ultimately the computational costs (among other reasons) led to a depreciation of BM/RBMs in machine learning — though there is still some research on them in the DL world. However, biological brains may be able to do the core computations which are so expensive in the BM (namely, sampling operations) at no extra cost, just by exploiting neural noise. Therefore, the BM and related models retain their fascination for theoretical neuroscience, the theory of cognitive computing, and maybe for future non-digital neural microchips.

In order to understand BMs, one must know two formal concepts from statistics and statistical physics, namely the Boltzmann distribution (which gave the Boltzmann machine its name) and the notion of a sampling algorithm. These concepts are also forming the basis for the general, large class of energy-based models. It is thus a well-invested effort to make friends with these two concepts. The class of energy-based models include, for instance,

in theoretical physics:

    so-called spin-glass or Ising models, which describe how solid-state materials change their properties under the influence of external controls like temperature, electric or magnetic fields;

in statistics and classical pattern recognition, especially computer vision:

    so-called Markov random fields, a 2D or 3D generalization of Markov processes, which can be used, for instance, for photographic image processing;

in the cognitive neurosciences:

    various concrete computational models for explaining hierarchical information processing in brains, where bottom-up sensor data processing and top-down attention and expectation mechanisms interact – in fact, one of the current leading paradigms in cognitive science is rooted in energy-based formalism, namely the free energy principle of learning in intelligent agents;

in optimization theory / operations research:

    the simulated annealing algorithm for finding a good local minimum in complex performance landscapes – besides evolutionary search methods (“genetic algorithms”) this is a last-resort, general-purpose optimization algorithm when all others falter in the face of hypercomplex performance landscapes;

in machine learning:

    a major branch of machine learning, namely Bayesian networks and the more general class of graphical models uses energy-based mechanisms to implement learning architectures and mechanisms for rational reasoning on the basis of stochastic sensor input / stochastic input data. These models have a wide range of uses which is orthogonal to the use cases of deep learning. This line of modeling is therefore not dimmed by the deep learning revolution, a fate from which so many other fields in machine learning have suffered.

Before we can start enjoying the Boltzmann machine, and before we can take a closer look at some of these other kinds of models, we must equip ourselves with a fair understanding of the Boltzmann distribution and the idea of sampling algorithms. This is what I will try to give you in this section.

The Boltzmann distribution

The Boltzmann distribution is a classical concept from the field of statistical physics. So, first question: what is statistical physics?

In statistical physics (SP), the general objective is to explain macroscopic phenomena from the interaction of very large “ensembles” of microscopic particles. “Macroscopic” means that something can be measured with instruments of everyday size: you can measure the temperature in a pot of water with a thermometer, you can measure electric fields with a voltmeter, etc. These instruments have sizes that you will find them again after you have put them away on a shelf – they belong to the macroscopic world. A “Microscopic” particle, in contrast, is so small that one cannot measure it with instruments and sensors of human-hand-manageable size (e.g. the current velocity of a water molecule); and sometimes one cannot measure it at all (quantum effects).

Two examples of how statistical physics connects the microscopic to the macroscopic levels of description:

-   The temperature (macroscopic) is explained by / reduced to a the average kinetic energies of atomic particles (microscopic) contained in a vessel or block of solid matter.

-   The magnetic strength (macroscopic) of a magnet is explained by / reduced to the average spatial alignment of the spins (microscopic) of the atoms in the crystal lattice of the magnet.

Macroscopic properties of materials have been a subject of research in physics since the beginnings of that field. In particular, in a classical branch of physics called thermodynamics, the macroscopic observables temperature, pressure, volume, viscosity and many others have been investigated, and many laws of how they depend on each other in different materials have been found. These laws are particularly interesting and nontrivial when it comes to phase transitions — sudden changes of macroscopic observables, for instance when water freezes or dynamite explodes.

These laws could be found, formally stated and experimentally verified in classical thermodynamics, but they could not be explained. This is what statistical thermodynamics (and subsequently, the more general discipline of statistical physics) strives to achieve: mathematically deduce the macroscopic laws of thermodynamics from assumed mechanisms of how microscopic particles interact in large numbers, such that the macroscopic observables and laws can be derived as properties of statistical distributions.

Brains are also macroscopic lumps of matter in which the interactions of large numbers of microscopic “particles” (the neurons) give rise to macroscopic observables (for instance, the words coming out of your mouth). It is a naturally inviting idea to describe brains with the tools of statistical physics. The Boltzmann machine is one way of doing exactly this.

A (rather, the) founding father of statistical thermodynamics was Ludwig Boltzmann (1844-1906, https://en.wikipedia.org/wiki/Ludwig_Boltzmann), an Austrian professor of physics and philosophy (!).

In order to start getting familiar with the Boltzmann distribution, let us consider the textbook example of a vessel filled with water.

The macroscopic description of this system is simple: volume, temperature and pressure are enough to characterize it. These can be measured with macroscopic instruments.

The microscopic description is based on the notion of a microstate. A microstate is (in a first approximation, treating water molecules as elastically rebounding balls) a specification of all the 3D position coordinates and 3D velocity components of all individual H₂O “balls” in that vessel. This is a very high-dimensional real-valued vector, say of dimension d, which completely characterizes the molecular-level state of affairs inside the vessel at given moment in time.

Boltzmann asked, and answered, the following fundamental question: what is the probability distribution of these microstates? — and from that distribution he inferred the laws and values of macroscopic observables.

First let us understand that this distribution is not uniform. Some kinds of microstates are more probable to occur than others. The water molecules bounce against each other and the vessel’s walls, exchanging impulses, in a wild stochastic dance. Boltzmann assumed that the vessel is submerged in a heat bath, which you can visualize as an infinite ocean of water that has the same temperature as the water in the vessel and the vessel’s walls. Ocean molecules bounce against the vessel walls, vessel wall molecules transmit these impulses to water molecules inside the vessel, and conversely there are stochastic transmissions of molecular impulses from the inside to the outside. Just by stochastic coincidences, at some moments large amounts of impulse energy will have found their way from the outside to the inside; and at other moments, less of it. A microstate’s (kinetic) energy is the sum of all the kinetic energies of the individual water molecules in the vessel. We don’t have to understand how the kinetic energy of a “ball” with a given mass and velocity is defined (you remember from high-school physics?). The bottom line is that the energy of microstates is wildly all the time due to the random exchange of energy with the heat bath.

Side remark: this energy fluctuation is due to the embedding of the vessel in a heath bath. If the vessel would be perfectly isolated, then the energy of all possible microstates would be constant due to the law of conservation of energy. The statistical physics of open versus isolated systems are quite different from each other!

Let us denote microstates by s and the energy of a microstate by E(s). Now we have put our foot on the doorstep of one of the grandest and most far-reaching principles of modern physics. Boltzmann reasoned that the probability of a microstate depends only on the microstate’s energy E(s) and the temperature T of the vessel/heatbath system. Note that physicists measure absolute temperature (in Kelvin), where zero is the lowest possible temperature. The probabilities of microstates are described by a simple formula which today is called the Boltzmann distribution. Since the positionvelocity vector microstates are continuous-valued vectors, this distribution is written down as a pdf which assigns a probability density value p(s) to each microstate:

  p(

In this equation, Z is the normalizing factor that ensures that the pdf p integrates to unity:

  Z = 
where S is the space of all possible microstates. I am not a physicist but I would think that S = ℝ_( ≥ 0)^(d) comprises all non-negative real-valued vectors of dimension d, where d = 6N and N is the number of particles in the vessel and the factor 6 comes from the three location and velocity coordinates of each particle.

Note that both p and Z depend on the temperature T, so we sometimes write p(s, T) and Z(T). Z is called the partition function in statistical physics (“function” because it depends on the temperature). The partition function plays a central role in physics. In most cases it cannot be calculated analytically. One needs supercomputing power to approximately estimate it, and the possibility to actually do this has changed the face of modern physics (computing Z is one of the reasons why physicists need supercomputing facilities so badly). Computing estimates of Z is also of importance in certain applications of machine learning, deep learning in particular. The entire Chapter 18 of the deep learning “bible” is devoted to estimation algorithms for Z. Luckily, in many applications, among them the Boltzmann machine, Z cancels out and need not be computed.

The Boltzmann distribution can also be defined on spaces of discrete microstates (finitely or countably many). Then the pdf from [eBoltzmannpdf] turns into a probability mass function (pmf) and the integral in [eBoltzmannZ] into a sum:

  P(
             
  Z   =   

The shape of the pdf (or pmf) changes quite dramatically when the temperature parameter is varied. Figure 61 shows an example. There are two noteworthy extreme cases. When the temperature is very high, the terms  all approach 1, which (after normalization by division with Z) gives a uniform distribution. At high temperatures, all microstates become almost equally probable. When the system is cooled down toward zero, the probabilities of the low-energy states grows relative to the probability of the high-energy states. Ultimately, the distribution converges to a point distribution where the (single) lowest-energy state has a probability of 1. Cooling a physical system down toward zero will localize its distribution at the lowest energy state! It should be noted however that this cooling down must be done veeery slowly, called adiabatic cooling — in the limit, even infinitely slowly — in order to see this effect in real physics experiments.

[A Boltzmann distribution over a finite space of 100 microstates. The distribution is given by its pmf (red). The underlying energy function, which is not changing with temperature, is rendered by a blue broken line. ]

The fact that a (sufficiently slow) cooling of a Boltzmann system leads to the global minimum of the energy landscape is the basis of the simulated annealing algorithm. This is a general-purpose optimization algorithm which can find, in principle, the global minimum of any performance surface. In this respect it is vastly more powerful than gradient-descent optimization algorithms which we discussed in Section 2.2.2. The simulated annealing algorithm is really good stuff to know and I will explain it in more detail in Section 6.4.

Two facts about the Boltzmann distribution worth knowing:

-   If an energy function E : S → ℝ gives rise to a Boltzmann distribution P, and E′ : S → ℝ, s ↦ E(s) + C is the same energy function shifted by some constant C, then the Boltzmann distribution P′ you get from the shifted energy function is the same as you had before: P′ = P. As a consequence, for the purposes of using Boltzmann distributions for neural-computational purposes or in simulated annealing, one can use “energy” functions that also have negative values, which wouldn’t be possible in real physical systems.

-   A Boltzmann distribution arises from an energy function at a given temperature T. Conversely, if P(s) is any (discrete) probability distribution which is nonzero for all s ∈ S, for any temperature T the energy function E(s) =  − T log (P(s)) makes P(s) the Boltzmann distribution associated with E(s). Every (globally nonzero) probability distribution can be written as a Boltzmann distribution of some energy function! As we will see in Section 7, this makes BMs universal approximators of (discrete, never-zero) probability distributions.

While the Boltzmann distribution has been found and explored in physics, its ideas and maths transfer to any other application domain where there are macroscopic systems which can switch between large numbers of microstates, each of which has an “energy”. In particular, neural networks (artificial ones and real brains) can be regarded as macroscopic systems, with vectors of activations of all neurons being the microstates. If one defines some sort of “energy” for the activation vectors, the Boltzmann machinery can be launched. The “energy” which one defines can be any function from microstates to the reals; physical considerations can be ignored.

Sampling algorithms

In this subsection I will first explain in plain English what a sampling algorithm is, then why they are important, and finally in formal terms how the specific sampling algorithm works that is used for the Boltzmann machine.

What is it?

A sampling algorithm, or for short a sampler, is a computational procedure which generates “random” examples from a given distribution. Samplers there are many. Some are highly specialized and can only generate random examples from a single distribution, others are generic and can “sample from” any distribution which is given by a pdf or pmf or energy function.

You all know a sampling algorithm that samples from the uniform distribution over the interval [0, 1]. It comes as a ready-made function with all programming languages that I know, including MS Word. It is typically named rand. Every time you evaluate this function, it generates a new “random” number from the interval [0, 1].

In fact, the algorithms that sit behind rand are deterministic, and the outputs from rand are only pseudorandom numbers. Many algorithms are known which deterministically generate numbers between [0, 1] in a way that can hardly be distinguished from true randomness. It is quite a sophisticated corner of maths where such clever “pseudorandom generators” are thought out. We just use them as if they generated truly random outputs and don’t think twice about the math miracles behind the curtain which turns digital determinism into (almost) randomness. If you want to get true randomness, for instance for unbreakable codes, you’d need to build a non-digital microchip that gets its randomness from quantum fluctuations.

[Sampling from a 2-dimensional pdf. The pdf is here rendered by a contour line plot; it consists of two “hills” separated from each other by a “valley”. Each blue dot is one output of a sampling algorithm. Graphic taken from the online course notes of IEOR E4703: Monte-Carlo Simulation, Columbia University https://martin-haugh.github.io/files/MonteCarlo/MCMC_Bayes.pdf where they were taken from the book Bayesian Reasoning and Machine Learning by D. Barber.]

Formalizing the concept of a “sampler” in mathematical rigor requires some stochastic processes theory and is beyond the scope of our course. But I think the idea is intuitively clear. Consider a pdf as a “landscape” with hills and valleys (as the two-hill landscape mapped in Figure 62). A sampler is a mechanism which, each time it is executed, lets fall down a grain of sand on a plane that was flat and empty at the beginning. In the long run, these grains of sand should pile up to a landscape whose profile is the same as the pdf landscape.

Note that a sampler need not “jump around” wildly. Modern sampling algorithms indeed have some kind of memory. They linger for a while in the vicinity of the place where the last grain of sand was dropped: the next grain will typically fall down somewhere close to the previous one. Such samplers perform a random walk over the sampling space, dropping a grain at each step, and the average progression rate of that walk can be slow. The entire landscape is covered only in the long run. The sampler that we will be using for the Boltzmann machine is of this kind. The general theory behind the design of such samplers is called Markov Chain Monte Carlo (MCMC) sampling. Once powerful computing hardware became available, MCMC changed the face first of physics, then of other sciences, because these methods (and only these methods) made it possible to simulate complex stochastic systems in physics, chemistry, biology, economics and the social sciences. We cannot and need not dig deeper here. If you are interested: a classical tutorial is , and in my legacy lecture notes on “Algorithmic and statistical modeling” I give a more in-depth outline (Section 4.5 in https://www.ai.rug.nl/minds/uploads/LN_AlgMod.pdf).

What are they good for?

Probability distributions are the raw material of all scientific research or practical or economical exploits when it comes to dealing with uncertainty in data. Often these distributions are defined over very high-dimensional state spaces, like brain states (activation vectors of all neurons), epidemiological states (healthy / ill assignments to the citizens of the world), or the spatial configurations of a folding protein. It is impossible to “write down”, or plot, or even imagine the global geometric shape concerned pdf or pmf. It is a very misleading experience which students of statistics or machine learning get from their textbooks, where all graphics show only one- or two-dimensional pdfs or pmfs — because onlythese can be readily visualized! This textbook reading experience falsely leaves the student with the impression that seriously real-life distributions can be described in terms of intuitively understandable geometric shapes, like the Gaussian bell curve. This is, in general, not possible! And worse yet, it is not only impossible for humans to get an intuitive visual idea of the shape of a complex distribution. It is, as per today, likewise impossible for mathematical formalism to characterize the overall “geometry” of most real-life pdfs or pmfs.

The only thing that usually can be computed is just the individual pdf or pmf value of a given point in the state space of the distribution. That is, for any point s in a high-dimensional state space S, it is possible to compute the corresponding pdf value p(s) or the pmf value P(s).

Oh... no. Not even that is usually possible. The enemy here is numerical underflow. For a demonstration, consider a probability distribution over the set S of possible binary health states of the world population. Assume for simplicity that 10 billion humans live on our planet. Each of them can be ill or healthy — 1 or 0. A global health status vector is thus a binary vector of dimension 10e10. This makes for a large but finite state space S = ^(10, 000, 000, 000). Since S is finite, a probability distribution over this space has to be described by a pmf. The probability P(s) of a state s is, on average, 2^( − 10, 000, 000, 000). Written to base 2, which is what digital computers do internally, this would be written as 0.0…01, with 10, 000, 000, 000 digits after the dot. But the machine precision on your 64-bit computer allows only for 53 bits of precision. Any number smaller than 2^( − 53) is treated as zero by your computer.

A standard escape from such numerical underflow issues is to always work not with the raw probabilities P(s) but with their log values log (P(s)). In base 2, the average log probability then is log₂(P(s)) =  − 10, 000, 000, 000, an order of magnitude (rather, minitude) that is within convenient precision reach of your machine. If you read math-oriented papers in machine learning you will see log probabilities all over the place.

Ok., let us return to our main thread. I said above that the only thing that can be computed for a distribution given by a pdf or pmf are the values p(s) or P(s), or rather their logs. Let us return to the global health status vector example. An epidemiologist might want to know, what is the probability Q that more than one tenth of the world population is ill? This turns out to be a computational show stopper. Let S_(Q) ⊂ S be the set of all health state vectors that have more than 10 percent 1’s entries. The mathematical formula that defines Q is
Q = ∑_(s ∈ S_(Q))P(s).

This is not computable for two reasons: numerical underflow, and the gigantic size of S_(Q).

But now assume you had a way to get a “fair sample” of manageable size, drawn from the distribution P. Say, you have “drawn” 1,000 examples s₁, …, s₁₀₀₀. You count how many of these example vectors have more than 10 percent 1’s in them. Your count number is q. Then, thinking about it, you see that q/1000 is an estimate of Q! This estimate will become more precise if you collect larger samples, converging to the correct probability Q as your sample size goes to infinity.

This example demonstrates one of two major things that sampling is good for: sampling can be used to get estimates of probabilities for events that a researcher is interested in. Similarly, sampling can be used to get estimates of other statistical quantities, like expectations, variances, partition functions, or all sorts of integrals over functions over the sample space. And there is no other way than sampling to get access to these quantities of interest. I think you can divine what a game-changer in the sciences it was to afford of both general-purpose sampling algorithms and computing hardware with enough bandwidth.

There is an angle to this success story which is worth knowing and thinking about. Both the mathematical development of general-purpose sampling algorithms, and the computational exploit of them on the first powerful enough digital computers, were done in the Los Alamos Labs in the context of developing the hydrogen bomb. See https://www.atomicheritage.org/history/computing-and-manhattan-project for a historical outline and for the landmark scientific publication (42K Google Scholar cites).

The other major thing that sampling is good for: it’s just the sampling itself. It can be used to generate examples from a statistical model of some interesting part of the world. When the Boltzmann machine is used after it has been trained, this is the way that it is used. In cognitive terms: the random walk of an artificial brain state sampler creates a “stream of thought” which in the long visits all the places and themes that the brain knows about. The technical term for this process, used both in psychology and machine learning, is confabulation.

The Metropolis algorithm

Today many general-purpose sampling algorithms are in use. Their common ancestor is the Metropolis-Hastings algorithm, often named just “Metropolis algorithm”. It was developed in a joint effort of eminent mathematicians and nuclear physicists. The classical reference is . Metropolis sampling works with a particular elegance in conjuction with Boltzmann distributions, and it is the sampler that is used in the Boltzmann machine. In this subsection I describe this sampler in detail.

The Metropolis sampler is applicable in very general situations. All that is needed is some set (or “space”) S of possible “states” of some modeled system, and for each state s ∈ S a computable pdf value p(s) (for continuous state spaces) or a computable pmf value P(s) (for discrete spaces). In fact, even less need be given: it is enough to have the pdf or pmf only up to some unknown normalization factor. The Metropolis sampler only needs the ratios p(s)/p(s′) or P(s)/P(s′) to run, and these ratios remain the same if the pdf or pmf is scaled by some constant factor. This also makes it unnecessary to compute partition functions, because they cancel in these ratios.

I will present the Metropolis sampler for the case of a given pmf, because that is the situation we will meet in the Boltzmann machine. The pdf case is entirely analogous and you can easily translate the pmf recipes into pdf recipes.

So, here is the scenario. We are given a finite state space S and a non-negative function F : S → ℝ^( ≥ 0) whose sum ∑_(s ∈ S)F(s) is finite. I call this function a proto-pmf because it could be turned into a pmf P by scaling it with 1/Z = 1/∑_(s ∈ S)F(s), but this normalization is not needed (and often not feasible), so we stick with the proto-pmf F.

The task is to generate a potentially endless sequence s¹, s², … such that, in the long run, this sequence of sampling points would re-model the pmf landscape in the intuitive sense of our “grain dropping” metaphor.

I do not give the mathematical derivation of why the Metropolis sampler does its job, but just describe the algorithmic procedure. If you are interested in the mathematical derivation, you can find it in Section 4.7 in my legacy lecture notes on Algorithmical and Statistical modeling, online at https://www.ai.rug.nl/minds/uploads/LN_AlgMod.pdf, or the MCMC tutorial of .

The Metropolis algorithm generates the sample point sequence s¹, s², … by implementing a Markov process mechanism, that is, every newly generated point s^(n) depends (only) on the predecessor s^(n − 1) (note: a Markov process generalizes discrete, finite-state Markov chains to continuous state spaces). In order to get the whole sequence started, you have to “guess” the initial point s¹. You can pick it arbitrarily.

Thus, assume s^(n) has already been computed. In order to compute s^(n + 1), the Metropolis algorithm executes two steps, each of which contains a cheap-to-compute random decision:

Step 1: randomly propose a candidate s^(*) for s^(n + 1).

    For this step one needs to design and implement a mechanism to sample from a proposal distribution. Each instance of a Metropolis algorithm comes with such a mechanism. Mathematically, a proposal distribution is a conditional distribution over S which gives the probability to choose s^(*) given s^(n). Let us write 
      

    The proposal distribution should be chosen such that one can sample from it cheaply. For instance, it could be the uniform distribution on a small hypercube centered on s^(n), or a multidimensional Gaussian distribution centered on s^(n). Efficient samplers exist for such elementary distributions.

Step 2: randomly accept or reject the candidate s^(*).

    If s^(*) is accepted, it becomes the next output of the sampler, that is s^(n + 1) = s^(*). If s^(*) is rejected, it is discarded and the sampler repeats the previous value in its next output, that is s^(n + 1) = s^(n).

    This requires a decision-making subroutine, whose choice is again random. Several such decision-making procedures, called acceptance functions, are known which result in sampling point sequences s¹, s², … which asymptotically re-model the “landscape” of the proto-pmf F. Here are the two most common and famous ones:

    The Boltzmann acceptance function

        computes an acceptance probability by the formula
        
              P
              

    The Metropolis acceptance function

        computes the acceptance probability by
        
              P
              
                       1, 
                       
                     
        that is, the Metropolis acceptance function accepts the proposed candidate with certainty if its proto-pmf is larger than the one of the previous value, and it accepts it with probability F(s^(*))/F(s^(n)) if the F-value of the candidate is lower than the one of the previous sample point.

    Both acceptance functions can be computed solely on the basis of the ratio . The Boltzmann acceptance function can be re-written as
    P
        
    and the Metropolis acceptance function as
    P
        
                                   1, 
                                   r  
                     

    The computation of 
                   
                   
                       accept(

I conclude this subsection with a few remarks to round off the picture.

-   In order to guarantee that the Metropolis algorithm, as outlined above, indeed yields a sampling point sequence which correctly re-shapes the F-landscape in the long run, some additional conditions must be satisfied.

    -   A necessary condition is that the “random walk” process which yields the sampling point sequences is ergodic. This is an involved concept from information theory which is beyond the scope of this course. In intuitive terms this means that every point in S can be reached by some sample point sequence at some time, regardless of where the sequence was started. This condition would be violated, for instance, if the F landscape is zero everywhere except at two “hills” H₁, H₂ which are separated from each other by a zero-F “lowland” of width w, and the proposal distribution is chosen such that it always proposes a candidate s^(*) which is closer to s^(n) then w. If the sampling sequence is started in the hill H₁, it can never cross the flatland to H₂. Checking whether a Metropolis sampler that one has designed has the ergodicity property is nontrivial and there is no general recipe to assure this property.

    -   Not every proposal distribution will yield a valid Metropolis sampler. Assuming ergodicity, a sufficient condition to obtain a proposal distribtution that makes a valid Metropolis sampler for F is to require that P
              P

-   The proposal distribution is the heart of a Metropolis algorithm. If it is designed poorly, the sampling process will take a long time to “cover the grounds” of the probability landscape. Some thinking is needed to design a good proposal distribution. If  is allowed to often propose candidates s^(*) that are far away from s^(n), the danger is that one lands in a low-probability zone of the landscape, which means that the candidate is rarely accepted and the sample point sequence has many repeated points. The net effect is slow coverage and long required sampling times. If the proposal distribution mostly suggests candidates from the close vicinity of the previous point, the resulting random walk will also be too slow. Thus, the art lies in finding a proposal distribution which jumps far — but mostly hits candidates that have a high probability. Finding such a proposal distribution needs insight into the nature of the probability distribution that one wants to sample from.

-   Often one wishes to create a sampler which produces an independently, identically distributed (i.i.d.) sequence of sample points s¹, s², …. That is, the choice of point s^(n + 1) should be statistically independent of the previous point s^(n) — the sampling point sequence should have no “memory”. But the proposal / acceptance mechanism of Metropolis sampling (and any other MCMC technique) makes the choice of s^(n + 1) depend on s^(n). The solution for this problem is to subsample the sequence generated by the sampler, that is, instead of using the original sequence s¹, s², …, retain only each h-th point, recording only s^(h), s^(2h), ….

-   A commonly used special variant of Metropolis sampling (and other MCMC samplers) is called Gibbs sampling. It can be employed when the states s ∈ S are vectors s = (s₁, …, s_(d))′. In Gibbs sampling, the proposal distribution changes only one component s_(i) of s at a time, cycling through the indices. That is, for s^(n) = (s₁^(n), …, s_(d)^(n))′, and change index i, the proposed candidate is of the form s^(*) = (s₁^(n), …, s_(i − 1)^(n), s_(i)^(*), s_(i + 1)^(n), …, s_(d)^(n))′.

-   The Metropolis sampler with the Metropolis acceptance function harmonizes nicely with the Boltzmann distribution. Recall that a Boltzmann pmf is given by  in [eMetropolisAccept] becomes
    
      r = 
      
      
    The acceptance probability depends only on the energy difference between the previous and the proposed microstate (and the temperature). That is, in order to carry out Metropolis sampling over a Boltzmann distribution defined by an energy E, all one needs to know is the energy; one can forget about the probabilistic framework around it.

Simulated annealing: principle

It would be a waste to introduce the Boltzmann / Metropolis framework only for the purpose of discussing the Boltzmann machine. Metropolis sampling has many important applications other than that, and some of these are very important indeed. From the listing that I gave at the end of the introduction to this section, I pick simulated annealing.

Simulated annealing is a general-purpose optimization algorithm. An optimization task, in the most general setting, consists of a search space S and a cost function R : S → ℝ. The goal is to solve the cost minimization problem




This is (please remember!) the form of the neural network training objective in supervised learning, where the states s would be parameter vectors θ of a NN and the cost function R would be the (empirical) risk.

But the minimization problem [eCostMinProblem] is absolutely general and could, for instance, mean the task to minimize financial loss in stock market transactions; or to find a protein folding which minimizes the energy of the resulting 3D molecular structure (which is what nature does, and which gives rise to the proteins you are made of, and which is a major task in biochemistry research).

Solving [eCostMinProblem] analytically is out of the question in many real-world optimization tasks. If the cost function is differentiable one can try to solve [eCostMinProblem] by gradient descent, which is computationally often quite feasible but which can spot only local cost minima. If one wants to find the global minimum, one has to “search” through the entire search space S. A systematic grid search is infeasible if the states s are high-dimensional vectors because the number of grid points in a search sub-volume of ℝ^(d) explodes exponentially with d. In this situation one needs a “clever” random search method which explores the search space in a way that low-cost candidates are (much) more often tried than high-cost ones, while making sure that the search does not become trapped in some subvolume of the search space S but will visit all parts of it.

Two particularly popular families of such “clever” stochastic search techniques are evolutionary optimization with the special case of genetic algorithms, and simulated annealing. Both approaches are inspired by nature:

Evolutionary optimization

    mimics how natural evolution finds “solutions” (= species) that are highly adapted to their ecological niches. The cost function here is (the inverse of) biological fitness. The main principle of evolutionary optimization is to compute a sequence of “generations” G¹, G², …, where a generation is a set of individual solution candidates (animals or plants in the biological world, network parameter vectors θ for us) G^(n) = . The next generation is derived from the previous generation by some “procreation” mechanism which favors parents that have a high fitness, and introduces some random variation in the offspring generation. See https://en.wikipedia.org/wiki/Evolutionary_computation for an introduction.

Simulated annealing

    (SA) mimics the behavior of a lump of material which is slowly cooled down from its gaseous phase until it crystallizes. The cost function is the energy of microstates. If you take another look at Figure 61 you will see that at low temperatures the Boltzmann distribution concentrates around the global minimum of the energy (cost) landscape. The core idea of SA is to consider the cost function as an energy function, then run an extended Metropolis sampling from the Boltzmann distribution associated with that energy function, starting with a high temperature (which facilitates an exploration of the entire state space), then slowly cooling down which will nudge the search toward the global minimum. The naming of this procedure, simulated annealing, comes from metallurgy where a slow cooling of heated metals leads to the formation of large crystals which harden the material.

I’ll now give a more detailed description of SA, again for the case of discrete search spaces where the Boltzmann distribution is characterized by a pmf. Here is an outline of the complete process:

1.  Identify the points of the search space S with microstates s of an artificial “thermodynamical” system.

2.  Identify the cost function, of which a global minimum should be found, with the energy function E(s).

3.  Start with some medium or high temperature T₀ and consider the Boltzmann distribution P(s, T₀), which will be close to uniform (as in the first panel of Figure 61). Start sampling from this distribution with the Metropolis algorithm. The sequence of created samples will cover the search space almost uniformly.

4.  Now lower the temperature gradually, thereby obtaining a sequence of Boltzmann distributions P(s, T₀), P(s, T₁), P(s, T₂), … which more and more concentrates around the microstates that have low cost/energy values. Continue sampling all the time. The sequence of samples should thus concentrate more and more on low-cost microstates.

5.  Continue until T_(n) ≈ 0. The hope is that then the cooling process has guided you toward the global minimum and that the samples that you now get are closely scattered around that global minimum.

A natural question at this point is, why not start immediately at low temperatures (e.g. in a situation like that shown in the 4th panel of Figure 61), wouldn’t that just save the time of “bouncing aimlessly” around in the search space at high temperatures, and instead directly lead you to the desired minimum, which is well pronounced at low temperatures? The answer is, if one starts at low temperatures — or, for that matter, if one cools to rapidly — one is likely to get “frozen” in a very suboptimal local minimum far from the best one, from which one cannot escape. This becomes intuitively clearer if we re-interpret the Metropolis sampling of P(s, T_(n)) in physical terms of “jumping around” in the energy landscape E(s) directly.

To see this, we consider the two cases when (A) the Metropolis algorithm accepts with certainty, and (B) when it accepts with probability 


Case A:

    This case occurs when P(s^(*), T) ≥ P(s^(n), T). This is equivalent to the condition E(s^(*)) ≤ E(s^(n)). Thus, whenever the energy of the proposed state is lower than the energy of the previous state, accept with certainty.

Case B:

    If P(s^(*), T) < P(s^(n), T), then 
      
    where ΔE < 0 is the energy difference between s^(n) and s^(*).

Summarizing we see that in terms of energy, a new proposed microstate is accepted with certainty if its corresponding energy jump goes “downhill”, and if it goes uphill, it is accepted with probability . That is, the greater the energy increase, the accept (exponentially) more unlikely is such a step taken; however, this may be compensated by a proportional increase in temperature. In other words, on the average, at higher temperatures we may take higher jumps uphill.

Equipped with this re-interpretation of the Metropolis algorithm in terms of an energy-based acceptance function, we can better understand why slow cooling is important for a final landing in a good local minimum of the energy landscape. We can now intuitively interpret the SA search process as a random jump sequence of a “search ball” in the energy landscape E(s), where the temperature determines the ability of the ball to (randomly) climb uphill and in this way overcome “energy barriers”. Figure 63 illustrates the different behavior of the SA search process at different temperatures.

[SA seen as an energetic ball game. At high temperatures, the ball can more easily jump high and overcome energy barrier than at low temperatures.]

We are now aware that the cooling process is important for the success of running an SA algorithm. A widely used, quick and dirty cooling scheme is exponential cooling: Put T_(n + 1) = k T_(n) for some k < 1 close to 1. Update T after every single sample point. Clearly the size of k is important — a typical way to determine it would be just to experiment.

However, such simple cooling schemes, although widely used, may yield unsatisfactory results. During a SA search run, one may encounter periods where a particularly slow cooling is required, while at other periods, one may cool faster. I will first explain this fact intuitively and then give a formal account.

Here are two intuitive examples that illustrate the necessity for slower-than-others cooling periods. The temperatures where particularly slow cooling is required are associated with phase transitions, like when water freezes at temperature T = 273.15 Kelvin.

The original physical metaphor: simulated annealing.

    In metallurgy and chemistry, “annealing” refers to a process of carefully cooling a liquid into crystallisation. A general observation is that if a liquid is cooled very quickly, the resulting solid will consist of many fine-granular microcrystals. By contrast, if the cooling is done slowly, large crystals or even a single solitary crystal result. Specifically, the cooling must be slow (heat being withdrawn from the liquid at a low rate) around the solidification temperature, because it is at this temperature that the final crystal structure is determined. You might know from own experience with deep-freezing condiments or producing ice-cream that fast cooling across the solidification temperature produces a substrate structured into many small crystals, whereas slow cooling results in fewer and larger crystals. This is important in both ways in many applications: for instance, when deep-freezing biological specimens (seeds, live cells) it is crucial to shock-freeze the material very rapidly, avoiding the growth of larger ice crystals which would destroy cell membranes; or in the industrial production of silicon wafers needed for microchip production, an almost or perfectly monocrystalline block of solid silicon is very slowly pulled out of the melted mass. In terms of energy landscape: large crystals corresponds to microstates of low energy (with a monocrystalline block corresponding to globally minimal-energy states). Therefore, slow cooling around the critical solidification temperature is a prerequisite for low-energy final products. When the molten material is still significantly hotter than the solidification point, it can be cooled fast; similarly, once it is solidified, further cooling will change the crystal structure only a extremely slow timescales which make further cooling practically meaningless.

Human problem solving.

    When solving some magazine puzzle question or a math homework, you will probably have experienced something similar to a phase transition. After an initial thinking phase where you have no clue of how to solve the problem and think of many possible approaches (= a high temperature search phase), the inklings of a solution appear on the horizon (= close to the phase transition), and suddenly aha! you lock in to a particular approach (= beyond the transition) from which it would require quite some effort (= mental re-heating) to escape. If you cooled to quickly (= decided for a solution strategy too early, too quickly) your approach is likely to fail (= lead to a quite suboptimal minimum); if you spent time to consider different solution options (= hover around the critical temperature) and then sloooowly decided, your chances of hitting a good solution strategy are much higher.

There is a physical / mathematical indicator of such phase transition temperatures when it is important to cool very slowly. I use concepts from thermodynamics without further explanation. The remainder of this subsection is optional reading.

The free energy of a system at temperature T is
F(T) = E_(T)[E] − T 𝒮(T),
where E_(T)[E] is the average energy at temperature T,

and 𝒮(T) is the entropy of the system at temperature T,
𝒮(T) =  − ∑_(s ∈ S)P(s, T) log (P(s, T)).

Thus the free energy relates the average energy at temperature T with the entropy. Intuitively, the free energy of a system is the “useable” part of its energy, energy that could be exploited at a macroscopic scale (for example, the free energy of a volume of gas would be the energy that one could exploit by expanding the gas in a piston, plus the energy that one might gain from cooling the volume). Phase transitions are defined in physics as discontinuities in the free energy (or one of its derivatives) as temperature (or another macroscopic variable) passes a critical value (check out https://en.wikipedia.org/wiki/Phase_transition). For instance, as a volume of water is cooled from some ε value above zero Celsius to some ε value below, such that it freezes, one has to extract a certain amount of energy (the melting heat) from that volume — one may in fact exploit this energy; it is part of the free energy of the volume of water. Therefore, the free energy of the volume of water just above zero jumps discontinuously to a lower value as the water is cooled to a temperature just below freezing.

Similarly, when running SA for an optimization problem, one can in principle compute, at every step n, the free energy F_(n) of the system, and make the cooling rate depend on the development of the free energy: cool slowly when F_(n) shows signs of changing rapidly, or in other words, cool in a fashion such F_(n) decreases smoothly.

The free energy can be computed from the partition function Z by F(T) =  − T log (Z(T)). So the question is, how can one compute the partition function, which is a gigantic sum (for discrete systems) or an intractable integral (for continuous systems)? The brutal answer is: use sampling (!) for an approximate evaluation of this integral. There are a number of specialized sampling procedures for the partition function, surveyed in and Chapter 18 of . This method of steering the cooling is obviously very expensive: within an SA run (which may have millions of steps), we repeatedly have to squeeze in complete auxiliary sampling runs.

It can be mathematically shown that if the cooling is done slower, on average across different SA runs one ends in lower-energy minima of the energy landscape. In the limit of infinitesimally slow cooling, SA is guaranteed to find a global minimum.

I conclude this section with two examples. The first one is in many ways representative for many SA applications in combinatorial optimization tasks; the second is just for fun.

Simulated annealing: examples

Optimizing computer hardware layout: circuit partitioning subtask

The first example is taken from the pioneering paper on SA (47K Google Scholar cites). According to Kirkpatrick et al., whose paper I closely follow in this subsection, in computer hardware layout one is confronted with a number of subproblems that build on each other, with circuit partitioning being the most elementary (the article also treats the subsequent optimizations of metrical placement and wiring). In the example described in , the partitioning task is to distribute a set of about 5000 elementary computational circuits (together forming a complete CPU architecture) over two microchips such that (i) the number of input/output pins of the chips is small and (ii) the circuits are distributed approximately in equal numbers across the microchips.

Formally, this task can be specified through describing possible distributions of the 5000 circuits on the two chips by microstates s, where each microstates s is 5000-dimensional binary vector with entries . The i-th entry s_(i) is set to  − 1 if the i-th circuit is assigned to the first chip, and it is set to  + 1 if it is assigned to the second chip. The cost function must reflect the two requirements (i) and (ii). Expanding on the treatment in the original article (it provides no detail), this can be done as follows:

For (i), consider a symmetric 5000 × 5000 matrix (a_(ij)) with 0-1 entries, a value of a_(ij) = 1 indicating that circuits i and j directly exchange a signal (and hence, if placed on different chips, require a pin at each chip). For a given partitioning-encoding microstate s, the number of signals that must cross between the two chips is

  j

The second sum term is independent of the circuit placement and can be dropped from the energy function, because it does not affect the location of its minima.

For (ii), the objective function should grow with the degree of imbalance of circuits assigned to the two chips. The squared imbalance score (∑_(i)s_(i))² is equal to 2∑_(i > j)s_(i) s_(j) + ∑_(i)s_(i)². Again, the second term is independent of the placement and can be dropped.

Assembling these two cost contributions and replacing the constant 2 by a weighting factor λ one gets a cost/energy function of the form
E(s) = ∑_(i > j)λ s_(i) s_(j) − ∑_(i > j)a_(ij) s_(i) s_(j).

The SA scheme used in involved a proposal distribution that simply flipped the assignment of a randomly chosen circuit. The temperature was lowered with a factor of 0.9 from one temperature to the next lower one. Starting at T = 10, at each temperature in the order of 500,000 flips were executed, until a temperature of 0.1 was reached (from which I infer a total runlength of about 20,000,000 updates). Figure 64 summarizes the distributions of number of pins obtained at different temperatures. As expected, the average number of pins sampled at decreasing temperatures decreases, as does the variance of that number. At the lowest temperature, the sampling has frozen into apparently a single (or very few) solution(s).

[Distributions of total number of pins at various temperatures. The arrow inserted at the x-axis indicates the average number of pins obtained by a greedy search algorithm (Metropolis sampling with T = 0). Figure taken from the Kirkpatrick et al paper.]

Laying a jigsaw puzzle

In my 2012 course “Statistical and Algorithmical Modeling”, a miniproject that I gave was to use SA to re-assemble the fragments of a shattered photographic image into the original photo (not knowing the original!). Figure 65 shows a solution from Ivaylo Enchev and Corneliu Prodescu. The two key design ingredients to set up SA were to find a suitable energy function and a good proposal distribution. All students used some measure of geometric / color agreement between the edge regions of neighboring “tiles” as a basis for the energy function (graphical mismatch = high energy). For the proposal distribution, the Enchev/Prodescu team opted for a weighted random choice of a pair of tiles which then would be swapping places. The weighting encouraged to choose swapping candidates that had a significant graphical mismatch with their neighbors.

[Unscrambling a photo that was shattered into 1938 fragments. Left: the mess that I gave out to students. Right: a low-energy re-ordering found by running SA for 24 hrs on a PC. Right picture taken from the project report of Ivaylo Enchev and Corneliu Prodescu (unpublished).]

The Boltzmann machine

The Boltzmann Machine (BM), introduced in 1983 by Hinton and Sejnowski (, didactic introduction in ), is not actually a machine but a neural network model for a representation of complex distributions — or, stated in the terms of cognitive science, which is the appropriate background: a model of a contents-addressable, associative, generative long-term memory.

Before going into the technical aspects, I want to explain the background intuitions and the intended use of BMs. A good way to approach BMs is to see them as an abstract model of human memory, including mechanisms for storing, retrieving, associating between stored items, and completing corrupted inputs. Let us consider the question what is the nature of a human’s long-term memory of the written digit pattern for the number “four” (in the Times bold font: 4). Numerous and diverse answers to this question have been given in the fields of cognitive science and neuroscience, for instance:

-   The digit-pattern-4-memory is a stored prototype, that is, we have a conceptual/neural representation of some ideal, prototypical pattern, – let’s say, something very clean and clear that looks like this:

    [image]

    In order to recognize new incoming instances of the digit “four”, like this one:

    [image]

    the stored prototype is matched against the new input, which is classified as “four” if the match is close enough. The prototype view is one of the classical models of memory in cognitive science.

-   The digit-pattern-4-memory is a set of processing rules, which specify how low-level features which are extracted from a visual input can (and must) be combined in order to be recognized as a “four” pattern. This is another classical model, especially in AI and pattern recognition.

-   The digit-pattern-4-memory is a huge set of contextual expectations (or anticipations or affordances) which specify in which contexts one should expect the pattern “four” to appear, and when it appears, which further actions or perceptions are likely to occur. This would be the approach of the school of thought of anticipatory representations.

This is only a selection among numerous other models of the nature of long-term memory. The most confusing part of this picture is that for each kind of model there is substantial empirical evidence from psychological or neurophysiological studies.

The BM should be seen on this background of a multitude of models of long-term memory (LTM), because it adds another such model. The fundamental assumption of the BM is that memory is a generative model of a probability distribution. Coming back to the pattern “four” example, our memory of this pattern should be seen as a distribution over possible variations of that pattern. A sample from this distribution might look like in Figure 66.

[A sample from a distribution of the pattern “four” (taken from the widely used MNIST digit benchmark dataset).]

The BM model of memory is generative. In technical terms this means that the BM comes complete with a sampling algorithm, which allows it to produce sample items from the memorized distribution. In more intuitive terms one could say that the BM can be run in a mode of “hallucination” or “dreaming” — the technical term that is mostly used is to say that the BM can confabulate pattern samples.

Architecture

A BM is a recurrent NN whose neurons are all binary, that is, every neuron can have an activation of 0 or an activation of 1. Neurons can be either visible or they can be hidden. As we will shortly see, the visible neurons can be very flexibly used in various ways (in the same BM) for input and output, whereas the hidden units add internal computing power to the achievable input/output mappings.

We will use the following notation. For a BM with L visible units, v = (v₁, …, v_(L))′ ∈ ^(L + M) for the entire network state. Always the first L members of an entire network state will be reserved for the visible units.

In a BM, there is an undirected synaptic link between any two visible and/or hidden units s_(i), s_(j): a BM is totally connected. Each link has a real-valued weight w_(ij) = w_(ji) ∈ ℝ. Special case: a zero weight w_(ij) = 0 amounts to “no link between units i and j”. Self-connections w_(ii) ≠ 0 are not allowed.

You will recognize the similarities with the Hopfield network: binary states, symmetric weights, no self-connections.

Thus, in summary, a BM is fully characterized (i) by its symmetric weight matrix W of size (L + M) × (L + M) with a zero diagonal and (ii) by the specification which of the units are visible, that is by the number L.

A BM can be used for several different purposes. Among others, it can be used for the same tasks as MLPs, namely the supervised training of pattern classification tasks. I now work this use case out a little more.

In the supervised learning setting, the training data are (u_(i), y_(i))_(i = 1, …, N) where the u_(i) ∈ ^(L_(y)) are binary classification vectors in one-hot encoding. The training objective is the same as we know it from MLPs: Upon input of a new testing pattern  from pattern class j, the j-th output unit should become activated to a value of 1 and the other output units should stay at zero activation. The inputs u_(i) and outputs y_(i) are assigned to the visible units of the BM, which means that L = L_(u) + L_(y).

And here’s the first amazing new thing about BMs that makes them so different from MLPs. A BM that was trained on a pattern classification task can also be run backwards in a generative confabulation mode. For instance, after it has been trained to classify the ten classes of handwritten digits 1, 2, …, 9, 0, one can “clamp” the ten classification neurons in one class, for instance in the class of the digit 4, by fixing the output neurons to the state (0, 0, 0, 1, 0, 0, 0, 0, 0, 0)′. In other words, the ten classification units are now used as input units. Then, examples of the digit 4 patterns will be generated on the input layer. This generation of examples will be driven by Metropolis sampling, that is, you will see a random sequence of various patterns “4” appearing on the “input” layer. This sequence of patterns “4” is a (Metropolis generated) sample from the learnt distribution of this pattern class. The examples shown in Figure 66 might well have been collected from the input layer while the classification layer was clamped to (0, 0, 0, 1, 0, 0, 0, 0, 0, 0)′.

The “input” layer can thus be interpreted/used sometimes as an input “retina” when the BM is run in classification mode, or at other times as an output projection screen when it is run in confabulation mode. Likewise, the classification layer with its ten class-coding neurons can be seen as output layer in classification mode, and as input layer in the confabulation mode. This is why it is common thinking and terminology in the BM world to drop the distinction between input and output layers. Instead, the neurons in these layers are just called visible neurons. Figure 67 gives a schematic view of the structure of a BM that is structured as indicated in this digit classification / generation set-up.

[Schematic of a Boltzmann machine. Only some of the all-to-all connections are drawn.]

The stochastic dynamics of a BM

A BM “runs” by sampling states with the Metropolis sampler. That is, the temporal evolution of the BM state proceeds by generating a random sequence s¹, s², … of binary network states which is computed by a Metropolis sampler.

Metropolis sampling needs an energy function. In a BM, the energy of a state s = (s₁, …, s_(L + M))′ is defined by
E(s) =  − ∑_(i < j)w_(ij) s_(i) s_(j).

In plain English, this energy function gives low values if pairs of neurons s_(i), s_(j) which both have an activation of 1 are connected by large positive weights. Conversely, the energy is high when such s_(i), s_(j) are connected by strong negative weights. Any unit s_(i) that has zero activation does not contribute to the energy of the network state.

If a unit s_(i) jumps from an activation of 0 to an activation of 1, while all other units s_(j) retain their activation, the energy [eEnergyBM] changes by adding the amount
 − ΔE_(i) =  − ∑_(j)w_(ij) s_(j).

This energy function gives rise to the Boltzmann distribution over the set S = ^(L + M) of all possible states, which has the pmf

  P(

In most usages of a BM, the temperature is not changed and can be fixed at an arbitrary value, typically T = 1, which simplifies [eDistributionBM] to P(s) = 1/Z exp ( − E(s)).

Thus each particular fixed setting of the weights defines a particular energy landscape and hence, a particular probability distribution over the possible binary network states (the microstates in statistical physics terminology). We write E_(W), P_(W) for the energy / probability distribution induced by a weight matrix W.

Sampling from the distribution P_(W) is achieved with a Gibbs version of the Metropolis sampler using the Boltzmann acceptance function. The sampler cycles through the components s_(i) of the states s, updating only the selected component to get a new sample point. This ansatz results in the following update rule:

When unit s_(i) is chosen for update from iteration n to n + 1, set it to a value of 1 in generation n + 1, regardless of its value at generation n, with the probability

  P(s_i^ = 1 
    / T),
where  − ΔE_(i) is the energy increment from [eBMaddEngy]. Proving that this rule is an instantiation of the Metropolis acceptance function is a recommended elementary exercise (hint: start from considering the conditional probabilities P(s_(i) = 1 | state of all other units), P(s_(i) = 0 | state of all other units); note that the ratio of these two probabilities is the same as the ratio P(s′)/P(s) where s′ is the same as s except for unit i where s′ has a value of 1 and s has a value of 0; exploit that P(s_(i) = 1 | state of all other units) + P(s_(i) = 0 | state of all other units) = 1.)

The learning task

A BM is trained to learn a probability distribution P₀(v) over the visible units. The training data consist in a sample S = (v_(i))_(i = 1, …, N) of patterns sampled from that target distribution. In formal terms, the training objective is to find weights W such that, if the trained network is run with the Metropolis sampler, the distribution of the patterns that can be read off the visible units is a good approximation of the target distribution:


An interesting special case occurs when the L-dimensional training data vectors v are split into an L_(u)-dimensional “input” part and an L_(y)-dimensional “output” part, as in pattern classification tasks. The training data vectors v = (u′, y′)′ are then composed of two parts, an “input” part u and an “output” part y. The BM learns a distribution P_(W)(v) over all visibles which approximates the joint distribution of the input and output vectors. After training, such a BM can be run in two ways:

1.  The input part of the visibles can be fixed (“frozen”, “clamped”) to a test input $
          test

2.  Conversely (staying with the digits example), the ten classfication units can be clamped to a classification vector, say to (0, 1, 0, 0, 0, 0, 0, 0, 0, 0)′ (for a change, not for class “4”). The sampling process should then lead to a sequence of random variations of handwritten-like “2” patterns on the “retina” part of the visible units.

A beautiful movie demonstration of exactly this kind of digit classifier / confabulator can be found at Geoffrey Hinton’s webpage at http://www.cs.toronto.edu/~hinton/digits.html. The architecture used there has two more hidden “layers” than sketched out in Figure 67 but is otherwise similar. The gray level rendering of the retinal visible units in these online movies is obtained by not plotting states (which would be binary black-and-white), but instead a suitably normalized version of the unit’s energy contribution ΔE_(i). Note: This interactive BM demo is an Adobe Flash implementation, and the Flash format has been discontinued by Adobe and is no longer supported by modern browsers. You need to have a Flash emulator installed on your computer to see this sort of contents. I use the “ruffle” browser plugin from https://ruffle.rs/ with my Firefox browser ... but it’s not certified by Mozilla — install such kind of software at your own risk... (there are several Flash emulator plugins for all major browsers, it seems to me that they all use the ruffle engine as their core module).

For a well-defined learning algorithm one needs to have a well-defined loss function. In the BM scenario, the loss function should measure how close a BM distribution P_(W)(v) is to the target distribution 
(

  KL(P, 
  

The KL distance is not actually a true distance because it is not symmetric. It is always nonnegative and it is zero if and only if P = P̂.

Thus, the learning task becomes to solve the problem of minimizing the empirical risk associated with this loss function: find

  W
    
  KL(P

The learning algorithm

Given a training sample S = (v_(i))_(i = 1, …, N) and a BM architecture with the right number of visible units, a weight matrix  is differentiable with respect to the weights w_(ij), gradient descent optimization can be used.

The beauty of BMs lies in the circumstance that the formula that gives us the gradients is very simple. A little non-deep maths reveals that

  
    P_(
  
where p_(ij) is the average (over training samples) probability that the units i and j are both active (that is, s_(i) = s_(j) = 1) when the visible units are clamped to the training data point v, and q_(ij) is the probability that these two units are simultaneously active in a “free-running” sampling mode with no external clamping. This yields the following update rule for weights:
w_(ij)(n + 1) = w_(ij)(n) + λ (p_(ij) − q_(ij)),
where w_(ij)(n) is the value of the weight wij at the n-th step of gradient descent and λ is a learning rate.

A single weight update w_(ij)(n) → w_(ij)(n + 1) thus involves the following operations:

1.  Estimation of p_(ij): for each training sample point v_(k), clamp the visible units to v_(k). While the visible units remain clamped to v_(k), run the BM Metropolis sampler [eBMupdateUnit] until a reasonably representative sample of network states under this clamping condition has been collected. Use this collection to estimate the probability p_(ij)^(k) of co-activity of units i and j in this clamping condition. Do this for all k = 1, …, N. Finally, set p_(ij) to the average of all of these p_(ij)^(k). All of this is sometimes called the “wake” phase of the BM learning algorithm (when it “sees” the visible input).

2.  Estimation of q_(ij): Similar, only without clamping the visible units. This is sometimes called the “sleep” phase (the BM has its eyes closed).

3.  Weight update: apply [eBMupdateWeights].

It is clear where the catch is: for a single weight update step, one has to run as many complete sampling runs as there are training patterns! At face value, this is prohibitive. In their original paper, introduce a number of simplifications which allowed them to learn some demo examples, even with the computers of the mid 80’ies. The main simplifications are the following:

1.  Instead of [eBMupdateWeights], use 
      q_) is the signum function. This reduces computational load because determining the sign of (p_(ij) − q_(ij)) needs a less accurate sampling than estimating the size of this difference.

2.  For each sampling run in steps 1 or 2 above, use a two-phase procedure. In the first phase, start from a higher temperature than the agreed T and carry out a simulated-annealing like cooling from the higher temperature to T. In the second phase, sample at the target temperature T. The initial annealing phase is intended to prevent that the (randomly generated) starting state remains stuck in some narrow, untypical local energy minimum.

Despite these simplifications and heuristics, the intrinsic computational challenge of a very large number of sampling runs needed to determine weight updates is not fundamentally dissolved. This is probably a good enough reason to explain that BMs were never used in practical applications.

The restricted Boltzmann machine

The maths underlying the classical BM is both so simple and so potentially powerful that Geoffrey Hinton and other academic researchers continued to research these architectures in the decades since 1985. I probably wouldn’t have elected BMs for this lecture nonetheless, if not many researchers in machine learning (including myself) have freshly become excited about BMs. This stir was triggered by the Science paper , where several developments which started from BMs were combined into a strikingly powerful architecture for learning complex distributions, now named restricted Boltzmann machines (RBMs) or deep belief networks (DBNs):

-   DBNs are layered neural networks, where each layer corresponds to one BM. The hidden units of one such BM make the visible units of the next-higher BM.

-   The connectivity of each participating layer BMs is very much reduced: there are no within-layer connections, only connections between adjacent layers exist. This led to the name “restricted” Boltzmann machines.

-   The learning is done in a divide-and-conquer fashion layer by layer.

-   Each sampling subroutine for the estimation of the probabilities p_(ij), q_(ij) is condensed to only two state updates of one layer BM, using a shortcut approximate algorithm called contrastive divergence.

These innovations together have brought BMs back on stage with a flourish, which in turn has triggered the deep learning revolution. I can’t possibly describe DBNs better (nor more concisely) than Hinton  Salakhutinov did in their celebrated Science paper, so if you are interested in digging deeper into the deep roots of deep learning, that paper is a must-read for you (only 3 pages).

Hinton and Salakhutinov did not foresee or plan what is now called deep learning. Their paper concentrates on a use of RBMs for data compression. Only in a 14-line paragraph they mention in passing that they could use RBMs for pre-training (initializing) the weights of a multilayer perceptron, giving a good starting point for the subsequent application of the standard backpropagation algorithm. The rest is history, and you pluck the fruit whenever you use Google translate, for instance.

DBNs remained fashionable for some years after 2006, and there were many sophisticated attempts to lift them to a generally useful, stand-alone learning approach for real-world modeling tasks. In fact, — at least, in my personal recollection — the entire machine learning community was thrilled about DBNs at that time (check out the 2007 youtube video https://www.youtube.com/watch?v=AyzOUbkUf3M to see Hinton himself presenting the BM in a Google talk). In the end, however, deep learning schemes which used cheaper initialization schemes than DBNs won over.

Reservoir computing

Reservoir computing (RC) is an umbrella term for a number of closely related approaches for designing and training recurrent neural networks. RC methods are in many ways complementary to the RNN methods which we met in Section 4. While the computational and learning algorithms differ, the tasks solved by RC networks are the same as those that are solved by the RNNs and LSTM networks from Section 4: supervised timeseries-in, timeseries-out learning tasks of all sorts and for all kinds of applications. I repeat the basic set-up from Section 4 for convenience. The generic formulas for RNNs and supervised learning tasks are:

Training data:

    One or several, long or short pairs of discrete-time input and output timeseries 
            

Network equations:

    
    
        
                              
                              
        
                            
      
    Re-read Section 4.1.1 if you are unsure about what the symbols in these equations mean.

Learning task:

    Find weights 
      
        
      (
    
        
        L(
        

The big difference between training RNNs in the “normal” way by gradient descent via BPTT, and how training is done in RC, is that BPTT optimizes all parameters in $


[Highlighting the difference between the “normal” BPTT training and RC training of an RNN. Arrows shown in red are trainable. ]

The recurrent neural network that lies between the input and the output neurons is called the reservoir in this field, and the weights  from the reservoir to the output neurons are called the readout weights or just the readouts.

Only training the readouts seems to be a very strong simplification and one might expect that it will lead to a drastic reduction in achievable performance. There are a number of reasons why reservoir computing methods are nonetheless being explored and utilized besides the BPTT training schemes which dominate in deep learning:

1.  The computational cost of RC training is only a tiny fraction of what is claimed by BPTT – seconds on a PC compared to days on a supercomputing cluster.

2.  The training algorithm is numerically robust and there are no local minima problems.

3.  For many tasks RC networks yield solutions that are on a par with what one gets from “normal” BPTT-trained RNNs. In some applications RC even systematically outperforms BPTT-based RNN training schemes.

4.  RC is biologically plausible. Neuroscientists have identified several circuits in real brains that might implement RC.

5.  RC can work with RNNs whose equations are not differentiable, which precludes BPTT training. In fact, one does not need a “neural network” between input and output at all — any kind of nonlinear dynamical system can be plugged in for the “reservoir”. Specifically, one can use exotic, non-digital and even non-electronic microchips as stand-ins for the RNN. This has made RC a leading computational paradigm in recent research in optical computing and other physical substrates which use non-electric nanoscale phenomena.

A note on history and significance: The core idea of RC, namely to use a fixed, non-trainable RNN and only train readouts, has been independently discovered a few times. The earliest publication known to me is , a contribution to a local low-key AI conference, which was immediately and totally forgotten, followed after 1994 by a series of papers from Peter F. Dominey, a cognitive neuroscientist who identified RC-like circuits in the human brain (for instance, ). He used a biologically inspired learning algorithm with low statistical efficiency, with the effect that this work was not taken up in the machine learning quarters. In the year 2001 the RC principle was again re-discovered (by myself) within a machine learning context, this time with an efficient learning algorithm, and branded under the name of echo state networks (ESNs) . At the same time it was also independently re-disovered by Wolfgang Maass in a theoretical neuroscience context, based on biologically detailed, spiking neuron models, and published under the name of liquid state machines (LSMs) . In those years, BPTT training of RNNs was not very practical because numerical instabilities and vanishing gradient problems were not yet under control. ESNs became popular in those years in machine learning, especially after the publication of where ESNs achieved accuracy levels on benchmark tasks of that time which were up to five orders of magnitude better than the state of the art.

The term “reservoir computing” established itself as an umbrella term for ESNs, LSMs and some variants. In machine learning contexts, the term “echo state networks” is still common. When it is used, it is implied that simple neural networks with equations like [eDefRNNupdateSectionRC] are used. When the word “liquid state machine” is used, this usually means that the author treats a neuroscience modeling topic and uses more involved, biologically motivated network models with spiking neurons. I will concentrate on the machine learning aspects of RC and therefore use the word “echo state network” in the next subsections.

In the decade until about 2015, the successful harnessing of BPTT in the deep learning field diminshed the interest in RC in the machine learning community, while it continued to be explored in neuroscience.

Since about 2015, both in academia and industry one could witness a quickly growing interest in developing “brain-inspired” computing microchips. Digital computing technologies will soon hit ultimate limits in miniaturization, and furthermore the energy demands of digital IT technologies are becoming prohibitive — it is estimated that today more than 10% of the world’s energy budget is eaten up by digitial computing hardware. The biological brain is estimated to have an energy efficiency that is four orders of magnitude better than what can be realized with classical digital microchip designs. This is a strong economical and ecological motif to explore non-digital, “brain-like” neuromorphic hardware solutions. The rise of neuromorphic computing research has pulled RC back into the focus of attention.

A basic demo

The easiest way to understand RC is to go through a simple demo example. Let us consider a two-class timeseries classification task. The training data 
      max

[A simple binary temporal pattern classification task. A segment of the training data is shown. Top: input, bottom: desired output. The duration shown comprises 80 discrete timesteps. Note that we are dealing with discrete-time signals; subsequent points are connected by a line for better visualization.]

In this demo I use a reservoir made of L = 100 neurons. Since the input and output signals are one-dimensional (K = M = 1), there is one input neuron and one output neuron. The L × L sized reservoir-internal weight matrix W, the L × 1 input weight matrix  and the L-dimensional bias vector b from Equation [eDefRNNupdateSectionRC] are filled with random values sampled from uniform distributions around 0, that is, positive and negative values occur roughly equally often.

Formal statement of the learning objective

The learning goal is to compute an 1 × L sized output matrix 


Step 1: state harvesting

Solving [eESNdemoObjective] is done in two steps. In the first step, the network (which has been randomly created) is driven by the teacher input, that is, for a duration of 
    max

[State harvesting — the first step in training an RC network. The network is driven by the training input u(n) (green) and the L reservoir neuron activation traces are recorded (blue traces on the right, only some are shown). Note that the readout weights are not known in this first step (indicated by the light gray coloring of the readout) and no network output is generated.]

Step 2: compute readouts

In step 2 the optimal readout weights 

In words: the L vectors x_(i) have to be linearly combined such that the combination sum best approximates the teacher vector  in the least mean square error sense. This is just a case of computing a linear regression. Every mathematical or statistical programming toolbox offers a choice of ready-made algorithms for computing a linear regression.

If you are not familiar with the concept of linear regression, I can recommend Section 3.1 in my lecture notes for the Machine Learning course. (In fact mandatory reading if you need to refresh your understanding of linear regression. Nobody should leave a neural networks course without knowing what linear regression is!)

[Step 2 in the RC training scheme: compute optimal readout weights  . ]

Figure 71 gives a graphical impression of this situation. After this step 2, the training is finished. The found weights 
    

Testing

For testing, the trained network is driven with fresh input data 
    

[Testing the trained network (which now has the readout weights installed) with fresh input data. The reference output  is shown in black and the network output in red.]

Computational cost

I conclude this demo with a summary of the computational cost. Assuming that the number K of input channels is less than the reservoir size L (which is typically the case), the random initialization of all the fixed weights comes at a cost of O(L²). The state harvesting needs O(L^2 + 


RC in practice

The demo example illustrated all that needs to be done in training a RNN in reservoir computing style. Seems simple... But if you want to squeeze good performance out of ESNs, a number of design decisions need to be made appropriately, and this is not so simple. After all, we are dealing with high-dimensional nonlinear dynamical systems, and these are never easy to handle. In my experience it takes some months for a RC novice, working full-time, to gain the insight and routine necessary to handle RC techniques adequately. The “tricks of the trade” are explained in the detailed practical RC tutorial . Here I mention some things that need to be considered. This summary account cannot replace reading by if you seriously want to get started with RC in practice.

Discard washouts.

    For state harvesting, the reservoir must be started in an initial state x(0). This state is arbitrary and unrelated to the learning task, and traces of it will remain in the next few networks states. Figure 73 shows this “initial state memory” effect. Our demo ESN was driven twice with the same input, but started from two different, randomly chosen initial network states. The top panel in Figure 73 shows the initial 10 step traces of four reservoir neurons, with the set of traces from the first initial state in solid lines and the traces arising fro the other initial state in broken lines. One can see that for the first few steps these state sequences differ from each other, but converge toward each other. The rate of convergence is typically exponential on average, as can be seen in the bottom plot. This plot was computed as follows. Let x(n), x^(*)(n) denote the two reservoir state sequences. The bottom plot shows the development of the Euclidean distance between x(n) and x^(*)(n) in log10 scaling, that is, it plots log₁₀∥x(n) − x^(*)(n)∥. The initial state differences are “forgotten”, in RC terminology: they are washed out. How fast this washing-out happens depends on many factors; it can be much slower than in this demo where the differences become invisible in the top plot after three or four steps already.

    [The initial state memory effect. For explanation see text.]

    In order not to pollute the linear regression in step 2 of the RC training procedure with the un-systematic effects of the arbitrary initial state setting, one simply starts the harvesting after an initial washout period. An appropriate length of this washout time is most easily determined by computing a diagnostic plot like our Figure 73.

Scaling of initial weights.

    This is the most important point. The geometrical and dynamical properties of the harvested state sequences depend crucially on the average absolute sizes of the weights in  and the bias b. Figure 74 illustrates this.

    [The effects of weight scaling. Top panel: the input signal. The other plots A – E show traces of four neurons in runs with that input, where the weights were scaled with different scaling combinations. From top to bottom the three matrices 
        

    Concretely, in this scaling exercise I always started from the same reference matrices 
    which thus had a spectral radius of 1.

    Given these reference matrices 
        

    Here are some observations and comments. I refer to the cases A – E in that figure:

    Case A:

        When all weights are scaled to an intermediate range, the network states behave in a way that normally works well in RC: their amplitudes span much of the possible value range [ − 1, 1]; they are clearly influenced by the driving input but the reservoir-internal interactions make them markedly different from each other. Note that such a behavior is not universally the best for all sorts of tasks.

    Case B:

        When the input weights are large and all other weights are small, the activation of a reservoir neurons will be dominated by the input term in [eDefRNNupdateSectionRC] and (almost) all neurons will exhibit activation traces that look like scaled versions of the input. This is not a desirable behavior in most applications, except sometimes when the desired output y(n) at time n depends only on the input u(n) at the same time (no memory effects needed). But in such cases, a feedforward network would be a better choice than an RNN.

    Case C:

        If both input weights and reservoir-internal weights are large, the activations of neurons will typically be pushed toward the  − 1,  + 1 limits of the tanh  sigmoid and an almost binary “switching” dynamics inside the reservoir results. This may be appropriate in extremely nonlinear, quasi Boolean input-output learning tasks.

    Case D:

        If the bias yields the dominating weight components, the reservoir dynamics degrades toward constant values in each neuron. I cannot imagine any interesting task where this would be beneficial.

    Case E:

        If the reservoir-internal weights are large and the input and bias not, then there is danger that the network dynamics falls prey to a wild recurrent self-excitation which is no longer modulated in a useful way by the input: chaotic dynamics (in the strict mathematical sense) emerge. This is certainly useless because the “echo state property”, to which I will devote a separate subsection below, is violated.

    In summary, you see that the absolute and relative scalings of 
        

    Unfortunately there is no general rule of how to set these scalings optimally. It depends on the task and also on the dimensions K and L of the input and reservoir. Like so often in practical work with neural networks, the beginner has to spend a lot of time experimenting, and experienced users will benefit from their well-honed intuitions. In any case, you should always create indicative plots of state dynamics as in Figure 74 to get a “feeling” for what is happening inside your reservoir. It’s like a doctor doing an X-ray.

Output feedback for signal generation tasks.

    Some tasks demand that the trained network should generate an output signal. The basic example is to train a network that has no input and a single output unit which should yield a generated signal, for instance a sinewave oscillation. Such signal generation tasks require that the generated output signal is fed back into the reservoir. Using a linear output unit, the network update equations are

    The weights  are trained. Figure 75 illustrates the set-up.

    [Training an ESN as a sinewave oscillator. At training time the output weights, shown in dotted red lines, are not there yet. For explanation see text. ]

    The teacher signal y(n) is a sample of the desired generated signal, here a sinewave (black signal in Figure 75). For state harvesting, the teacher signal is written into the output node in a version that is delayed by one timestep: in update cycle n the value written into the output unit is y(n − 1). This time lag accounts for the relative time difference of the y signal that you witness in [eSineESNx] versus [eSineESNy]. The feedback weights  assume a role as input weights in the state harvesting phase.

    After the output weights are computed as usual with linear regression, the network is ready for use. If all worked out well, started from a random initial state the network will settle into an oscillation mode which after an initial transient, where the random initial reservoir state is washed out, settles into the desired oscillation. Figure 76 shows this.

    [After training, the network functions as an oscillator. The output unit will (after an initial washout resulting from the arbitrary initial reservoir state) generate the kind of signal that was used as teacher. The network output (red) is overlaid with a suitably phase-shifted version of the training sine (thin black line). The reservoir neurons display all kinds of oscillations whose shapes result from the nonlinear interactions of the reservoir neurons (four traces shown).]

    One can also add separate input signals which modulate the generated output signals. Figure 42 (Section 4.1) shows an example where the task was to generate a sinewave output whose frequency is set by an input signal. When the input signal has a high value, the generated sine should have a high frequency. The demo illustrated in Figure 42 was based on an ESN.

    ESNs are particularly well suited for signal generation tasks. Specifically, they are unrivalled when it comes to generate chaotic signals. The paper which popularized ESNs included demos of generating chaotic signals whose precision was essentially machine precision. In the last few years, the surprising performance of RC in modeling chaotic systems has attracted the attention of mathematicians and physicists, who exploit this phenomenon for the study of chaotic systems (for example spatial wavefront dynamics, ).

Leaky integrator neurons.

    We have seen in Figure 74 that reservoir dynamics can have quite different properties. For good RC results, the dynamical and geometrical properties of the reservoir should match the properties of the task. Such properties which should match are, for example, the degree of nonlinearity, the length of memory timespans, or the sheer complexity of the task.

    One of the most important properties is something that one could call the “speed” of the system from which the training data come. There are fast systems and there are slow systems. A gigahertz computer clock is faster than the tides of the ocean. While it is intuitively clear that there are fast and slow systems, there exists no universal best mathematical way to define or measure the “speed” of a dynamical system. I will not dig deeper here although I would love to do so — coming to terms with timescales is the core of my current research in the European Project MeM-Scales (“Memory Technologies with multi-scale time constants for neuromorphic architectures”, memscales.eu).

    In practical applications of RC a crucial factor for success is to adjust the reservoir’s “speed” to the task’s “speed”. To make this possible, one needs a way to design slow or fast reservoirs.

    The standard approach is to use a neuron model where each neuron’s dynamics is governed by the leaky integration update equations. In fact, RC experts will almost always use leaky integrator neurons. Such neurons can be slowed down or sped up as one wishes.

    They are best explained by starting from a continuous-time neuron model. Consider a reservoir that runs in continuous time t ≥ 0. The L-dimensional activation vector at time t is x(t) (notice that often one uses symbol n ∈ ℤ for discrete time points and t ∈ ℝ for continuous time). The activation x_(i)(t) of neuron i at time t is a continous-time signal. For a leaky integrator neuron, it is governed by the ODE
    
      
      
    where the w_(ij) are the synaptic connection weights of the incoming connections of neuron i, u_(k)(t) is the k-th component of the input vector u(t) at time t, the  are the input connection weights leading into neuron i, b_(i) is this neuron’s bias, and — the most interesting bit in this equation — c is the time constant of this ODE.

    If this ODE is integrated with different settings of the time constant, the obtained trajectory that one could plot in a phase portrait will look the same. Remember that the trajectories follow tangentially the vectors of the vector field given by the right hand side of the ODE. The effect of changing the time constant c only scales the length of these vectors but not their direction. If the time constant grows, the length of the vectors in the vector field shrinks in inverse proportion. This means that the “forward speed” of the point x_(i)(t) along the trajectory slows down when c grows. By setting c one can create slow or fast continuous-time reservoir dynamics at one’s discretion.

    This dynamical law is called the leaky integration model of a neuron because the term $
    

    This is, by the way, an effect that is also active in biological neurons: due to elementary electrophysics, their electric potential (measured in millivolt) likewise would dissipate at a rate proportional to its current level, because the cell membrane is not a perfect insulator. Therefore, leaky integrator models are a much better fit to biological neurons than the simple neuron model that we have been using in this section so far. In computational neuroscience one almost always uses leaky integrator models of various sorts.

    However, for practical machine learning applications run on digital computers one needs a discrete-time neuron model. Instead of a continuous trajectory (x_(i)(t))_(0 ≤ t) one needs a timeseries which advances in discrete steps with a chosen stepsize Δ, that is one wants to have a discrete-time version (x̃_(i)(n Δ))_(n = 0, 1, 2, …) of the continuous trajectory. At times t = n Δ the two trajectories should be (approximately) equal, x_(i)(n Δ) ≈ x̃_(i)(n Δ).

    Finding discretization methods which allow such a discretization of ODEs with a good balance between approximation accuracy and computational cost is a main subject of numerical mathematics. Whenever you simulate an ODE on a digital machine — which is the daily bread and butter in all the natural sciences — a numerical ODE solver is invoked. Every mathematical toolbox offers a choice of such solvers.

    The simplest of all ODE solvers is called the Euler method (see https://en.wikipedia.org/wiki/Euler_method for an easy introduction). After some point x̃_(i)(n Δ) has been computed, the next point x̃_(i)((n + 1) Δ) is computed by following the direction of the vector given by the right-hand side of the ODE for a timespan of Δ:
    
      
       
        

    Using Δ = 1, renaming 1/c to a and joining the two x̃_(i) terms leads to the simpler looking version
    
       (1-a)
        
    which is the form that you will most commonly find in the RC literature (without the tilde). Now you can control the “speed” of neurons by setting a ∈ [0, 1]: the larger this is set, the faster the neuron. In the extreme case a = 1 one recovers our accustomed simple update equation [eDefRNNupdateSectionRC]. The slowest “dynamics” is obtained with a = 0: then nothing happens — the network state remains frozen in its initial state. The number a is often called the leaking rate of the neuron.

    One can set leaking rates individually for different reservoir neurons. This is advisable if one wants to have a reservoir that can respond to complex input signals that have both fast and slow components. Often it is good enough to tune a single leaking rate uniformly for all reservoir neurons. This gives the following matrix form for the state update equation:
    
    
    

    Many real-world physical systems and all truly cognitive systems operate on several timescales simultaneously. For instance, in atmospheric dynamics small whirls of air (like gust eddies between houses) have a typical time constant of a few seconds, while the large whirl of a low over central Europe evolves over several days. Or, for a cognitive dynamics example, while you are reading this lecture notes section, your brain at any moment has to integrate information bits that come just from the preceding syllable (read a few milliseconds before) with information from the beginning of this section (maybe an hour ago). As of today, neither deep learning LSTM networks trained with BPTT, nor ESNs are capable of integrating information across many timescales. Extending the multi-timescale capabilities of RNNs is a major topic of current research. In ESNs, one approach is to design reservoirs with leaky integrator neurons, where different submodules of the reservoir have different values for the inverse time constant a. Typically, fast modules or processing layers are closer to the input than slow layers. An example is where “deep” ESNs are constructed in this way for speech recognition and music composition.

Regularization and reservoir size.

    Like any machine learning method, RC is susceptible to overfitting. In order to cope with this problem, the flexibility of a RC training scheme must be adapted to the available training data volume and task complexity by experimentation in a cross-validation scheme (recall this from Section 1.4). This needs a way to tune the degree of modeling flexibility. The recommended approach with ESNs is

    -   use a large reservoir, choosing a size that would allow overfitting,

    -   then use a regularized version of linear regression for the readout weight calculation, called ridge regression.

    Ridge regression, also known as Tykhonov regularization (https://en.wikipedia.org/wiki/Tikhonov_regularization) should always be used in machine learning when a linear regression has to be carried out — which means, all over the place, not only in a RC context. It is a very valuable thing to know, and I will explain it in a little detail and present an example.

    Let us first rehearse the maths of linear regression, framed in an ESN training scenario. Assume we want to compute the 1 × L-dimensional readout weight vector w for a single output neuron from harvested L-dimensional reservoir states  such that the mean squared training error is minimized, that is, we want to solve the problem
    
      
      
      (
    where y(n) is the teacher for the output unit (note that w is a row vector). The analytical solution for this problem is the well-known solution formula for linear regression problems,
    
      
      (
    where X is the  containing the teacher output values.

    In ridge regression, the objective function in [eESNMSEmin] is augmented by a regularization term which penalizes the sum of squared weights,
    
      
      
      (
    where α ≥ 0 weighs the strength of the added regularization. The analytical solution of this problem is
    
      
      (
    where I is the L-dimensional identity matrix. Ridge regression thus just adds α on the diagonal (“ridge”) of X′ X, which gave this method its name.

    The larger α is chosen, the stronger the regularization and the smaller the resulting entries in $
        opt

    [The training signal (top) and the network output (red) in three regularization conditions. The thin black sines drawn into the panels are the clean sinewaves; they are shown for visual intuition only and the training and testing procedures did not use this information.]

    I demonstrate the working with ridge regression on a simple example. The task is to predict the next value of a sinewave signal. The training data  consists of a sinewave signal u(n) to which noise was added, and the teacher is the same signal advanced by one step into the future, that is y(n) = u(n − 1). The top panel in Figure 77 shows the noisy sinewave used for training.

    The reservoir was made of L = 100 leaky integrator neurons with a leaking rate a = 0.5. I omit a discussion of how the various weight scaling factors were set — this sinewave prediction is not a challenging task and the ESN training works well in a wide range of these scalings. I computed different versions of output weights with the regularization parameter α in [eESNMSEminSolveridge] chosen differently, namely as 10000, 1000, 100, 20, 10, 1, and 0. I computed training and testing MES’s and plotted them against α (Figure 78). This is the same kind of graphic as Figure 6 in Section 1.3.5. You should always draw this plot when carrying out a supervised training task with whatever kind of method! The curves behave as in a textbook they should: the less regularization is applied, the lower the training error; but the test error is high both with very strong and very low amounts of regularization and has a minimum for some intermediate degree of regularization (here at α = 20). The figure also shows how the regularizing term α w′w in [eESNMSEminridge] pulls down the resulting weight sizes when α gets larger.

    [THE paradigmatic textbook behavior of training and testing errors as they vary with the degree of regularization. For explanation see text. The black curve shows how the mean absolute output weights grow as the regularization strength is decreased. For α = 0 the average absolute weight size is about 5.2 (not drawn). ]

    Figure 77 illustrates the performance of the ESN when it is trained in underfitting (α = 10000), optimal (α = 20) and overfitting (α = 0) conditions. Again, this is true proper textbook behavior as it should be.

Online reservoir training

While the analytical solution [eESNMSEminSolveridge] will be used in most practical RC applications, it is sometimes necessary to use an iterative stochastic gradient descent method to solve the linear regression task [eESNMSEminridge]. This is mandatory in online adaptive learning, where the properties of the input-to-output signal transformation change with time and the learning system must continually track the target system.

For example, if an RNN is set up to control the carburettor valves of a combustion engine for optimal fuel efficiency, the operating conditions of the engine change with temperature, runtime, load, age of the engine, quality of the fuel, and other unforseeable factors. RNNs are indeed employed for this purpose (or at least they have been — I learnt about this from Ford engineers two decades ago). In such a scenario, the RNN must be continuously re-trained as new measurement data (which are used as input) come in. If you are interested, you find an entire section devoted to online adaptive signal processing in my machine learning lecture notes which are online on our Nestor course pages. Furthermore, biological neural systems need to learn in incremental adaptive fashion — biology has no way to invert a matrix, as required be the analytical solution [eESNMSEminSolveridge].

Again considering, for simplicity, the case where there is only a single output neuron, the training data used in online adaptive learning is a potentially endless stream (u(n), y(n))_(n = 1, 2, …). Its input-output transformation properties will slowly change as time runs on, which is why it makes no sense collecting data from a long stretch of this stream and use it for RNN training of any kind: the RNN would “learn” a useless compromise blend of the early and late behavior of the system which produces the training data.

Instead, with ESNs one uses the following online algorithm to continuously adapt the current model to the ongoing data stream (u(n), y(n))_(n = 1, 2, …).

-   The reservoir is continually fed with the input data stream u(n), leading to a reservoir state sequence x(n).

-   Assume that at time n, an output weight vector  which leads it in that direction which most strongly reduces the squared error (y(n + 1) − ŷ(n + 1))². Skipping the maths (derivation is straightforward, if interested you find it in Section 11.3 in my Machine Learning lecture notes, or in hundred other textbooks and online tutorials), this leads to the update equation
    
        
        
        y(n+1)-
    where λ is a small adaptation rate, say λ = 0.01.

That’s it. This very cheap and simple rule sits behind most of the signal processing procedures which make the radio receiver module in your smartphone work; neuroscientists say that it is biologically plausible; and it enables reservoir computing to function well in online adaptive task settings, where deep learning methods are severely challenged. If you have a very good memory of the earlier parts of these lecture notes you will recognize that the Perceptron learning rule is in fact just a version of this algorithm with λ = 1.

Like so many powerful ideas in engineering and science, this rule has been discovered independently several times in different disciplines, where it is known under different names. In signal processing and control it is called the LMS algorithm, in the neurosciences and sometimes in artificial neural network research it is called the Widrow-Hoff rule or the Delta-rule, and a plain mathematician would likely refer to it as stochastic gradient descent on a quadratic error surface. No introductory course on neural networks would be complete without it, and I seized the opportunity to introduce it in an ESN context.

The echo state property

Not every design of a reservoir will work. Specifically, when the reservoir weight matrix W is scaled too large, the reservoir will recurrently excite itself so strongly that its internal, self-generated dynamics will overrule any influence of the input, and the harvested states will become useless. If run twice with the same input but from different initial states, the two state sequences will not converge to each other after a washout as they should do, like visualized in Figure 73. Instead, anything can happen, including unpredictable chaotic dynamics. Figure 79 demonstrates this.

When the network washes out differences of initial states under the influence of an input signal u(n), the combination of the network and this input is said to possess the echo state property (ESP).

[The echo state property: to have or have not. Top: with a scaling of the reservoir weight matrix under a critical value, the ESP is granted and state sequences converge to each other after some washout time, when the reservoir is started from different initial states with the same input. Four reservoir neurons are plotted, with the two runs distinguished by solid / dashed lines. The input was a sinewave (not shown). Bottom: the ESP is lost when the reservoir weight matrix is scaled up beyond a critical value. Here the reservoir engages in a chaotic dynamics that has lost the connection to the input signal. ]

The scaling of W is standardly expressed by the spectral radius 𝜚 of W. The spectral radius has an important effect on the learning accuracy and one usually has to do some manual experimentation to find a good value for 𝜚. One typically starts with a reference weight matrix 
    

For a given input signal u(n) and reservoir weight matrix  it is lost. This critical value marks a bifurcation in the reservoir dynamics.

There is no known way to predict the critical spectral radius  analytically. A lot of effort has been spent and is being spent on the mathematical study of the ESP. There are two reasons why the ESP is attracting so much attention:

-   For machine learning applications of RC it is obviously important because RC training only works for input-reservoir combinations that exhibit the ESP.

-   In the general research on cognitive neurodynamics, a currently popular hypothesis states that, roughly speaking, the human brain works at its best when it increases its self-excitiation level just below the point where it bifurcates into chaos. Google “edge of chaos” neural network to get a glimpse of the lively research on this topic, both in machine learning and neuroscience quarters.

Physical reservoir computing

This subsection is not mandatory reading. It’s just exotic, visionary, slightly crazy fun stuff — a peek into the future of computing maybe — or maybe not.

Since a few years, interest in reservoir computing has been re-kindled. The reason is that RC is one of the few computational approaches which do not need, in principle, digital computers. Non-digital, “neuromorphic”, “brain-inspired”, “unconventional” computing microchips are gaining relevance due to the reasons that I briefly mentioned in the last paragraph before subsection 8.1. The Groningen Cognitive Systems and Materials Center (CogniGron) (https://www.rug.nl/research/fse/cognitive-systems-and-materials/), which was founded at RUG in the year 2018, strives to become a European pioneer in this field. The principle of reservoir computing, which can be summarized as

use an input signal to drive a nonlinear excitable medium — the “reservoir” — and generate a desired output signal by combining many of the local response signals that can be observed in the medium

can be applied to many kinds of “reservoirs” other than neural networks simulated on digital machines. The recent survey of gives an overview. If the reservoir is a real, physical piece of material which can be excited into interesting dynamical responses by some physical driver input, one speaks of physical reservoir computing. The potential benefits are inherent parallelism, a low energy budget (in physical nanoscale devices), high speed, extreme high-dimensionality of the reservoir states (even, in principle, infinite-dimensional states in continuous materials), and, hopefully, low cost. As of today, all of this is still academic research. I will not go into details but just illustrate the flavor of this kind of research with a few examples. I present them by figures with detailed captions.

[Reservoir computing in a bucket. Wolfgang Maass, the inventor of the “liquid state machine” version of RC, used to call the reservoir “the liquid” . This was taken literally by students Chrisantha Fernando and Sampsa Sojakka at the School of Cognitive and Computer Sciences, University of Sussex. They filled a small transparent acryl basin with real water, excited it with eight Lego-made mechanical pushrods whose oscillations were derived from speech signals, optically recorded the states of the water surface ripples, and used these states for RC. Their “liquid brain” could solve the infamous XOR task and classify spoken “Zero” versus “One”. Their paper at the ECAL 2003 won the highest impact paper award. Christian, now a Senior Research Scientist at Google DeepMind, explains it on youtube (https://www.youtube.com/watch?v=nmxV0FtsOnc). ]

[Reservoir computing in a plastic worm. In robotics, the control of body motion needs (among other items) a “forward model” of how the body limbs will react to motor or muscle action. While in classical industrial robotics this forward model can be calculated analytically with high precision, this is not possible with soft robot bodies or body parts — snakes, worms, trunks, tongues. One way to get such a model nonetheless is to use the very physical body itself as a reservoir. Its states are observed by sensors placed on it. The twofold charm of this approach is, first, that this enables almost delay-less online computing, and second, that this reservoir naturally has exactly the right dynamical properties to “model” itself. At the University of Zurich, a team around Rolf Pfeifer realized this idea with a plastic worm. Their paper comes with a youtube demo (https://www.youtube.com/watch?v=rUVAWQ7cvPg). The idea to use robot bodies themselves as computational modules within “algorithms” needed for motion control has become branded as morphological computation — google it if you find this interesting. ]

[Reservoir computing in mechanical silicon microchips. Julien Sylvestre and his group at the Department of Mechanical Engineering, Université de Sherbrooke, Canada, explores how the mechanical oscillatory dynamics of freely suspended microscale silicon beams can be exploited in microchips which combine mechanical sensing with RC signal processing. The microbeams (marked red in image) can be etched into the silicon wafer with standard microchip fabrication technologies . Several such microbeams on the same chip interact nonlinearly with each other by mechanical couplings.]

[Reservoir sensing. A potentially quite promising future application for RC is biochemical and environmental sensing. The reservoir is here a carrier plate coated with some chemical or biochemical material or mix of materials which change their properties when the surface is exposed to (traces of) chemical or biological substances whose presence or concentration has to be measured. The property changes induced in the active coating can be amplified and “dynamified” by additional electrical impulses given to the plate. The resulting spatiotemporal dynamics are recorded from the plate in some way, for instance electrically, and used as reservoir states. The desired measurement signal is trained by the RC principles. This line of sensor engineering was explored in a European FET-OPEN project (RECORD-IT, 2015-2018) coordinated by Zoran Konkoli from the Department of Microtechnology and Nanoscience, Chalmers University of Technology, Gothenburg, Sweden. An interim report is .]

[Optical reservoir computing. Optical computing — that is, computing with light instead of with electricity — is a large field of communication engineering, comparable in importance and promises with quantum computing. The potential benefits of optical versus electrical computing are (i) speed: the natural time constants of optical devices are many orders of magnitude smaller than of electronic devices; (ii) 3D wiring: unlike electrical signals, which need wires which in turn lead to headaches in microchip design because they must not cross each other, light beams can cross each other without interference, (iii) potentially extremely low energy consumption. These potential benefits have boosted optical computing research at a large scale, but breakthroughs are still missing — same as in quantum computing. One approach in this field is optical reservoir computing. This has developed into the currently most important branch among the many versions of physical RC (google optical reservoir computing). A wide spectrum of optical effects and reservoir architectures is being explored. The image shows a microphotograph of an optical microchip (real size 16 mm²) developed at the University of Gent which implements an all-optical reservoir with 16 “neurons”. It was demonstrated in that with this reservoir one could realize 5-bit header recognition in internet packages. There are two aspects of this chip which fascinate me. First, the coils that you see in the image are long spirals of silicon waveguides whose function is to slow down the reservoir dynamics by inserting lengthy light travel paths between the “neurons”. Without these slow-down coils, the native processing speed of this chip would be orders of magnitude too fast for feeding and analysing I/O signals with the available electronic lab equipment. Second, this chip is entirely passive: it needs no extra energy besides the energy in the incoming light signals. — Interestingly, very recently, also the quantum computing field has discovered RC as a potential venue for progress. ]

Appendix

Elementary mathematical structure-forming operations

Pairs, tuples and indexed families

If two mathematical objects 𝒪₁, 𝒪₂ are given, they can be grouped together in a single new mathematical structure called the ordered pair (or just pair) of 𝒪₁, 𝒪₂. It is written as
(𝒪₁, 𝒪₂).
In many cases, 𝒪₁, 𝒪₂ will be of the same kind, for instance both are integers. But the two objects need not be of the same kind. For instance, it is perfectly possible to group integer 𝒪₁ = 3 together with a random variable (a function!) 𝒪₂ = X₇ in a pair, getting (3, X₇).

The crucial property of a pair (𝒪₁, 𝒪₂) which distinguishes it from the set  makes no sense.

A generalization of pairs is N-tuples. For an integer N > 0, an N-tuple of N objects 𝒪₁, 𝒪₂, …, 𝒪_(N) is written as
(𝒪₁, 𝒪₂, …, 𝒪_(N)).
1-tuples are just individual objects; 2-tuples are pairs, and for N > 2, N-tuples are also called lists (by computer scientists that is; mathematicians rather don’t use that term). Again, the crucial property of N-tuples is that one can identify its i-th member by its position in the tuple, or in more technical terminology, by its index. That is, in an N-tuple, every index 1 ≤ i ≤ N “picks” one member from the tuple.

The infinite generalization of N-tuples is provided by indexed families. For any nonempty set I, called an index set in this context,
(𝒪_(i))_(i ∈ I)
denotes a compound object assembled from as many mathematical objects as there are index elements i ∈ I, and within this compound object, every individual member 𝒪_(i) can be “addressed” by its index i. One simply writes
𝒪_(i)
to denote the ith “component” of (𝒪_(i))_(i ∈ I). Writing 𝒪_(i) is a shorthand for applying the ith projection function on (𝒪_(i))_(i ∈ I), that is, 𝒪_(i) = π_(i)((𝒪_(i))_(i ∈ I)).

Products of sets

We first treat the case of products of a finite number of sets. Let S₁, …, S_(N) be (any) sets. Then the product S₁ × … × S_(N) is the set of all N-tuples of elements from the corresponding sets, that is,
S₁ × … × S_(N) = .

This generalizes to infinite products as follows. Let I be any set — we call it an index set in this context. For every i ∈ I, let S_(i) be some set. Then the product set indexed by I is the set of functions
∏_(i ∈ I) S_(i) = .
Using the notation of indexed families, this could equivalently be written as
∏_(i ∈ I) S_(i) = .

If all the sets S_(i) are the same, say S, then the product ∏_(i ∈ I) S_(i) = ∏_(i ∈ I) S is also written as S^(I).

An important special case of infinite products is obtained when I = ℕ. This situation occurs universally in modeling stochastic processes with discrete time. The elements n ∈ ℕ are the points in time when the amplitude of some signal is measured. The amplitude is a real number, so at any time n ∈ ℕ, one records an amplitude value a_(n) ∈ S_(n) = ℝ. The product set
∏_(n ∈ ℕ) S_(n) = 
is the set of all right-infinite real-valued timeseries (with discrete time points starting at time n = 0).

Products of functions

First, again, the case of finite products: let f₁, …, f_(N) be functions, all sharing the same domain D, with image sets S_(i). Then the product f₁ ⊗ … ⊗ f_(N) of these functions is the function with domain D and image set S₁ × … × S_(N) given by

f_1 
S_N
 d  

Again this generalizes to arbitrary products. Let (f_(i) : D → S_(i))_(i ∈ I) be an indexed family of functions, all of them sharing the same domain D, and where the image set of f_(i) is S_(i). The product ⨂_(i ∈ I)f_(i) of this set of functions is defined by


d  
 

Joint, conditional and marginal probabilities

Note. This little section is only a quick memory refresher of some of the most basic concepts of probability. It does not replace a textbook chapter!

We first consider the case of two observations of some part of reality that have discrete values. For instance, an online shop creating customer profiles may record from their customers their age and gender (among many other items). The marketing optimizers of that shop are not interested in the exact age but only in age brackets, say a₁= at most 10 years old, a₂ = 11 − 20 years, a₃ = 21 − 30 years, a₄= older than 30. Gender is roughly categorized into the possibilities . From their customer data the marketing guys estimate the following probability table:



P(X = g_i, Y = a_j)  a_1  a_2  a_3  a_4

g_1  0.005  0.3  0.2  0.04

g_2  0.005  0.15  0.15  0.04

g_3  0.0  0.05  0.05  0.01



The cell (i, j) in this 3 × 4 table contains the probability that a customer with gender g_(i) falls into the age bracket a_(j). This is the joint probability of the two observation values g_(i) and a_(j). Notice that all the numbers in the table sum to 1.

The mathematical tool to formally describe a category of an observable value is a random variable (RV). We typically use symbols X, Y, Z, … for RVs in abstract mathematical formulas. When we deal with concrete applications, we may also use “telling names” for RVs. For instance, in Table [eApp1_1], instead of P(X = g_(i), Y = a_(j)) we could have written 
a_j)

Some more info bits of concepts and terminology connected with RVs. You should consider a RV as the mathematical counterpart of a procedure or apparatus to make observations or measurements. For instance, the real-world counterpart of the RV could be an electronic questionnaire posted by the online shop, or more precisely, the “what is your age?” box on that questionnaire, plus the whole internet infrastructure needed to send the information entered by the customer back to the company’s webserver. Or in a very different example (measuring the speed of a car and showing it to the driver on the speedometer) the real-world counterpart of a RV would be the total on-board circuitry in a car, comprising the wheel rotation sensor, the processing DSP microchip, and the display at the dashboard.

A RV always comes with a set of possible outcomes. This set is called the sample space of the RV, and I usually denote it with the symbol S. Mathematically, a sample space is a set. The sample space for the RV would be the set 
= 

Back to our table and the information it contains. If we are interested only in the age distribution of customers, ignoring the gender aspects, we sum the entries in each age column and get the marginal probabilities of the RV Y. Formally, we compute

P(Y = a_(j)) = ∑_(i = 1, 2, 3)P(X = g_(i), Y = a_(j)).

Similarly, we get the marginal distribution of the gender variable by summing along the rows. The two resulting marginal distributions are indicated in the table [eApp1_2].



  a_1  a_2  a_3  a_4 

g_1  0.005  0.3  0.2  0.04  

g_2  0.005  0.15  0.15  0.04  

g_3  0.0  0.05  0.05  0.01   

 



Notice that the marginal probabilities of age 0.01, 0.5, 0.4, 0.09 sum to 1, as do the gender marginal probabilities.

Finally, the conditional probability P(X = g_(i) | Y = a_(j)) that a customer has gender g_(i) given that the age bracket is a_(j) is computed through dividing the joint probabilities in column j by the sum of all values in this column:


P(X = g_i 

There are two equivalent versions of this formula:

P(X = g_(i), Y = a_(j)) = P(X = g_(i) | Y = a_(j))P(Y = a_(j))

where the righthand side is called a factorization of the joint distribution on the lefthand side, and


P(Y = a_j) = 

demonstrating that each of the three quantities (joint, conditional, marginal probability) can be expressed by the respective two others. If you memorize one of these formulas – I recommend the second one – you have memorized the very key to master “probability arithmetics” and will never get lost when manipulating probability formulas.

The factorization [eA1_3] can be done in two ways: P(Y = a_(j) | X = g_(i))P(X = g_(i)) = P(X = g_(i) | Y = a_(j))P(Y = a_(j)), which gives rise to Bayes’ formula


  P(Y = a_j 
which has many uses in statistical modeling because it shows how one can revert the conditioning direction.

Joint, conditional, and marginal probabilities are also defined when there are more than two categories of observations. For instance, the online shop marketing people also record how much a customer spends on average, and formalize this by a third random variable, say Z. The values that Z can take are spending brackets, say s₁= less than 5 Euros to s₂₀= more than 5000 Euros. The joint probability values P(X = g_(i), Y = a_(j), Z = s_(k)) would be arranged in a 3-dimensional array sized 3 × 4 × 20, and again all values in this array together sum to 1. Now there are different arrangements for conditional and marginal probabilities, for instance P(Z = s_(k) | X = g_(i), Y = a_(j)) is the probability that among the group of customers with gender g_(i) and age a_(j), a person spends an amount in the range s_(k). Or P(Z = s_(k), Y = a_(j) | X = g_(i)) is the probability that in the gender group g_(i) a person is aged a_(j) and spends s_(k). As a last example, the probabilities P(X = g_(i), Z = s_(j)) are the marginal probabilities obtained by summing away the Y variable:

P(X = g_(i), Z = s_(j)) = ∑_(k = 1, 2, 3, 4)P(X = g_(i), Y = a_(k), Z = s_(j))

So far I have described cases where all kinds of observations were discrete, that is, the respective sample spaces S were finite (for example, three gender values) or countably infinite (for example, the natural numbers 1, 2, 3, …). The function P : S → [0, 1] which assigns to each possible outcome s ∈ S its probability P(s) is called a probability mass function (pmf) and we denote it with an upper-case P. The sum of the pmf over all possible outcomes is one: ∑_(s ∈ S)P(s) = 1. If one behaves mathematically very correctly, the symbol P is indexed with the random variable that gives rise to the distribution, that is one would write P_(X)(s). This is often not done out of convenience.

Equally often one faces continuous random values which arise from observations that yield real numbers – for instance, measuring the body height or the weight of a person. Since each such RV can give uncountably infinite many different observation outcomes, their probabilities cannot be represented in a table or array, and they cannot be summed up. Instead, one uses probability density functions (pdf’s) to write down and compute probability values. We denote pdfs by lower-case p.

In order to explain pdfs, let’s start with a single RV, say H= Body Height. Since body heights are non-negative and, say, never larger than 3 m, the distribution of body heights within some reference population can be represented by a pdf f : [0, 3] → ℝ^( ≥ 0) which maps the interval [0, 3] of possible values to the nonnegative reals (Figure 85). We will be using subscripts to make it clear which RV a pdf refers to, so the pdf describing the distribution of body height will be written f_(H).

[A hypothetical distribution of human body sizes in some reference population, represented by a pdf. ]

A pdf for the distribution of a continuous RV X can be used to calculate the probability that this RV takes values within a particular interval, by integrating the pdf over that interval. For instance, the probability that a measurement of body height comes out between 1.5 and 2.0 meters is obtained by
P(H ∈ [1.5, 2.0]) = ∫_(1.5)^(2.0)f_(H)(x)dx,
see the shaded area in Figure 85. Some comments:

-   A probability density function is actually defined to be a function which allows one to compute probabilities of value intervals as in Equation [eapp1_1]. For a given continuous RV X over the reals there is exactly one function f_(X) which has this property, the pdf for X. (This is not quite true. There exist also continuous-valued RVs whose distribution is so complex that it cannot be captured by a pdf, but we will not meet with such phenomena in this lecture. Furthermore, a given pdf can be altered on isolated points – which come from what is called a null set in probability theory – and still be a pdf for the same distribution. But again, we will not be concerned with such subtelties in this lecture.)

-   As a consequence, any pdf f : ℝ → ℝ^( ≥ 0) has the property that it integrates to 1, that is, ∫_( − ∞)^(∞)f(x)dx = 1.

-   Be aware that the values f(x) of a pdf are not probabilities! Pdf’s turn into probabilities only through integration over intervals.

-   Values f(x) can be greater than 1 (as in Figure 85), again indicating that they cannot be taken as probabilities.

Joint distributions of two continuous RVs X, Y can be captured by a pdf f_(X, Y) : ℝ² → ℝ^( ≥ 0). Figure 86 shows an example. Again, the pdf f_(X, Y) of a bivariate continuous distribution must integrate to 1 and be non-negative; and conversely, every such function is the pdf of a continuous distribution of two RV’s.

[An exemplary joint distribution of two continuous-valued RVs X, Y, represented by its pdf.]

Continuing on this track, the joint distribution of k continuous-valued RVs X₁, …, X_(k), where the possible values of each X_(i) are bounded to lie between a_(i) and b_(i) can be described by a unique pdf function f_(X₁, …, X_(k)) : ℝ^(M) → ℝ^( ≥ 0) which integrates to 1, i.e.
∫_(a₁)^(b₁)…∫_(a_(k))^(b_(k))f(x₁, …, x_(k)) dx_(k)…dx₁,
where also the cases a_(i) =  − ∞ and b_(i) = ∞ are possible. A more compact notation for the same integral is
∫_(D)f(u) du,
where D denotes the k-dimensional box [a₁, b₁] × … × [a_(k), b_(k)] and u denotes vectors in ℝ^(k). Mathematicians speak of k-dimensional intervals instead of “boxes”. The set of points S =  is called the support of the distribution. Obviously S ⊆ D.

In analogy to the 1-dim case from Figure 85, probabilities are obtained from a k-dimensional pdf f_(X₁, …, X_(k)) by integrating over sub-intervals. For such a k-dimensional subinterval [r₁, s₁] × … × [r_(k), s_(k)] ⊆ [a₁, b₁] × … × [a_(k), b_(k)], we get its probability by
P(X₁ ∈ [r₁, s₁], …, X_(k) ∈ [r_(k), s_(k)]) = ∫_(r₁)^(s₁)…∫_(r_(k))^(s_(k))f(x₁, …, x_(k)) dx_(k)…dx₁.

In essentially the same way as we did for discrete distributions, the pdf’s of marginal distributions are obtained by integrating away the RV’s that one wishes to expel. In analogy to [eApp1_1a], for instance, one would get
f_(X₁, X₃)(x₁, x₃) = ∫_(a₂)^(b₂)f_(X₁, X₂, X₃)(x₁, x₂, x₃) dx₂.

And finally, pdf’s of conditional distributions are obtained through dividing joint pdfs by marginal pdfs. Such conditional pdfs are used to calculate that some RVs fall into a certain multidimensional interval given that some other RVs take specific values. We only inspect a simple case analog to [eA1_2] where we want to calculate the probability that X falls into a range [a, b] given that Y is known to be c, that is, we want to evaluate the probability P(X ∈ [a, b] | Y = c), using pdfs. We can obtain this probability from the joint pdf f_(X, Y) and the marginal pdf f_(Y) by

P(X 

The r.h.s. expression ∫_(a)^(b)f_(X, Y)(x, c) dx / f_(Y)(c) is a function of x, parametrized by c. This function is a pdf, denoted by f_(X | Y = c), and defined by
Let me illustrate this with a concrete example. An electronics engineer is testing a device which transforms voltages V into currents I. In order to empirically measure the behavior of this device (an electronics engineer would say, in order to “characterize” the device), the engineer carries out a sequence of measurement trials where he first sets the input voltage V to a specific value, say V = 0.0. Then he (or she) measures the resulting current many times, in order to get an idea of the stochastic spread of the current. In mathematical terms, the engineer wants to get an idea of the pdf f_(I | V = 0.0). The engineer then carries on, setting the voltage to other values c₁, c₂, ..., measuring resulting currents in each case, and getting ideas of the conditional pdfs f_(I | V = c_(i)). For understanding the characteristics of this device, the engineer needs to know all of these pdfs.

Conditional distributions arise whenever cause-effect relationships are being modeled. The conditioning variables are causes, the conditioned variables describe effects. In experimental and empirical research, the causes are under the control of an experimenter and can (and have to) be set to specific values in order to assess the statistics of the effects – which are not under the control of the experimenter. In ML pattern classification scenarios, the “causes” are the input patterns and the “effects” are the (stochastically distributed) class label assignments. Since research in the natural sciences is very much focussed on determining Nature’s cause-effect workings, and 90% of the applications in machine learning concern pattern classification (my estimate), it is obvious that conditional distributions lie at the very heart of scientific (and engineering) modeling and data analysis.

In this appendix (and in the lecture) I consider only two ways of representing probability distributions: discrete ones by finite probability tables or probability tables; continuous ones by pdfs. These are the most elementary formats of representing probability distributions. There are many others which ML experts readily command on. This large and varied universe of concrete representations of probability distributions is tied together by an abstract mathematical theory of the probability distributions themselves, independent of particular representations. This theory is called probability theory. It is not an easy theory and we don’t attempt an introduction to it. If you are mathematically minded, then you can get an introduction to probability theory in my graduate lecture notes “Principles of Statistical Modeling” (https://www.ai.rug.nl/minds/uploads/LN_PSM.pdf). At this point I only highlight two core facts from probability theory:

-   A main object of study in probability theory are distributions. They are abstractly and axiomatically defined and analyzed, without reference to particular representations (such as tables or pdfs).

-   A probability distribution always comes together with random variables. We write P_(X) for the distribution of a RV X, P_(X, Y) for the joint distribution of two RVs X, Y, and P_(X | Y) for the conditional distribution (a truly involved concept since it is actually a family of distributions) of X given Y.

The argmax operator

Let φ : D → ℝ be some function from some domain D to the reals. Then

is that d ∈ D for which φ(d) is maximal among all values of φ on D. If there are several arguments a for which φ gives the same maximal value, – that is, φ does not have a unique maximum –, or if φ has no maximum at all, then the argmax is undefined.

The softmax function

In many applications one wishes a neural network to output a probability vector. If the network has d output units, the d-dimensional output vector should be non-negative and its components should sum to 1. This allows one to treat the network output as a “hypothesis vector”, for instance in order to express the network’s “belief” in how an input pattern should be classified. However, the outputs of a trained MLP will not usually perfectly sum to 1, and the activations of output neurons may fall outside the range [0, 1] of admissible probability values. In this situation one takes resort to a method for transforming any real-valued, d-dimensional vector v = (v₁, …, v_(d))′ ∈ ℝ^(d) into a valid probability vector, by passing v through the softmax function:

  
where Z = ∑_(i = 1, …, d)exp (v_(i)) is the normalization constant.

The softmax is more than just a trick to enforce non-negativity and normalization of some vector. It is the key to an elementary machine learning algorithm called logistic regression (https://en.wikipedia.org/wiki/Logistic_regression) and has a direct connection to the Boltzmann distribution (compare Section 6.1).

Expectation, variance, covariance, and correlation of numerical random variables

Recall that a random variable is the mathematical model of an observation / measurement / recording procedure by which one can “sample” observations from that piece of reality that one wishes to model. We usually denote RVs by capital roman letters like X, Y or the like. For example, a data engineer of an internet shop who wants to get a statistical model of its (potential) customers might record the gender and age and spending of shop visitors – this would be formally captured by three random variables G, A, S. A random variable always comes together with a sample space. This is the set of values that might be delivered by the random variable. For instance, the sample space of the gender RV G could be cast as  – a symbolic (and finite) set. A reasonable sample space for the age random variable A would be the set of integers between 0 and 200 – assuming that no customer will be older than 200 years and that age is measured in integers (years). Finally, a reasonable sample space for the spending RV S could be just the real numbers ℝ.

Note that in the A and S examples, the sample spaces that I proposed look very generous. We would not really expect that some customer is 200 years old, nor would we think that ever a customer spends 10¹⁰⁰⁰ Euros – although both values are included in the respective sample space. The important thing about a sample space is that it must contain all the values that might be returned by the RV; but it may also contain values that will never be observed in practice.

Every mathematical set can serve as a sample space. We just saw symbolic, integer, and real sample spaces. Real sample spaces are used whenever one is dealing with an observation procedure that returns numerical values. Real-valued RVs are of great practical importance, and they allow many insightful statistical analyses that are not defined for non-numerical RVs. The most important analytical characteristics of real RVs are expectation, variance, and covariance, which I will now present in turn.

For the remainder of this appendix section we will be considering random variables X whose sample space is ℝ^(K) — that is, observation procedures which return scalars (case n = 1) or vectors. We will furthermore assume that the distributions of all RVs X under consideration will be represented by pdf’s f_(X) : ℝ^(K) → ℝ^( ≥ 0). (In mathematical probability theory, more general numerical sample spaces are considered, as well as distributions that have no pdf — but we will focus on this basic scenario of real-valued RVs with pdfs).

The expectation of a RV X with sample space ℝ^(K) and pdf f_(X) is defined as
E[X] = ∫_(ℝ^(K))x f_(X)(x) dx,
where the integral is written in a common shorthand for
∫_(x₁ =  − ∞)^(∞)…∫_(x_(n) =  − ∞)^(∞)(x₁, …, x_(n))′ f_(X)((x₁, …, x_(n))) dx_(n)…dx₁.

The expectation of a RV X can be intuitively understood as the “average” value that is delivered when the observation procedure X would be carried out infinitely often. The crucial thing to understand about the expectation is that it does not depend on a sample, – it does not depend on specific data.

In contrast, whenever in machine learning we base some learning algorithm on a (numerical) training sample (x_(i), y_(i))_(i = 1, …, N) drawn from the joint distribution P_(X, Y) of two RVs X, Y, we may compute the average value of the x_(i) by

but this sample mean is NOT the expectation of X. If we would have used another random sample, we would most likely have obtained another sample mean. In contrast, the expectation E[X] of X is defined not on the basis of a finite, random sample of X, but it is defined by averaging over the true underlying distribution.

Since in practice we will not have access to the true pdf f_(X), the expectation of a RV X cannot usually be determined in full precision. The best one can do is to estimate it from observed sample data. The sample mean is an estimator for the expectation of a numerical RV X. Marking estimated quantities by a “hat” accent, we may write


A random variable X is centered if its expectation is zero. By subtracting the expectation one gets a centered RV. In these lecture notes I use the bar notation to mark centered RVs:
X̄ := X − E[X].

The variance of a scalar RV with sample space ℝ is the expected squared deviation from the expectation
σ²(X) = E[X̄²],
which in terms of the pdf f_(X̄) of X̄ can be written as
σ²(X) = ∫_(ℝ)x² f_(X̄)(x) dx.

Like the expectation, the variance is an intrinsic property of an observation procedure X and the part of the real world where the measurements may be taken from — it is independent of a concrete sample. A natural way to estimate the variance of X from a sample (x_(i))_(i = 1, …, N) is

1/N 
but in fact this estimator is not the best possible – on average (across different samples) it underestimates the true variance. If one wishes to have an estimator that is unbiased, that is, which on average across different samples gives the correct variance, one must use

1/N 
instead. The Wikipedia article on “Variance”, section “Population variance and sample variance” points out a number of other pitfalls and corrections that one should consider when one estimates variance from samples.

The square root of the variance of X, $


The covariance between two real-valued scalar random variables X, Y is defined as
Cov(X, Y) = E[X̄ Ȳ],
which in terms of a pdf f_(X̄ Ȳ) for the joint distribution for the centered RVs spells out to
Cov(X, Y) = ∫_(ℝ × ℝ)x y f_(X̄ Ȳ)((x, y)′) dx dy.
An unbiased estimate of the covariance, based on a sample (x_(i), y_(i))_(i = 1, …, N) is given by

1/N 

Finally, let us inspect the correlation of two scalar RVs X, Y. Here we have to be careful because this term is used differently in different fields. In statistics, the correlation is defined as


  
It is easy to show that  − 1 ≤ Corr(X, Y) ≤ 1. The correlation in the understanding of statistics can be regarded as a normalized covariance. It has a value of 1 if X and Y are identical up to some positive scaling factor, it has a value of  − 1 if X and Y are identical up to some negative scaling factor. When Corr(X, Y) = 0, X and Y are said to be uncorrelated.

The quantity Corr(X, Y) is also referred to as (population) Pearson’s correlation coefficient, and is often denoted by the greek letter 𝜚(X, Y) = Corr(X, Y).

In the signal processing literature (for instance in my favorite textbook ), the term “correlation” is sometimes used in quite a different way, denoting the quantity
E[X Y],
that is, simply the expectation of the product of the uncentered RVs X and Y. Just be careful when you read terms like “correlation” or “cross-correlation” or “cross-correlation matrix” and make sure that your understanding of the term is the same as the respective author’s.

There are some basic rules for doing calculations with expectations and covariance which one should know:

1.  Expectation is a linear operator:
    E[α X + β Y] = α E[X] + β E[Y],
    where α X is the RV obtained from X by scaling observations with a factor α.

2.  Expectation is idempotent:
    E[E[X]] = E[X].

3.  
    Cov(X, Y) = E[X Y] − E[X] E[Y].